[
    {
        "label": "warnings",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "warnings",
        "description": "warnings",
        "detail": "warnings",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "copy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "copy",
        "description": "copy",
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "lib.capeval.bleu.bleu",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "lib.capeval.bleu.bleu",
        "description": "lib.capeval.bleu.bleu",
        "detail": "lib.capeval.bleu.bleu",
        "documentation": {}
    },
    {
        "label": "lib.capeval.cider.cider",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "lib.capeval.cider.cider",
        "description": "lib.capeval.cider.cider",
        "detail": "lib.capeval.cider.cider",
        "documentation": {}
    },
    {
        "label": "lib.capeval.rouge.rouge",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "lib.capeval.rouge.rouge",
        "description": "lib.capeval.rouge.rouge",
        "detail": "lib.capeval.rouge.rouge",
        "documentation": {}
    },
    {
        "label": "ScannetDatasetConfig",
        "importPath": "data.scannet.model_util_scannet",
        "description": "data.scannet.model_util_scannet",
        "isExtraImport": true,
        "detail": "data.scannet.model_util_scannet",
        "documentation": {}
    },
    {
        "label": "ScannetDatasetConfig",
        "importPath": "data.scannet.model_util_scannet",
        "description": "data.scannet.model_util_scannet",
        "isExtraImport": true,
        "detail": "data.scannet.model_util_scannet",
        "documentation": {}
    },
    {
        "label": "ScannetDatasetConfig",
        "importPath": "data.scannet.model_util_scannet",
        "description": "data.scannet.model_util_scannet",
        "isExtraImport": true,
        "detail": "data.scannet.model_util_scannet",
        "documentation": {}
    },
    {
        "label": "ScannetDatasetConfig",
        "importPath": "data.scannet.model_util_scannet",
        "description": "data.scannet.model_util_scannet",
        "isExtraImport": true,
        "detail": "data.scannet.model_util_scannet",
        "documentation": {}
    },
    {
        "label": "ScannetDatasetConfig",
        "importPath": "data.scannet.model_util_scannet",
        "description": "data.scannet.model_util_scannet",
        "isExtraImport": true,
        "detail": "data.scannet.model_util_scannet",
        "documentation": {}
    },
    {
        "label": "ScannetDatasetConfig",
        "importPath": "data.scannet.model_util_scannet",
        "description": "data.scannet.model_util_scannet",
        "isExtraImport": true,
        "detail": "data.scannet.model_util_scannet",
        "documentation": {}
    },
    {
        "label": "ScannetDatasetConfig",
        "importPath": "data.scannet.model_util_scannet",
        "description": "data.scannet.model_util_scannet",
        "isExtraImport": true,
        "detail": "data.scannet.model_util_scannet",
        "documentation": {}
    },
    {
        "label": "ScannetDatasetConfig",
        "importPath": "data.scannet.model_util_scannet",
        "description": "data.scannet.model_util_scannet",
        "isExtraImport": true,
        "detail": "data.scannet.model_util_scannet",
        "documentation": {}
    },
    {
        "label": "ScannetDatasetConfig",
        "importPath": "data.scannet.model_util_scannet",
        "description": "data.scannet.model_util_scannet",
        "isExtraImport": true,
        "detail": "data.scannet.model_util_scannet",
        "documentation": {}
    },
    {
        "label": "ScannetDatasetConfig",
        "importPath": "data.scannet.model_util_scannet",
        "description": "data.scannet.model_util_scannet",
        "isExtraImport": true,
        "detail": "data.scannet.model_util_scannet",
        "documentation": {}
    },
    {
        "label": "ScannetDatasetConfig",
        "importPath": "data.scannet.model_util_scannet",
        "description": "data.scannet.model_util_scannet",
        "isExtraImport": true,
        "detail": "data.scannet.model_util_scannet",
        "documentation": {}
    },
    {
        "label": "ScannetReferenceDataset",
        "importPath": "lib.joint.dataset",
        "description": "lib.joint.dataset",
        "isExtraImport": true,
        "detail": "lib.joint.dataset",
        "documentation": {}
    },
    {
        "label": "ScannetReferenceDataset",
        "importPath": "lib.joint.dataset",
        "description": "lib.joint.dataset",
        "isExtraImport": true,
        "detail": "lib.joint.dataset",
        "documentation": {}
    },
    {
        "label": "ScannetReferenceDataset",
        "importPath": "lib.joint.dataset",
        "description": "lib.joint.dataset",
        "isExtraImport": true,
        "detail": "lib.joint.dataset",
        "documentation": {}
    },
    {
        "label": "ScannetReferenceDataset",
        "importPath": "lib.joint.dataset",
        "description": "lib.joint.dataset",
        "isExtraImport": true,
        "detail": "lib.joint.dataset",
        "documentation": {}
    },
    {
        "label": "ScannetReferenceDataset",
        "importPath": "lib.joint.dataset",
        "description": "lib.joint.dataset",
        "isExtraImport": true,
        "detail": "lib.joint.dataset",
        "documentation": {}
    },
    {
        "label": "ScannetReferenceDataset",
        "importPath": "lib.joint.dataset",
        "description": "lib.joint.dataset",
        "isExtraImport": true,
        "detail": "lib.joint.dataset",
        "documentation": {}
    },
    {
        "label": "ScannetReferenceDataset",
        "importPath": "lib.joint.dataset",
        "description": "lib.joint.dataset",
        "isExtraImport": true,
        "detail": "lib.joint.dataset",
        "documentation": {}
    },
    {
        "label": "CONF",
        "importPath": "lib.configs.config_ft",
        "description": "lib.configs.config_ft",
        "isExtraImport": true,
        "detail": "lib.configs.config_ft",
        "documentation": {}
    },
    {
        "label": "CONF",
        "importPath": "lib.configs.config_ft",
        "description": "lib.configs.config_ft",
        "isExtraImport": true,
        "detail": "lib.configs.config_ft",
        "documentation": {}
    },
    {
        "label": "CONF",
        "importPath": "lib.configs.config_ft",
        "description": "lib.configs.config_ft",
        "isExtraImport": true,
        "detail": "lib.configs.config_ft",
        "documentation": {}
    },
    {
        "label": "CONF",
        "importPath": "lib.configs.config_ft",
        "description": "lib.configs.config_ft",
        "isExtraImport": true,
        "detail": "lib.configs.config_ft",
        "documentation": {}
    },
    {
        "label": "CONF",
        "importPath": "lib.configs.config_ft",
        "description": "lib.configs.config_ft",
        "isExtraImport": true,
        "detail": "lib.configs.config_ft",
        "documentation": {}
    },
    {
        "label": "CONF",
        "importPath": "lib.configs.config_ft",
        "description": "lib.configs.config_ft",
        "isExtraImport": true,
        "detail": "lib.configs.config_ft",
        "documentation": {}
    },
    {
        "label": "CONF",
        "importPath": "lib.configs.config_ft",
        "description": "lib.configs.config_ft",
        "isExtraImport": true,
        "detail": "lib.configs.config_ft",
        "documentation": {}
    },
    {
        "label": "APCalculator",
        "importPath": "lib.ap_helper.ap_helper_fcos",
        "description": "lib.ap_helper.ap_helper_fcos",
        "isExtraImport": true,
        "detail": "lib.ap_helper.ap_helper_fcos",
        "documentation": {}
    },
    {
        "label": "parse_predictions",
        "importPath": "lib.ap_helper.ap_helper_fcos",
        "description": "lib.ap_helper.ap_helper_fcos",
        "isExtraImport": true,
        "detail": "lib.ap_helper.ap_helper_fcos",
        "documentation": {}
    },
    {
        "label": "parse_groundtruths",
        "importPath": "lib.ap_helper.ap_helper_fcos",
        "description": "lib.ap_helper.ap_helper_fcos",
        "isExtraImport": true,
        "detail": "lib.ap_helper.ap_helper_fcos",
        "documentation": {}
    },
    {
        "label": "APCalculator",
        "importPath": "lib.ap_helper.ap_helper_fcos",
        "description": "lib.ap_helper.ap_helper_fcos",
        "isExtraImport": true,
        "detail": "lib.ap_helper.ap_helper_fcos",
        "documentation": {}
    },
    {
        "label": "parse_predictions",
        "importPath": "lib.ap_helper.ap_helper_fcos",
        "description": "lib.ap_helper.ap_helper_fcos",
        "isExtraImport": true,
        "detail": "lib.ap_helper.ap_helper_fcos",
        "documentation": {}
    },
    {
        "label": "parse_groundtruths",
        "importPath": "lib.ap_helper.ap_helper_fcos",
        "description": "lib.ap_helper.ap_helper_fcos",
        "isExtraImport": true,
        "detail": "lib.ap_helper.ap_helper_fcos",
        "documentation": {}
    },
    {
        "label": "APCalculator",
        "importPath": "lib.ap_helper.ap_helper_fcos",
        "description": "lib.ap_helper.ap_helper_fcos",
        "isExtraImport": true,
        "detail": "lib.ap_helper.ap_helper_fcos",
        "documentation": {}
    },
    {
        "label": "parse_predictions",
        "importPath": "lib.ap_helper.ap_helper_fcos",
        "description": "lib.ap_helper.ap_helper_fcos",
        "isExtraImport": true,
        "detail": "lib.ap_helper.ap_helper_fcos",
        "documentation": {}
    },
    {
        "label": "parse_groundtruths",
        "importPath": "lib.ap_helper.ap_helper_fcos",
        "description": "lib.ap_helper.ap_helper_fcos",
        "isExtraImport": true,
        "detail": "lib.ap_helper.ap_helper_fcos",
        "documentation": {}
    },
    {
        "label": "APCalculator",
        "importPath": "lib.ap_helper.ap_helper_fcos",
        "description": "lib.ap_helper.ap_helper_fcos",
        "isExtraImport": true,
        "detail": "lib.ap_helper.ap_helper_fcos",
        "documentation": {}
    },
    {
        "label": "parse_predictions",
        "importPath": "lib.ap_helper.ap_helper_fcos",
        "description": "lib.ap_helper.ap_helper_fcos",
        "isExtraImport": true,
        "detail": "lib.ap_helper.ap_helper_fcos",
        "documentation": {}
    },
    {
        "label": "parse_groundtruths",
        "importPath": "lib.ap_helper.ap_helper_fcos",
        "description": "lib.ap_helper.ap_helper_fcos",
        "isExtraImport": true,
        "detail": "lib.ap_helper.ap_helper_fcos",
        "documentation": {}
    },
    {
        "label": "get_joint_loss",
        "importPath": "lib.loss_helper.loss_joint",
        "description": "lib.loss_helper.loss_joint",
        "isExtraImport": true,
        "detail": "lib.loss_helper.loss_joint",
        "documentation": {}
    },
    {
        "label": "get_joint_loss",
        "importPath": "lib.loss_helper.loss_joint",
        "description": "lib.loss_helper.loss_joint",
        "isExtraImport": true,
        "detail": "lib.loss_helper.loss_joint",
        "documentation": {}
    },
    {
        "label": "get_joint_loss",
        "importPath": "lib.loss_helper.loss_joint",
        "description": "lib.loss_helper.loss_joint",
        "isExtraImport": true,
        "detail": "lib.loss_helper.loss_joint",
        "documentation": {}
    },
    {
        "label": "get_joint_loss",
        "importPath": "lib.loss_helper.loss_joint",
        "description": "lib.loss_helper.loss_joint",
        "isExtraImport": true,
        "detail": "lib.loss_helper.loss_joint",
        "documentation": {}
    },
    {
        "label": "get_joint_loss",
        "importPath": "lib.loss_helper.loss_joint",
        "description": "lib.loss_helper.loss_joint",
        "isExtraImport": true,
        "detail": "lib.loss_helper.loss_joint",
        "documentation": {}
    },
    {
        "label": "JointNet",
        "importPath": "models.network.network_ft",
        "description": "models.network.network_ft",
        "isExtraImport": true,
        "detail": "models.network.network_ft",
        "documentation": {}
    },
    {
        "label": "JointNet",
        "importPath": "models.network.network_ft",
        "description": "models.network.network_ft",
        "isExtraImport": true,
        "detail": "models.network.network_ft",
        "documentation": {}
    },
    {
        "label": "JointNet",
        "importPath": "models.network.network_ft",
        "description": "models.network.network_ft",
        "isExtraImport": true,
        "detail": "models.network.network_ft",
        "documentation": {}
    },
    {
        "label": "JointNet",
        "importPath": "models.network.network_ft",
        "description": "models.network.network_ft",
        "isExtraImport": true,
        "detail": "models.network.network_ft",
        "documentation": {}
    },
    {
        "label": "JointNet",
        "importPath": "models.network.network_ft",
        "description": "models.network.network_ft",
        "isExtraImport": true,
        "detail": "models.network.network_ft",
        "documentation": {}
    },
    {
        "label": "eval_cap",
        "importPath": "lib.joint.eval_caption",
        "description": "lib.joint.eval_caption",
        "isExtraImport": true,
        "detail": "lib.joint.eval_caption",
        "documentation": {}
    },
    {
        "label": "eval_cap",
        "importPath": "lib.joint.eval_caption",
        "description": "lib.joint.eval_caption",
        "isExtraImport": true,
        "detail": "lib.joint.eval_caption",
        "documentation": {}
    },
    {
        "label": "h5py",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "h5py",
        "description": "h5py",
        "detail": "h5py",
        "documentation": {}
    },
    {
        "label": "PlyData",
        "importPath": "plyfile",
        "description": "plyfile",
        "isExtraImport": true,
        "detail": "plyfile",
        "documentation": {}
    },
    {
        "label": "PlyElement",
        "importPath": "plyfile",
        "description": "plyfile",
        "isExtraImport": true,
        "detail": "plyfile",
        "documentation": {}
    },
    {
        "label": "PlyData",
        "importPath": "plyfile",
        "description": "plyfile",
        "isExtraImport": true,
        "detail": "plyfile",
        "documentation": {}
    },
    {
        "label": "PlyElement",
        "importPath": "plyfile",
        "description": "plyfile",
        "isExtraImport": true,
        "detail": "plyfile",
        "documentation": {}
    },
    {
        "label": "PlyData",
        "importPath": "plyfile",
        "description": "plyfile",
        "isExtraImport": true,
        "detail": "plyfile",
        "documentation": {}
    },
    {
        "label": "PlyElement",
        "importPath": "plyfile",
        "description": "plyfile",
        "isExtraImport": true,
        "detail": "plyfile",
        "documentation": {}
    },
    {
        "label": "PlyData",
        "importPath": "plyfile",
        "description": "plyfile",
        "isExtraImport": true,
        "detail": "plyfile",
        "documentation": {}
    },
    {
        "label": "PlyElement",
        "importPath": "plyfile",
        "description": "plyfile",
        "isExtraImport": true,
        "detail": "plyfile",
        "documentation": {}
    },
    {
        "label": "PlyData",
        "importPath": "plyfile",
        "description": "plyfile",
        "isExtraImport": true,
        "detail": "plyfile",
        "documentation": {}
    },
    {
        "label": "PlyData",
        "importPath": "plyfile",
        "description": "plyfile",
        "isExtraImport": true,
        "detail": "plyfile",
        "documentation": {}
    },
    {
        "label": "copyfile",
        "importPath": "shutil",
        "description": "shutil",
        "isExtraImport": true,
        "detail": "shutil",
        "documentation": {}
    },
    {
        "label": "copyfile",
        "importPath": "shutil",
        "description": "shutil",
        "isExtraImport": true,
        "detail": "shutil",
        "documentation": {}
    },
    {
        "label": "COLORS",
        "importPath": "scripts.colors",
        "description": "scripts.colors",
        "isExtraImport": true,
        "detail": "scripts.colors",
        "documentation": {}
    },
    {
        "label": "pickle",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pickle",
        "description": "pickle",
        "detail": "pickle",
        "documentation": {}
    },
    {
        "label": "importlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "importlib",
        "description": "importlib",
        "detail": "importlib",
        "documentation": {}
    },
    {
        "label": "torch.optim",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.optim",
        "description": "torch.optim",
        "detail": "torch.optim",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "torch.optim",
        "description": "torch.optim",
        "isExtraImport": true,
        "detail": "torch.optim",
        "documentation": {}
    },
    {
        "label": "torch.nn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn",
        "description": "torch.nn",
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "functional",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "CrossEntropyLoss",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "datetime",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "datetime",
        "description": "datetime",
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "get_eval",
        "importPath": "lib.joint.eval_ground",
        "description": "lib.joint.eval_ground",
        "isExtraImport": true,
        "detail": "lib.joint.eval_ground",
        "documentation": {}
    },
    {
        "label": "get_eval",
        "importPath": "lib.joint.eval_ground",
        "description": "lib.joint.eval_ground",
        "isExtraImport": true,
        "detail": "lib.joint.eval_ground",
        "documentation": {}
    },
    {
        "label": "collections",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "collections",
        "description": "collections",
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "get_3d_box",
        "importPath": "utils.box_util",
        "description": "utils.box_util",
        "isExtraImport": true,
        "detail": "utils.box_util",
        "documentation": {}
    },
    {
        "label": "get_3d_box",
        "importPath": "utils.box_util",
        "description": "utils.box_util",
        "isExtraImport": true,
        "detail": "utils.box_util",
        "documentation": {}
    },
    {
        "label": "box3d_iou",
        "importPath": "utils.box_util",
        "description": "utils.box_util",
        "isExtraImport": true,
        "detail": "utils.box_util",
        "documentation": {}
    },
    {
        "label": "get_3d_box",
        "importPath": "utils.box_util",
        "description": "utils.box_util",
        "isExtraImport": true,
        "detail": "utils.box_util",
        "documentation": {}
    },
    {
        "label": "box3d_iou",
        "importPath": "utils.box_util",
        "description": "utils.box_util",
        "isExtraImport": true,
        "detail": "utils.box_util",
        "documentation": {}
    },
    {
        "label": "get_3d_box",
        "importPath": "utils.box_util",
        "description": "utils.box_util",
        "isExtraImport": true,
        "detail": "utils.box_util",
        "documentation": {}
    },
    {
        "label": "box3d_iou_batch_tensor",
        "importPath": "utils.box_util",
        "description": "utils.box_util",
        "isExtraImport": true,
        "detail": "utils.box_util",
        "documentation": {}
    },
    {
        "label": "get_3d_box_batch",
        "importPath": "utils.box_util",
        "description": "utils.box_util",
        "isExtraImport": true,
        "detail": "utils.box_util",
        "documentation": {}
    },
    {
        "label": "rotz_batch_pytorch",
        "importPath": "utils.box_util",
        "description": "utils.box_util",
        "isExtraImport": true,
        "detail": "utils.box_util",
        "documentation": {}
    },
    {
        "label": "get_3d_box_batch",
        "importPath": "utils.box_util",
        "description": "utils.box_util",
        "isExtraImport": true,
        "detail": "utils.box_util",
        "documentation": {}
    },
    {
        "label": "rotz_batch_pytorch",
        "importPath": "utils.box_util",
        "description": "utils.box_util",
        "isExtraImport": true,
        "detail": "utils.box_util",
        "documentation": {}
    },
    {
        "label": "get_3d_box",
        "importPath": "utils.box_util",
        "description": "utils.box_util",
        "isExtraImport": true,
        "detail": "utils.box_util",
        "documentation": {}
    },
    {
        "label": "get_3d_box",
        "importPath": "utils.box_util",
        "description": "utils.box_util",
        "isExtraImport": true,
        "detail": "utils.box_util",
        "documentation": {}
    },
    {
        "label": "box3d_iou",
        "importPath": "utils.box_util",
        "description": "utils.box_util",
        "isExtraImport": true,
        "detail": "utils.box_util",
        "documentation": {}
    },
    {
        "label": "torch.nn.functional",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn.functional",
        "description": "torch.nn.functional",
        "detail": "torch.nn.functional",
        "documentation": {}
    },
    {
        "label": "write_ply_rgb",
        "importPath": "utils.pc_utils",
        "description": "utils.pc_utils",
        "isExtraImport": true,
        "detail": "utils.pc_utils",
        "documentation": {}
    },
    {
        "label": "write_oriented_bbox",
        "importPath": "utils.pc_utils",
        "description": "utils.pc_utils",
        "isExtraImport": true,
        "detail": "utils.pc_utils",
        "documentation": {}
    },
    {
        "label": "bbox_corner_dist_measure",
        "importPath": "utils.pc_utils",
        "description": "utils.pc_utils",
        "isExtraImport": true,
        "detail": "utils.pc_utils",
        "documentation": {}
    },
    {
        "label": "JointNet",
        "importPath": "models.network.jointnet",
        "description": "models.network.jointnet",
        "isExtraImport": true,
        "detail": "models.network.jointnet",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "overwrite_config",
        "importPath": "utils.misc",
        "description": "utils.misc",
        "isExtraImport": true,
        "detail": "utils.misc",
        "documentation": {}
    },
    {
        "label": "overwrite_config",
        "importPath": "utils.misc",
        "description": "utils.misc",
        "isExtraImport": true,
        "detail": "utils.misc",
        "documentation": {}
    },
    {
        "label": "CONF",
        "importPath": "lib.configs.config_qa",
        "description": "lib.configs.config_qa",
        "isExtraImport": true,
        "detail": "lib.configs.config_qa",
        "documentation": {}
    },
    {
        "label": "CONF",
        "importPath": "lib.configs.config_qa",
        "description": "lib.configs.config_qa",
        "isExtraImport": true,
        "detail": "lib.configs.config_qa",
        "documentation": {}
    },
    {
        "label": "CONF",
        "importPath": "lib.configs.config_qa",
        "description": "lib.configs.config_qa",
        "isExtraImport": true,
        "detail": "lib.configs.config_qa",
        "documentation": {}
    },
    {
        "label": "CONF",
        "importPath": "lib.configs.config_qa",
        "description": "lib.configs.config_qa",
        "isExtraImport": true,
        "detail": "lib.configs.config_qa",
        "documentation": {}
    },
    {
        "label": "ScannetQADataset",
        "importPath": "lib.joint.dataset_qa",
        "description": "lib.joint.dataset_qa",
        "isExtraImport": true,
        "detail": "lib.joint.dataset_qa",
        "documentation": {}
    },
    {
        "label": "ScannetQADatasetConfig",
        "importPath": "lib.joint.dataset_qa",
        "description": "lib.joint.dataset_qa",
        "isExtraImport": true,
        "detail": "lib.joint.dataset_qa",
        "documentation": {}
    },
    {
        "label": "ScannetQADataset",
        "importPath": "lib.joint.dataset_qa",
        "description": "lib.joint.dataset_qa",
        "isExtraImport": true,
        "detail": "lib.joint.dataset_qa",
        "documentation": {}
    },
    {
        "label": "ScannetQADataset",
        "importPath": "lib.joint.dataset_qa",
        "description": "lib.joint.dataset_qa",
        "isExtraImport": true,
        "detail": "lib.joint.dataset_qa",
        "documentation": {}
    },
    {
        "label": "ScannetQADatasetConfig",
        "importPath": "lib.joint.dataset_qa",
        "description": "lib.joint.dataset_qa",
        "isExtraImport": true,
        "detail": "lib.joint.dataset_qa",
        "documentation": {}
    },
    {
        "label": "APCalculator",
        "importPath": "lib.ap_helper.ap_helper_votenet",
        "description": "lib.ap_helper.ap_helper_votenet",
        "isExtraImport": true,
        "detail": "lib.ap_helper.ap_helper_votenet",
        "documentation": {}
    },
    {
        "label": "parse_predictions",
        "importPath": "lib.ap_helper.ap_helper_votenet",
        "description": "lib.ap_helper.ap_helper_votenet",
        "isExtraImport": true,
        "detail": "lib.ap_helper.ap_helper_votenet",
        "documentation": {}
    },
    {
        "label": "parse_groundtruths",
        "importPath": "lib.ap_helper.ap_helper_votenet",
        "description": "lib.ap_helper.ap_helper_votenet",
        "isExtraImport": true,
        "detail": "lib.ap_helper.ap_helper_votenet",
        "documentation": {}
    },
    {
        "label": "parse_predictions",
        "importPath": "lib.ap_helper.ap_helper_votenet",
        "description": "lib.ap_helper.ap_helper_votenet",
        "isExtraImport": true,
        "detail": "lib.ap_helper.ap_helper_votenet",
        "documentation": {}
    },
    {
        "label": "get_loss",
        "importPath": "lib.loss_helper.loss_qa",
        "description": "lib.loss_helper.loss_qa",
        "isExtraImport": true,
        "detail": "lib.loss_helper.loss_qa",
        "documentation": {}
    },
    {
        "label": "get_loss",
        "importPath": "lib.loss_helper.loss_qa",
        "description": "lib.loss_helper.loss_qa",
        "isExtraImport": true,
        "detail": "lib.loss_helper.loss_qa",
        "documentation": {}
    },
    {
        "label": "get_eval",
        "importPath": "lib.joint.eval_qa",
        "description": "lib.joint.eval_qa",
        "isExtraImport": true,
        "detail": "lib.joint.eval_qa",
        "documentation": {}
    },
    {
        "label": "JointNet",
        "importPath": "models.network.network_ft_qa",
        "description": "models.network.network_ft_qa",
        "isExtraImport": true,
        "detail": "models.network.network_ft_qa",
        "documentation": {}
    },
    {
        "label": "JointNet",
        "importPath": "models.network.network_ft_qa",
        "description": "models.network.network_ft_qa",
        "isExtraImport": true,
        "detail": "models.network.network_ft_qa",
        "documentation": {}
    },
    {
        "label": "JointNet",
        "importPath": "models.network.network_ft_qa",
        "description": "models.network.network_ft_qa",
        "isExtraImport": true,
        "detail": "models.network.network_ft_qa",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoConfig",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "BertConfig",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "BertModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "BertTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "BertConfig",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "BertModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "BertTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "torchvision.transforms",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "imread",
        "importPath": "imageio",
        "description": "imageio",
        "isExtraImport": true,
        "detail": "imageio",
        "documentation": {}
    },
    {
        "label": "imread",
        "importPath": "imageio",
        "description": "imageio",
        "isExtraImport": true,
        "detail": "imageio",
        "documentation": {}
    },
    {
        "label": "imread",
        "importPath": "imageio",
        "description": "imageio",
        "isExtraImport": true,
        "detail": "imageio",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "create_enet_for_3d",
        "importPath": "lib.enet",
        "description": "lib.enet",
        "isExtraImport": true,
        "detail": "lib.enet",
        "documentation": {}
    },
    {
        "label": "create_enet_for_3d",
        "importPath": "lib.enet",
        "description": "lib.enet",
        "isExtraImport": true,
        "detail": "lib.enet",
        "documentation": {}
    },
    {
        "label": "CONF",
        "importPath": "lib.config",
        "description": "lib.config",
        "isExtraImport": true,
        "detail": "lib.config",
        "documentation": {}
    },
    {
        "label": "CONF",
        "importPath": "lib.config",
        "description": "lib.config",
        "isExtraImport": true,
        "detail": "lib.config",
        "documentation": {}
    },
    {
        "label": "CONF",
        "importPath": "lib.config",
        "description": "lib.config",
        "isExtraImport": true,
        "detail": "lib.config",
        "documentation": {}
    },
    {
        "label": "ProjectionHelper",
        "importPath": "lib.projection",
        "description": "lib.projection",
        "isExtraImport": true,
        "detail": "lib.projection",
        "documentation": {}
    },
    {
        "label": "ProjectionHelper",
        "importPath": "lib.projection",
        "description": "lib.projection",
        "isExtraImport": true,
        "detail": "lib.projection",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "Solver",
        "importPath": "lib.joint.solver_ft",
        "description": "lib.joint.solver_ft",
        "isExtraImport": true,
        "detail": "lib.joint.solver_ft",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "scripts.utils.AdamW",
        "description": "scripts.utils.AdamW",
        "isExtraImport": true,
        "detail": "scripts.utils.AdamW",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "importPath": "scripts.utils.AdamW",
        "description": "scripts.utils.AdamW",
        "isExtraImport": true,
        "detail": "scripts.utils.AdamW",
        "documentation": {}
    },
    {
        "label": "set_params_lr_dict",
        "importPath": "scripts.utils.script_utils",
        "description": "scripts.utils.script_utils",
        "isExtraImport": true,
        "detail": "scripts.utils.script_utils",
        "documentation": {}
    },
    {
        "label": "set_params_lr_dict",
        "importPath": "scripts.utils.script_utils",
        "description": "scripts.utils.script_utils",
        "isExtraImport": true,
        "detail": "scripts.utils.script_utils",
        "documentation": {}
    },
    {
        "label": "Solver",
        "importPath": "lib.joint.solver_qa",
        "description": "lib.joint.solver_qa",
        "isExtraImport": true,
        "detail": "lib.joint.solver_qa",
        "documentation": {}
    },
    {
        "label": "Solver",
        "importPath": "lib.joint.solver_pretrain",
        "description": "lib.joint.solver_pretrain",
        "isExtraImport": true,
        "detail": "lib.joint.solver_pretrain",
        "documentation": {}
    },
    {
        "label": "CONF",
        "importPath": "lib.configs.config_pretrain",
        "description": "lib.configs.config_pretrain",
        "isExtraImport": true,
        "detail": "lib.configs.config_pretrain",
        "documentation": {}
    },
    {
        "label": "JointNet",
        "importPath": "models.network.network_pretrain",
        "description": "models.network.network_pretrain",
        "isExtraImport": true,
        "detail": "models.network.network_pretrain",
        "documentation": {}
    },
    {
        "label": "Optimizer",
        "importPath": "torch.optim.optimizer",
        "description": "torch.optim.optimizer",
        "isExtraImport": true,
        "detail": "torch.optim.optimizer",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "with_statement",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "unicode_literals",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "ConvexHull",
        "importPath": "scipy.spatial",
        "description": "scipy.spatial",
        "isExtraImport": true,
        "detail": "scipy.spatial",
        "documentation": {}
    },
    {
        "label": "subprocess",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "subprocess",
        "description": "subprocess",
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "calc_iou",
        "importPath": "utils.metric_util",
        "description": "utils.metric_util",
        "isExtraImport": true,
        "detail": "utils.metric_util",
        "documentation": {}
    },
    {
        "label": "multiprocessing",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "multiprocessing",
        "description": "multiprocessing",
        "detail": "multiprocessing",
        "documentation": {}
    },
    {
        "label": "Pool",
        "importPath": "multiprocessing",
        "description": "multiprocessing",
        "isExtraImport": true,
        "detail": "multiprocessing",
        "documentation": {}
    },
    {
        "label": "trimesh",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "trimesh",
        "description": "trimesh",
        "detail": "trimesh",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "torch.distributed",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.distributed",
        "description": "torch.distributed",
        "detail": "torch.distributed",
        "documentation": {}
    },
    {
        "label": "numba",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numba",
        "description": "numba",
        "detail": "numba",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "export",
        "importPath": "load_scannet_data",
        "description": "load_scannet_data",
        "isExtraImport": true,
        "detail": "load_scannet_data",
        "documentation": {}
    },
    {
        "label": "export",
        "importPath": "load_scannet_data",
        "description": "load_scannet_data",
        "isExtraImport": true,
        "detail": "load_scannet_data",
        "documentation": {}
    },
    {
        "label": "pdb",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pdb",
        "description": "pdb",
        "detail": "pdb",
        "documentation": {}
    },
    {
        "label": "urllib.request",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "urllib.request",
        "description": "urllib.request",
        "detail": "urllib.request",
        "documentation": {}
    },
    {
        "label": "tempfile",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tempfile",
        "description": "tempfile",
        "detail": "tempfile",
        "documentation": {}
    },
    {
        "label": "os,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os.",
        "description": "os.",
        "detail": "os.",
        "documentation": {}
    },
    {
        "label": "inspect",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "inspect",
        "description": "inspect",
        "detail": "inspect",
        "documentation": {}
    },
    {
        "label": "scannet_utils",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "scannet_utils",
        "description": "scannet_utils",
        "detail": "scannet_utils",
        "documentation": {}
    },
    {
        "label": "CONF",
        "importPath": "lib.configs.config",
        "description": "lib.configs.config",
        "isExtraImport": true,
        "detail": "lib.configs.config",
        "documentation": {}
    },
    {
        "label": "CONF",
        "importPath": "lib.configs.config",
        "description": "lib.configs.config",
        "isExtraImport": true,
        "detail": "lib.configs.config",
        "documentation": {}
    },
    {
        "label": "csv",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "csv",
        "description": "csv",
        "detail": "csv",
        "documentation": {}
    },
    {
        "label": "quaternion",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "quaternion",
        "description": "quaternion",
        "detail": "quaternion",
        "documentation": {}
    },
    {
        "label": "registry",
        "importPath": "pipeline.registry",
        "description": "pipeline.registry",
        "isExtraImport": true,
        "detail": "pipeline.registry",
        "documentation": {}
    },
    {
        "label": "registry",
        "importPath": "pipeline.registry",
        "description": "pipeline.registry",
        "isExtraImport": true,
        "detail": "pipeline.registry",
        "documentation": {}
    },
    {
        "label": "registry",
        "importPath": "pipeline.registry",
        "description": "pipeline.registry",
        "isExtraImport": true,
        "detail": "pipeline.registry",
        "documentation": {}
    },
    {
        "label": "registry",
        "importPath": "pipeline.registry",
        "description": "pipeline.registry",
        "isExtraImport": true,
        "detail": "pipeline.registry",
        "documentation": {}
    },
    {
        "label": "registry",
        "importPath": "pipeline.registry",
        "description": "pipeline.registry",
        "isExtraImport": true,
        "detail": "pipeline.registry",
        "documentation": {}
    },
    {
        "label": "registry",
        "importPath": "pipeline.registry",
        "description": "pipeline.registry",
        "isExtraImport": true,
        "detail": "pipeline.registry",
        "documentation": {}
    },
    {
        "label": "registry",
        "importPath": "pipeline.registry",
        "description": "pipeline.registry",
        "isExtraImport": true,
        "detail": "pipeline.registry",
        "documentation": {}
    },
    {
        "label": "registry",
        "importPath": "pipeline.registry",
        "description": "pipeline.registry",
        "isExtraImport": true,
        "detail": "pipeline.registry",
        "documentation": {}
    },
    {
        "label": "registry",
        "importPath": "pipeline.registry",
        "description": "pipeline.registry",
        "isExtraImport": true,
        "detail": "pipeline.registry",
        "documentation": {}
    },
    {
        "label": "registry",
        "importPath": "pipeline.registry",
        "description": "pipeline.registry",
        "isExtraImport": true,
        "detail": "pipeline.registry",
        "documentation": {}
    },
    {
        "label": "registry",
        "importPath": "pipeline.registry",
        "description": "pipeline.registry",
        "isExtraImport": true,
        "detail": "pipeline.registry",
        "documentation": {}
    },
    {
        "label": "registry",
        "importPath": "pipeline.registry",
        "description": "pipeline.registry",
        "isExtraImport": true,
        "detail": "pipeline.registry",
        "documentation": {}
    },
    {
        "label": "registry",
        "importPath": "pipeline.registry",
        "description": "pipeline.registry",
        "isExtraImport": true,
        "detail": "pipeline.registry",
        "documentation": {}
    },
    {
        "label": "registry",
        "importPath": "pipeline.registry",
        "description": "pipeline.registry",
        "isExtraImport": true,
        "detail": "pipeline.registry",
        "documentation": {}
    },
    {
        "label": "registry",
        "importPath": "pipeline.registry",
        "description": "pipeline.registry",
        "isExtraImport": true,
        "detail": "pipeline.registry",
        "documentation": {}
    },
    {
        "label": "registry",
        "importPath": "pipeline.registry",
        "description": "pipeline.registry",
        "isExtraImport": true,
        "detail": "pipeline.registry",
        "documentation": {}
    },
    {
        "label": "registry",
        "importPath": "pipeline.registry",
        "description": "pipeline.registry",
        "isExtraImport": true,
        "detail": "pipeline.registry",
        "documentation": {}
    },
    {
        "label": "setup_imports",
        "importPath": "pipeline.registry",
        "description": "pipeline.registry",
        "isExtraImport": true,
        "detail": "pipeline.registry",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "dataset.data_converter",
        "description": "dataset.data_converter",
        "isExtraImport": true,
        "detail": "dataset.data_converter",
        "documentation": {}
    },
    {
        "label": "L",
        "importPath": "re",
        "description": "re",
        "isExtraImport": true,
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "dataset.data_wrapper",
        "description": "dataset.data_wrapper",
        "isExtraImport": true,
        "detail": "dataset.data_wrapper",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "dataset.scanqa",
        "description": "dataset.scanqa",
        "isExtraImport": true,
        "detail": "dataset.scanqa",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "dataset.scanrefer",
        "description": "dataset.scanrefer",
        "isExtraImport": true,
        "detail": "dataset.scanrefer",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "dataset.referit3d",
        "description": "dataset.referit3d",
        "isExtraImport": true,
        "detail": "dataset.referit3d",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "dataset.scan2cap",
        "description": "dataset.scan2cap",
        "isExtraImport": true,
        "detail": "dataset.scan2cap",
        "documentation": {}
    },
    {
        "label": "SQADataset",
        "importPath": "dataset.sqa",
        "description": "dataset.sqa",
        "isExtraImport": true,
        "detail": "dataset.sqa",
        "documentation": {}
    },
    {
        "label": "SynDataset",
        "importPath": "dataset.syn",
        "description": "dataset.syn",
        "isExtraImport": true,
        "detail": "dataset.syn",
        "documentation": {}
    },
    {
        "label": "SCAN_FAMILY_BASE",
        "importPath": "dataset.path_config",
        "description": "dataset.path_config",
        "isExtraImport": true,
        "detail": "dataset.path_config",
        "documentation": {}
    },
    {
        "label": "MASK_BASE",
        "importPath": "dataset.path_config",
        "description": "dataset.path_config",
        "isExtraImport": true,
        "detail": "dataset.path_config",
        "documentation": {}
    },
    {
        "label": "SCAN_FAMILY_BASE",
        "importPath": "dataset.path_config",
        "description": "dataset.path_config",
        "isExtraImport": true,
        "detail": "dataset.path_config",
        "documentation": {}
    },
    {
        "label": "SCAN_FAMILY_BASE",
        "importPath": "dataset.path_config",
        "description": "dataset.path_config",
        "isExtraImport": true,
        "detail": "dataset.path_config",
        "documentation": {}
    },
    {
        "label": "SCAN_FAMILY_BASE",
        "importPath": "dataset.path_config",
        "description": "dataset.path_config",
        "isExtraImport": true,
        "detail": "dataset.path_config",
        "documentation": {}
    },
    {
        "label": "MASK_BASE",
        "importPath": "dataset.path_config",
        "description": "dataset.path_config",
        "isExtraImport": true,
        "detail": "dataset.path_config",
        "documentation": {}
    },
    {
        "label": "SCAN_FAMILY_BASE",
        "importPath": "dataset.path_config",
        "description": "dataset.path_config",
        "isExtraImport": true,
        "detail": "dataset.path_config",
        "documentation": {}
    },
    {
        "label": "SCAN_FAMILY_BASE",
        "importPath": "dataset.path_config",
        "description": "dataset.path_config",
        "isExtraImport": true,
        "detail": "dataset.path_config",
        "documentation": {}
    },
    {
        "label": "SCAN_FAMILY_BASE",
        "importPath": "dataset.path_config",
        "description": "dataset.path_config",
        "isExtraImport": true,
        "detail": "dataset.path_config",
        "documentation": {}
    },
    {
        "label": "SCAN_FAMILY_BASE",
        "importPath": "dataset.path_config",
        "description": "dataset.path_config",
        "isExtraImport": true,
        "detail": "dataset.path_config",
        "documentation": {}
    },
    {
        "label": "SCAN_FAMILY_BASE",
        "importPath": "dataset.path_config",
        "description": "dataset.path_config",
        "isExtraImport": true,
        "detail": "dataset.path_config",
        "documentation": {}
    },
    {
        "label": "MASK_BASE",
        "importPath": "dataset.path_config",
        "description": "dataset.path_config",
        "isExtraImport": true,
        "detail": "dataset.path_config",
        "documentation": {}
    },
    {
        "label": "SCAN_FAMILY_BASE",
        "importPath": "dataset.path_config",
        "description": "dataset.path_config",
        "isExtraImport": true,
        "detail": "dataset.path_config",
        "documentation": {}
    },
    {
        "label": "SCAN_FAMILY_BASE",
        "importPath": "dataset.path_config",
        "description": "dataset.path_config",
        "isExtraImport": true,
        "detail": "dataset.path_config",
        "documentation": {}
    },
    {
        "label": "sparse",
        "importPath": "scipy",
        "description": "scipy",
        "isExtraImport": true,
        "detail": "scipy",
        "documentation": {}
    },
    {
        "label": "sparse",
        "importPath": "scipy",
        "description": "scipy",
        "isExtraImport": true,
        "detail": "scipy",
        "documentation": {}
    },
    {
        "label": "sparse",
        "importPath": "scipy",
        "description": "scipy",
        "isExtraImport": true,
        "detail": "scipy",
        "documentation": {}
    },
    {
        "label": "convert_pc_to_box",
        "importPath": "utils.eval_helper",
        "description": "utils.eval_helper",
        "isExtraImport": true,
        "detail": "utils.eval_helper",
        "documentation": {}
    },
    {
        "label": "construct_bbox_corners",
        "importPath": "utils.eval_helper",
        "description": "utils.eval_helper",
        "isExtraImport": true,
        "detail": "utils.eval_helper",
        "documentation": {}
    },
    {
        "label": "convert_pc_to_box",
        "importPath": "utils.eval_helper",
        "description": "utils.eval_helper",
        "isExtraImport": true,
        "detail": "utils.eval_helper",
        "documentation": {}
    },
    {
        "label": "eval_ref_one_sample",
        "importPath": "utils.eval_helper",
        "description": "utils.eval_helper",
        "isExtraImport": true,
        "detail": "utils.eval_helper",
        "documentation": {}
    },
    {
        "label": "is_explicitly_view_dependent",
        "importPath": "utils.eval_helper",
        "description": "utils.eval_helper",
        "isExtraImport": true,
        "detail": "utils.eval_helper",
        "documentation": {}
    },
    {
        "label": "construct_bbox_corners",
        "importPath": "utils.eval_helper",
        "description": "utils.eval_helper",
        "isExtraImport": true,
        "detail": "utils.eval_helper",
        "documentation": {}
    },
    {
        "label": "convert_pc_to_box",
        "importPath": "utils.eval_helper",
        "description": "utils.eval_helper",
        "isExtraImport": true,
        "detail": "utils.eval_helper",
        "documentation": {}
    },
    {
        "label": "eval_ref_one_sample",
        "importPath": "utils.eval_helper",
        "description": "utils.eval_helper",
        "isExtraImport": true,
        "detail": "utils.eval_helper",
        "documentation": {}
    },
    {
        "label": "convert_pc_to_box",
        "importPath": "utils.eval_helper",
        "description": "utils.eval_helper",
        "isExtraImport": true,
        "detail": "utils.eval_helper",
        "documentation": {}
    },
    {
        "label": "construct_bbox_corners",
        "importPath": "utils.eval_helper",
        "description": "utils.eval_helper",
        "isExtraImport": true,
        "detail": "utils.eval_helper",
        "documentation": {}
    },
    {
        "label": "eval_ref_one_sample",
        "importPath": "utils.eval_helper",
        "description": "utils.eval_helper",
        "isExtraImport": true,
        "detail": "utils.eval_helper",
        "documentation": {}
    },
    {
        "label": "construct_bbox_corners",
        "importPath": "utils.eval_helper",
        "description": "utils.eval_helper",
        "isExtraImport": true,
        "detail": "utils.eval_helper",
        "documentation": {}
    },
    {
        "label": "convert_pc_to_box",
        "importPath": "utils.eval_helper",
        "description": "utils.eval_helper",
        "isExtraImport": true,
        "detail": "utils.eval_helper",
        "documentation": {}
    },
    {
        "label": "eval_ref_one_sample",
        "importPath": "utils.eval_helper",
        "description": "utils.eval_helper",
        "isExtraImport": true,
        "detail": "utils.eval_helper",
        "documentation": {}
    },
    {
        "label": "convert_pc_to_box",
        "importPath": "utils.eval_helper",
        "description": "utils.eval_helper",
        "isExtraImport": true,
        "detail": "utils.eval_helper",
        "documentation": {}
    },
    {
        "label": "construct_bbox_corners",
        "importPath": "utils.eval_helper",
        "description": "utils.eval_helper",
        "isExtraImport": true,
        "detail": "utils.eval_helper",
        "documentation": {}
    },
    {
        "label": "eval_ref_one_sample",
        "importPath": "utils.eval_helper",
        "description": "utils.eval_helper",
        "isExtraImport": true,
        "detail": "utils.eval_helper",
        "documentation": {}
    },
    {
        "label": "construct_bbox_corners",
        "importPath": "utils.eval_helper",
        "description": "utils.eval_helper",
        "isExtraImport": true,
        "detail": "utils.eval_helper",
        "documentation": {}
    },
    {
        "label": "convert_pc_to_box",
        "importPath": "utils.eval_helper",
        "description": "utils.eval_helper",
        "isExtraImport": true,
        "detail": "utils.eval_helper",
        "documentation": {}
    },
    {
        "label": "eval_ref_one_sample",
        "importPath": "utils.eval_helper",
        "description": "utils.eval_helper",
        "isExtraImport": true,
        "detail": "utils.eval_helper",
        "documentation": {}
    },
    {
        "label": "jsonlines",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "jsonlines",
        "description": "jsonlines",
        "detail": "jsonlines",
        "documentation": {}
    },
    {
        "label": "DataAugmentationMixin",
        "importPath": "dataset.dataset_mixin",
        "description": "dataset.dataset_mixin",
        "isExtraImport": true,
        "detail": "dataset.dataset_mixin",
        "documentation": {}
    },
    {
        "label": "LoadScannetMixin",
        "importPath": "dataset.dataset_mixin",
        "description": "dataset.dataset_mixin",
        "isExtraImport": true,
        "detail": "dataset.dataset_mixin",
        "documentation": {}
    },
    {
        "label": "DataAugmentationMixin",
        "importPath": "dataset.dataset_mixin",
        "description": "dataset.dataset_mixin",
        "isExtraImport": true,
        "detail": "dataset.dataset_mixin",
        "documentation": {}
    },
    {
        "label": "LoadScannetMixin",
        "importPath": "dataset.dataset_mixin",
        "description": "dataset.dataset_mixin",
        "isExtraImport": true,
        "detail": "dataset.dataset_mixin",
        "documentation": {}
    },
    {
        "label": "LoadScannetMixin",
        "importPath": "dataset.dataset_mixin",
        "description": "dataset.dataset_mixin",
        "isExtraImport": true,
        "detail": "dataset.dataset_mixin",
        "documentation": {}
    },
    {
        "label": "DataAugmentationMixin",
        "importPath": "dataset.dataset_mixin",
        "description": "dataset.dataset_mixin",
        "isExtraImport": true,
        "detail": "dataset.dataset_mixin",
        "documentation": {}
    },
    {
        "label": "DataAugmentationMixin",
        "importPath": "dataset.dataset_mixin",
        "description": "dataset.dataset_mixin",
        "isExtraImport": true,
        "detail": "dataset.dataset_mixin",
        "documentation": {}
    },
    {
        "label": "LoadScannetMixin",
        "importPath": "dataset.dataset_mixin",
        "description": "dataset.dataset_mixin",
        "isExtraImport": true,
        "detail": "dataset.dataset_mixin",
        "documentation": {}
    },
    {
        "label": "LoadScannetMixin",
        "importPath": "dataset.dataset_mixin",
        "description": "dataset.dataset_mixin",
        "isExtraImport": true,
        "detail": "dataset.dataset_mixin",
        "documentation": {}
    },
    {
        "label": "DataAugmentationMixin",
        "importPath": "dataset.dataset_mixin",
        "description": "dataset.dataset_mixin",
        "isExtraImport": true,
        "detail": "dataset.dataset_mixin",
        "documentation": {}
    },
    {
        "label": "DataAugmentationMixin",
        "importPath": "dataset.dataset_mixin",
        "description": "dataset.dataset_mixin",
        "isExtraImport": true,
        "detail": "dataset.dataset_mixin",
        "documentation": {}
    },
    {
        "label": "LoadScannetMixin",
        "importPath": "dataset.dataset_mixin",
        "description": "dataset.dataset_mixin",
        "isExtraImport": true,
        "detail": "dataset.dataset_mixin",
        "documentation": {}
    },
    {
        "label": "LabelConverter",
        "importPath": "utils.label_utils",
        "description": "utils.label_utils",
        "isExtraImport": true,
        "detail": "utils.label_utils",
        "documentation": {}
    },
    {
        "label": "LabelConverter",
        "importPath": "utils.label_utils",
        "description": "utils.label_utils",
        "isExtraImport": true,
        "detail": "utils.label_utils",
        "documentation": {}
    },
    {
        "label": "LabelConverter",
        "importPath": "utils.label_utils",
        "description": "utils.label_utils",
        "isExtraImport": true,
        "detail": "utils.label_utils",
        "documentation": {}
    },
    {
        "label": "LabelConverter",
        "importPath": "utils.label_utils",
        "description": "utils.label_utils",
        "isExtraImport": true,
        "detail": "utils.label_utils",
        "documentation": {}
    },
    {
        "label": "LabelConverter",
        "importPath": "utils.label_utils",
        "description": "utils.label_utils",
        "isExtraImport": true,
        "detail": "utils.label_utils",
        "documentation": {}
    },
    {
        "label": "LabelConverter",
        "importPath": "utils.label_utils",
        "description": "utils.label_utils",
        "isExtraImport": true,
        "detail": "utils.label_utils",
        "documentation": {}
    },
    {
        "label": "pointnet2_utils",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pointnet2_utils",
        "description": "pointnet2_utils",
        "detail": "pointnet2_utils",
        "documentation": {}
    },
    {
        "label": "pytorch_utils",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pytorch_utils",
        "description": "pytorch_utils",
        "detail": "pytorch_utils",
        "documentation": {}
    },
    {
        "label": "SharedMLP",
        "importPath": "pytorch_utils",
        "description": "pytorch_utils",
        "isExtraImport": true,
        "detail": "pytorch_utils",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "gradcheck",
        "importPath": "torch.autograd",
        "description": "torch.autograd",
        "isExtraImport": true,
        "detail": "torch.autograd",
        "documentation": {}
    },
    {
        "label": "Function",
        "importPath": "torch.autograd",
        "description": "torch.autograd",
        "isExtraImport": true,
        "detail": "torch.autograd",
        "documentation": {}
    },
    {
        "label": "model.vision.pointnet2.pytorch_utils",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "model.vision.pointnet2.pytorch_utils",
        "description": "model.vision.pointnet2.pytorch_utils",
        "detail": "model.vision.pointnet2.pytorch_utils",
        "documentation": {}
    },
    {
        "label": "glob",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "glob",
        "description": "glob",
        "detail": "glob",
        "documentation": {}
    },
    {
        "label": "os.path",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os.path",
        "description": "os.path",
        "detail": "os.path",
        "documentation": {}
    },
    {
        "label": "find_packages",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "setup",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "BuildExtension",
        "importPath": "torch.utils.cpp_extension",
        "description": "torch.utils.cpp_extension",
        "isExtraImport": true,
        "detail": "torch.utils.cpp_extension",
        "documentation": {}
    },
    {
        "label": "CUDAExtension",
        "importPath": "torch.utils.cpp_extension",
        "description": "torch.utils.cpp_extension",
        "isExtraImport": true,
        "detail": "torch.utils.cpp_extension",
        "documentation": {}
    },
    {
        "label": "einops",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "einops",
        "description": "einops",
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "get_mlp_head",
        "importPath": "model.vision.basic_modules",
        "description": "model.vision.basic_modules",
        "isExtraImport": true,
        "detail": "model.vision.basic_modules",
        "documentation": {}
    },
    {
        "label": "get_mlp_head",
        "importPath": "model.vision.basic_modules",
        "description": "model.vision.basic_modules",
        "isExtraImport": true,
        "detail": "model.vision.basic_modules",
        "documentation": {}
    },
    {
        "label": "_get_clones",
        "importPath": "model.vision.basic_modules",
        "description": "model.vision.basic_modules",
        "isExtraImport": true,
        "detail": "model.vision.basic_modules",
        "documentation": {}
    },
    {
        "label": "calc_pairwise_locs",
        "importPath": "model.vision.basic_modules",
        "description": "model.vision.basic_modules",
        "isExtraImport": true,
        "detail": "model.vision.basic_modules",
        "documentation": {}
    },
    {
        "label": "get_mlp_head",
        "importPath": "model.vision.basic_modules",
        "description": "model.vision.basic_modules",
        "isExtraImport": true,
        "detail": "model.vision.basic_modules",
        "documentation": {}
    },
    {
        "label": "init_weights",
        "importPath": "model.vision.basic_modules",
        "description": "model.vision.basic_modules",
        "isExtraImport": true,
        "detail": "model.vision.basic_modules",
        "documentation": {}
    },
    {
        "label": "get_mixup_function",
        "importPath": "model.vision.basic_modules",
        "description": "model.vision.basic_modules",
        "isExtraImport": true,
        "detail": "model.vision.basic_modules",
        "documentation": {}
    },
    {
        "label": "get_mlp_head",
        "importPath": "model.vision.basic_modules",
        "description": "model.vision.basic_modules",
        "isExtraImport": true,
        "detail": "model.vision.basic_modules",
        "documentation": {}
    },
    {
        "label": "_get_activation_fn",
        "importPath": "model.vision.basic_modules",
        "description": "model.vision.basic_modules",
        "isExtraImport": true,
        "detail": "model.vision.basic_modules",
        "documentation": {}
    },
    {
        "label": "_get_clones",
        "importPath": "model.vision.basic_modules",
        "description": "model.vision.basic_modules",
        "isExtraImport": true,
        "detail": "model.vision.basic_modules",
        "documentation": {}
    },
    {
        "label": "_get_clones",
        "importPath": "model.vision.basic_modules",
        "description": "model.vision.basic_modules",
        "isExtraImport": true,
        "detail": "model.vision.basic_modules",
        "documentation": {}
    },
    {
        "label": "init_weights",
        "importPath": "model.vision.basic_modules",
        "description": "model.vision.basic_modules",
        "isExtraImport": true,
        "detail": "model.vision.basic_modules",
        "documentation": {}
    },
    {
        "label": "generate_mm_casual_mask",
        "importPath": "model.vision.basic_modules",
        "description": "model.vision.basic_modules",
        "isExtraImport": true,
        "detail": "model.vision.basic_modules",
        "documentation": {}
    },
    {
        "label": "generate_causal_mask",
        "importPath": "model.vision.basic_modules",
        "description": "model.vision.basic_modules",
        "isExtraImport": true,
        "detail": "model.vision.basic_modules",
        "documentation": {}
    },
    {
        "label": "model.vision.pointnet2.pointnet2_utils",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "model.vision.pointnet2.pointnet2_utils",
        "description": "model.vision.pointnet2.pointnet2_utils",
        "detail": "model.vision.pointnet2.pointnet2_utils",
        "documentation": {}
    },
    {
        "label": "PointnetSAModule",
        "importPath": "model.vision.pointnet2.pointnet2_modules",
        "description": "model.vision.pointnet2.pointnet2_modules",
        "isExtraImport": true,
        "detail": "model.vision.pointnet2.pointnet2_modules",
        "documentation": {}
    },
    {
        "label": "TransformerDecoderLayer",
        "importPath": "model.vision.transformers",
        "description": "model.vision.transformers",
        "isExtraImport": true,
        "detail": "model.vision.transformers",
        "documentation": {}
    },
    {
        "label": "TransformerEncoderLayer",
        "importPath": "model.vision.transformers",
        "description": "model.vision.transformers",
        "isExtraImport": true,
        "detail": "model.vision.transformers",
        "documentation": {}
    },
    {
        "label": "TransformerSpatialDecoderLayer",
        "importPath": "model.vision.transformers",
        "description": "model.vision.transformers",
        "isExtraImport": true,
        "detail": "model.vision.transformers",
        "documentation": {}
    },
    {
        "label": "TransformerSpatialEncoderLayer",
        "importPath": "model.vision.transformers",
        "description": "model.vision.transformers",
        "isExtraImport": true,
        "detail": "model.vision.transformers",
        "documentation": {}
    },
    {
        "label": "TransformerDecoderLayer",
        "importPath": "model.vision.transformers",
        "description": "model.vision.transformers",
        "isExtraImport": true,
        "detail": "model.vision.transformers",
        "documentation": {}
    },
    {
        "label": "TransformerEncoderLayer",
        "importPath": "model.vision.transformers",
        "description": "model.vision.transformers",
        "isExtraImport": true,
        "detail": "model.vision.transformers",
        "documentation": {}
    },
    {
        "label": "TransformerSpatialDecoderLayer",
        "importPath": "model.vision.transformers",
        "description": "model.vision.transformers",
        "isExtraImport": true,
        "detail": "model.vision.transformers",
        "documentation": {}
    },
    {
        "label": "TransformerSpatialEncoderLayer",
        "importPath": "model.vision.transformers",
        "description": "model.vision.transformers",
        "isExtraImport": true,
        "detail": "model.vision.transformers",
        "documentation": {}
    },
    {
        "label": "PointnetSAModuleVotes",
        "importPath": "lib.pointnet2.pointnet2_modules",
        "description": "lib.pointnet2.pointnet2_modules",
        "isExtraImport": true,
        "detail": "lib.pointnet2.pointnet2_modules",
        "documentation": {}
    },
    {
        "label": "PointnetFPModule",
        "importPath": "lib.pointnet2.pointnet2_modules",
        "description": "lib.pointnet2.pointnet2_modules",
        "isExtraImport": true,
        "detail": "lib.pointnet2.pointnet2_modules",
        "documentation": {}
    },
    {
        "label": "PointnetSAModuleVotes",
        "importPath": "lib.pointnet2.pointnet2_modules",
        "description": "lib.pointnet2.pointnet2_modules",
        "isExtraImport": true,
        "detail": "lib.pointnet2.pointnet2_modules",
        "documentation": {}
    },
    {
        "label": "PointnetSAModuleVotes",
        "importPath": "lib.pointnet2.pointnet2_modules",
        "description": "lib.pointnet2.pointnet2_modules",
        "isExtraImport": true,
        "detail": "lib.pointnet2.pointnet2_modules",
        "documentation": {}
    },
    {
        "label": "PointnetSAModuleVotes",
        "importPath": "lib.pointnet2.pointnet2_modules",
        "description": "lib.pointnet2.pointnet2_modules",
        "isExtraImport": true,
        "detail": "lib.pointnet2.pointnet2_modules",
        "documentation": {}
    },
    {
        "label": "pack_padded_sequence",
        "importPath": "torch.nn.utils.rnn",
        "description": "torch.nn.utils.rnn",
        "isExtraImport": true,
        "detail": "torch.nn.utils.rnn",
        "documentation": {}
    },
    {
        "label": "pad_packed_sequence",
        "importPath": "torch.nn.utils.rnn",
        "description": "torch.nn.utils.rnn",
        "isExtraImport": true,
        "detail": "torch.nn.utils.rnn",
        "documentation": {}
    },
    {
        "label": "MultiHeadAttention",
        "importPath": "models.transformer.attention",
        "description": "models.transformer.attention",
        "isExtraImport": true,
        "detail": "models.transformer.attention",
        "documentation": {}
    },
    {
        "label": "MultiHeadAttention",
        "importPath": "models.transformer.attention",
        "description": "models.transformer.attention",
        "isExtraImport": true,
        "detail": "models.transformer.attention",
        "documentation": {}
    },
    {
        "label": "MultiHeadAttention",
        "importPath": "models.transformer.attention",
        "description": "models.transformer.attention",
        "isExtraImport": true,
        "detail": "models.transformer.attention",
        "documentation": {}
    },
    {
        "label": "MultiHeadAttention",
        "importPath": "models.transformer.attention",
        "description": "models.transformer.attention",
        "isExtraImport": true,
        "detail": "models.transformer.attention",
        "documentation": {}
    },
    {
        "label": "MultiHeadAttention",
        "importPath": "models.transformer.attention",
        "description": "models.transformer.attention",
        "isExtraImport": true,
        "detail": "models.transformer.attention",
        "documentation": {}
    },
    {
        "label": "MultiHeadAttention",
        "importPath": "models.transformer.attention",
        "description": "models.transformer.attention",
        "isExtraImport": true,
        "detail": "models.transformer.attention",
        "documentation": {}
    },
    {
        "label": "MultiHeadAttention",
        "importPath": "models.transformer.attention",
        "description": "models.transformer.attention",
        "isExtraImport": true,
        "detail": "models.transformer.attention",
        "documentation": {}
    },
    {
        "label": "MultiHeadAttention",
        "importPath": "models.transformer.attention",
        "description": "models.transformer.attention",
        "isExtraImport": true,
        "detail": "models.transformer.attention",
        "documentation": {}
    },
    {
        "label": "MultiHeadAttention",
        "importPath": "models.transformer.attention",
        "description": "models.transformer.attention",
        "isExtraImport": true,
        "detail": "models.transformer.attention",
        "documentation": {}
    },
    {
        "label": "Pointnet2Backbone",
        "importPath": "models.base_module.backbone_module",
        "description": "models.base_module.backbone_module",
        "isExtraImport": true,
        "detail": "models.base_module.backbone_module",
        "documentation": {}
    },
    {
        "label": "Pointnet2Backbone",
        "importPath": "models.base_module.backbone_module",
        "description": "models.base_module.backbone_module",
        "isExtraImport": true,
        "detail": "models.base_module.backbone_module",
        "documentation": {}
    },
    {
        "label": "Pointnet2Backbone",
        "importPath": "models.base_module.backbone_module",
        "description": "models.base_module.backbone_module",
        "isExtraImport": true,
        "detail": "models.base_module.backbone_module",
        "documentation": {}
    },
    {
        "label": "Pointnet2Backbone",
        "importPath": "models.base_module.backbone_module",
        "description": "models.base_module.backbone_module",
        "isExtraImport": true,
        "detail": "models.base_module.backbone_module",
        "documentation": {}
    },
    {
        "label": "Pointnet2Backbone",
        "importPath": "models.base_module.backbone_module",
        "description": "models.base_module.backbone_module",
        "isExtraImport": true,
        "detail": "models.base_module.backbone_module",
        "documentation": {}
    },
    {
        "label": "Pointnet2Backbone",
        "importPath": "models.base_module.backbone_module",
        "description": "models.base_module.backbone_module",
        "isExtraImport": true,
        "detail": "models.base_module.backbone_module",
        "documentation": {}
    },
    {
        "label": "VotingModule",
        "importPath": "models.base_module.voting_module",
        "description": "models.base_module.voting_module",
        "isExtraImport": true,
        "detail": "models.base_module.voting_module",
        "documentation": {}
    },
    {
        "label": "VotingModule",
        "importPath": "models.base_module.voting_module",
        "description": "models.base_module.voting_module",
        "isExtraImport": true,
        "detail": "models.base_module.voting_module",
        "documentation": {}
    },
    {
        "label": "VotingModule",
        "importPath": "models.base_module.voting_module",
        "description": "models.base_module.voting_module",
        "isExtraImport": true,
        "detail": "models.base_module.voting_module",
        "documentation": {}
    },
    {
        "label": "VotingModule",
        "importPath": "models.base_module.voting_module",
        "description": "models.base_module.voting_module",
        "isExtraImport": true,
        "detail": "models.base_module.voting_module",
        "documentation": {}
    },
    {
        "label": "VotingModule",
        "importPath": "models.base_module.voting_module",
        "description": "models.base_module.voting_module",
        "isExtraImport": true,
        "detail": "models.base_module.voting_module",
        "documentation": {}
    },
    {
        "label": "VotingModule",
        "importPath": "models.base_module.voting_module",
        "description": "models.base_module.voting_module",
        "isExtraImport": true,
        "detail": "models.base_module.voting_module",
        "documentation": {}
    },
    {
        "label": "ProposalModule",
        "importPath": "models.proposal_module.proposal_module_fcos",
        "description": "models.proposal_module.proposal_module_fcos",
        "isExtraImport": true,
        "detail": "models.proposal_module.proposal_module_fcos",
        "documentation": {}
    },
    {
        "label": "ProposalModule",
        "importPath": "models.proposal_module.proposal_module_fcos",
        "description": "models.proposal_module.proposal_module_fcos",
        "isExtraImport": true,
        "detail": "models.proposal_module.proposal_module_fcos",
        "documentation": {}
    },
    {
        "label": "ProposalModule",
        "importPath": "models.proposal_module.proposal_module_fcos",
        "description": "models.proposal_module.proposal_module_fcos",
        "isExtraImport": true,
        "detail": "models.proposal_module.proposal_module_fcos",
        "documentation": {}
    },
    {
        "label": "ProposalModule",
        "importPath": "models.proposal_module.proposal_module_fcos",
        "description": "models.proposal_module.proposal_module_fcos",
        "isExtraImport": true,
        "detail": "models.proposal_module.proposal_module_fcos",
        "documentation": {}
    },
    {
        "label": "ProposalModule",
        "importPath": "models.proposal_module.proposal_module_fcos",
        "description": "models.proposal_module.proposal_module_fcos",
        "isExtraImport": true,
        "detail": "models.proposal_module.proposal_module_fcos",
        "documentation": {}
    },
    {
        "label": "ProposalModule",
        "importPath": "models.proposal_module.proposal_module_fcos",
        "description": "models.proposal_module.proposal_module_fcos",
        "isExtraImport": true,
        "detail": "models.proposal_module.proposal_module_fcos",
        "documentation": {}
    },
    {
        "label": "RelationModule",
        "importPath": "models.proposal_module.relation_module",
        "description": "models.proposal_module.relation_module",
        "isExtraImport": true,
        "detail": "models.proposal_module.relation_module",
        "documentation": {}
    },
    {
        "label": "RelationModule",
        "importPath": "models.proposal_module.relation_module",
        "description": "models.proposal_module.relation_module",
        "isExtraImport": true,
        "detail": "models.proposal_module.relation_module",
        "documentation": {}
    },
    {
        "label": "RelationModule",
        "importPath": "models.proposal_module.relation_module",
        "description": "models.proposal_module.relation_module",
        "isExtraImport": true,
        "detail": "models.proposal_module.relation_module",
        "documentation": {}
    },
    {
        "label": "PositionWiseFeedForward",
        "importPath": "models.transformer.utils",
        "description": "models.transformer.utils",
        "isExtraImport": true,
        "detail": "models.transformer.utils",
        "documentation": {}
    },
    {
        "label": "PositionWiseFeedForward",
        "importPath": "models.transformer.utils",
        "description": "models.transformer.utils",
        "isExtraImport": true,
        "detail": "models.transformer.utils",
        "documentation": {}
    },
    {
        "label": "PositionWiseFeedForward",
        "importPath": "models.transformer.utils",
        "description": "models.transformer.utils",
        "isExtraImport": true,
        "detail": "models.transformer.utils",
        "documentation": {}
    },
    {
        "label": "PositionWiseFeedForward",
        "importPath": "models.transformer.utils",
        "description": "models.transformer.utils",
        "isExtraImport": true,
        "detail": "models.transformer.utils",
        "documentation": {}
    },
    {
        "label": "PositionWiseFeedForward",
        "importPath": "models.transformer.utils",
        "description": "models.transformer.utils",
        "isExtraImport": true,
        "detail": "models.transformer.utils",
        "documentation": {}
    },
    {
        "label": "PositionWiseFeedForward",
        "importPath": "models.transformer.utils",
        "description": "models.transformer.utils",
        "isExtraImport": true,
        "detail": "models.transformer.utils",
        "documentation": {}
    },
    {
        "label": "timm",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "timm",
        "description": "timm",
        "detail": "timm",
        "documentation": {}
    },
    {
        "label": "trunc_normal_",
        "importPath": "timm.models.layers",
        "description": "timm.models.layers",
        "isExtraImport": true,
        "detail": "timm.models.layers",
        "documentation": {}
    },
    {
        "label": "trunc_normal_",
        "importPath": "timm.models.layers",
        "description": "timm.models.layers",
        "isExtraImport": true,
        "detail": "timm.models.layers",
        "documentation": {}
    },
    {
        "label": "LangModule",
        "importPath": "models.base_module.lang_module",
        "description": "models.base_module.lang_module",
        "isExtraImport": true,
        "detail": "models.base_module.lang_module",
        "documentation": {}
    },
    {
        "label": "LangModule",
        "importPath": "models.base_module.lang_module",
        "description": "models.base_module.lang_module",
        "isExtraImport": true,
        "detail": "models.base_module.lang_module",
        "documentation": {}
    },
    {
        "label": "LangModule",
        "importPath": "models.base_module.lang_module",
        "description": "models.base_module.lang_module",
        "isExtraImport": true,
        "detail": "models.base_module.lang_module",
        "documentation": {}
    },
    {
        "label": "LangModule",
        "importPath": "models.base_module.lang_module",
        "description": "models.base_module.lang_module",
        "isExtraImport": true,
        "detail": "models.base_module.lang_module",
        "documentation": {}
    },
    {
        "label": "LangModule",
        "importPath": "models.base_module.lang_module",
        "description": "models.base_module.lang_module",
        "isExtraImport": true,
        "detail": "models.base_module.lang_module",
        "documentation": {}
    },
    {
        "label": "MatchModule",
        "importPath": "models.refnet.match_module",
        "description": "models.refnet.match_module",
        "isExtraImport": true,
        "detail": "models.refnet.match_module",
        "documentation": {}
    },
    {
        "label": "SceneCaptionModule",
        "importPath": "models.capnet.caption_module",
        "description": "models.capnet.caption_module",
        "isExtraImport": true,
        "detail": "models.capnet.caption_module",
        "documentation": {}
    },
    {
        "label": "TopDownSceneCaptionModule",
        "importPath": "models.capnet.caption_module",
        "description": "models.capnet.caption_module",
        "isExtraImport": true,
        "detail": "models.capnet.caption_module",
        "documentation": {}
    },
    {
        "label": "SceneCaptionModule",
        "importPath": "models.capnet.caption_module",
        "description": "models.capnet.caption_module",
        "isExtraImport": true,
        "detail": "models.capnet.caption_module",
        "documentation": {}
    },
    {
        "label": "TopDownSceneCaptionModule",
        "importPath": "models.capnet.caption_module",
        "description": "models.capnet.caption_module",
        "isExtraImport": true,
        "detail": "models.capnet.caption_module",
        "documentation": {}
    },
    {
        "label": "RelationModule",
        "importPath": "models.ft_module.relation_ft",
        "description": "models.ft_module.relation_ft",
        "isExtraImport": true,
        "detail": "models.ft_module.relation_ft",
        "documentation": {}
    },
    {
        "label": "RelationModule",
        "importPath": "models.ft_module.relation_ft",
        "description": "models.ft_module.relation_ft",
        "isExtraImport": true,
        "detail": "models.ft_module.relation_ft",
        "documentation": {}
    },
    {
        "label": "MatchModule",
        "importPath": "models.ft_module.fusion_ft",
        "description": "models.ft_module.fusion_ft",
        "isExtraImport": true,
        "detail": "models.ft_module.fusion_ft",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "models.pretrain_module.mask_utils",
        "description": "models.pretrain_module.mask_utils",
        "isExtraImport": true,
        "detail": "models.pretrain_module.mask_utils",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "models.pretrain_module.mask_utils",
        "description": "models.pretrain_module.mask_utils",
        "isExtraImport": true,
        "detail": "models.pretrain_module.mask_utils",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "models.pretrain_module.mask_utils",
        "description": "models.pretrain_module.mask_utils",
        "isExtraImport": true,
        "detail": "models.pretrain_module.mask_utils",
        "documentation": {}
    },
    {
        "label": "MCAN_ED",
        "importPath": "models.qa_module.mcan_module",
        "description": "models.qa_module.mcan_module",
        "isExtraImport": true,
        "detail": "models.qa_module.mcan_module",
        "documentation": {}
    },
    {
        "label": "AttFlat",
        "importPath": "models.qa_module.mcan_module",
        "description": "models.qa_module.mcan_module",
        "isExtraImport": true,
        "detail": "models.qa_module.mcan_module",
        "documentation": {}
    },
    {
        "label": "LayerNorm",
        "importPath": "models.qa_module.mcan_module",
        "description": "models.qa_module.mcan_module",
        "isExtraImport": true,
        "detail": "models.qa_module.mcan_module",
        "documentation": {}
    },
    {
        "label": "RelationModule",
        "importPath": "models.pretrain_module.relation_module",
        "description": "models.pretrain_module.relation_module",
        "isExtraImport": true,
        "detail": "models.pretrain_module.relation_module",
        "documentation": {}
    },
    {
        "label": "MatchModule",
        "importPath": "models.pretrain_module.fusion_module",
        "description": "models.pretrain_module.fusion_module",
        "isExtraImport": true,
        "detail": "models.pretrain_module.fusion_module",
        "documentation": {}
    },
    {
        "label": "copy_params",
        "importPath": "models.base_module.ema_utils",
        "description": "models.base_module.ema_utils",
        "isExtraImport": true,
        "detail": "models.base_module.ema_utils",
        "documentation": {}
    },
    {
        "label": "_momentum_update",
        "importPath": "models.base_module.ema_utils",
        "description": "models.base_module.ema_utils",
        "isExtraImport": true,
        "detail": "models.base_module.ema_utils",
        "documentation": {}
    },
    {
        "label": "copy_params",
        "importPath": "models.base_module.ema_utils",
        "description": "models.base_module.ema_utils",
        "isExtraImport": true,
        "detail": "models.base_module.ema_utils",
        "documentation": {}
    },
    {
        "label": "_momentum_update",
        "importPath": "models.base_module.ema_utils",
        "description": "models.base_module.ema_utils",
        "isExtraImport": true,
        "detail": "models.base_module.ema_utils",
        "documentation": {}
    },
    {
        "label": "PointnetSAModuleVotes",
        "importPath": "pointnet2_modules",
        "description": "pointnet2_modules",
        "isExtraImport": true,
        "detail": "pointnet2_modules",
        "documentation": {}
    },
    {
        "label": "PointnetSAMoudleAgg",
        "importPath": "pointnet2_modules",
        "description": "pointnet2_modules",
        "isExtraImport": true,
        "detail": "pointnet2_modules",
        "documentation": {}
    },
    {
        "label": "lib.pointnet2.pointnet2_utils",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "lib.pointnet2.pointnet2_utils",
        "description": "lib.pointnet2.pointnet2_utils",
        "detail": "lib.pointnet2.pointnet2_utils",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "StandardROIHeads",
        "importPath": "models.proposal_module.ROI_heads.roi_heads",
        "description": "models.proposal_module.ROI_heads.roi_heads",
        "isExtraImport": true,
        "detail": "models.proposal_module.ROI_heads.roi_heads",
        "documentation": {}
    },
    {
        "label": "StandardROIHeads_qa",
        "importPath": "models.proposal_module.ROI_heads.roi_heads",
        "description": "models.proposal_module.ROI_heads.roi_heads",
        "isExtraImport": true,
        "detail": "models.proposal_module.ROI_heads.roi_heads",
        "documentation": {}
    },
    {
        "label": "ABC",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "LambdaLR",
        "importPath": "torch.optim.lr_scheduler",
        "description": "torch.optim.lr_scheduler",
        "isExtraImport": true,
        "detail": "torch.optim.lr_scheduler",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "pipeline.pipeline_mixin",
        "description": "pipeline.pipeline_mixin",
        "isExtraImport": true,
        "detail": "pipeline.pipeline_mixin",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "pipeline.pipeline",
        "description": "pipeline.pipeline",
        "isExtraImport": true,
        "detail": "pipeline.pipeline",
        "documentation": {}
    },
    {
        "label": "yaml",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "yaml",
        "description": "yaml",
        "detail": "yaml",
        "documentation": {}
    },
    {
        "label": "Cider",
        "importPath": "utils.cider.cider",
        "description": "utils.cider.cider",
        "isExtraImport": true,
        "detail": "utils.cider.cider",
        "documentation": {}
    },
    {
        "label": "Bleu",
        "importPath": "utils.bleu.bleu",
        "description": "utils.bleu.bleu",
        "isExtraImport": true,
        "detail": "utils.bleu.bleu",
        "documentation": {}
    },
    {
        "label": "Meteor",
        "importPath": "utils.meteor.meteor",
        "description": "utils.meteor.meteor",
        "isExtraImport": true,
        "detail": "utils.meteor.meteor",
        "documentation": {}
    },
    {
        "label": "Rouge",
        "importPath": "utils.rouge.rouge",
        "description": "utils.rouge.rouge",
        "isExtraImport": true,
        "detail": "utils.rouge.rouge",
        "documentation": {}
    },
    {
        "label": "GreedySearch",
        "importPath": "utils.caption_search",
        "description": "utils.caption_search",
        "isExtraImport": true,
        "detail": "utils.caption_search",
        "documentation": {}
    },
    {
        "label": "BleuScorer",
        "importPath": "utils.bleu.bleu_scorer",
        "description": "utils.bleu.bleu_scorer",
        "isExtraImport": true,
        "detail": "utils.bleu.bleu_scorer",
        "documentation": {}
    },
    {
        "label": "sys,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys.",
        "description": "sys.",
        "detail": "sys.",
        "documentation": {}
    },
    {
        "label": "CiderScorer",
        "importPath": "utils.cider.cider_scorer",
        "description": "utils.cider.cider_scorer",
        "isExtraImport": true,
        "detail": "utils.cider.cider_scorer",
        "documentation": {}
    },
    {
        "label": "threading",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "threading",
        "description": "threading",
        "detail": "threading",
        "documentation": {}
    },
    {
        "label": "pycocoevalcap",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pycocoevalcap",
        "description": "pycocoevalcap",
        "detail": "pycocoevalcap",
        "documentation": {}
    },
    {
        "label": "Bleu",
        "importPath": "pycocoevalcap.bleu.bleu",
        "description": "pycocoevalcap.bleu.bleu",
        "isExtraImport": true,
        "detail": "pycocoevalcap.bleu.bleu",
        "documentation": {}
    },
    {
        "label": "Meteor",
        "importPath": "pycocoevalcap.meteor.meteor",
        "description": "pycocoevalcap.meteor.meteor",
        "isExtraImport": true,
        "detail": "pycocoevalcap.meteor.meteor",
        "documentation": {}
    },
    {
        "label": "Rouge",
        "importPath": "pycocoevalcap.rouge.rouge",
        "description": "pycocoevalcap.rouge.rouge",
        "isExtraImport": true,
        "detail": "pycocoevalcap.rouge.rouge",
        "documentation": {}
    },
    {
        "label": "Cider",
        "importPath": "pycocoevalcap.cider.cider",
        "description": "pycocoevalcap.cider.cider",
        "isExtraImport": true,
        "detail": "pycocoevalcap.cider.cider",
        "documentation": {}
    },
    {
        "label": "PTBTokenizer",
        "importPath": "pycocoevalcap.tokenizer.ptbtokenizer",
        "description": "pycocoevalcap.tokenizer.ptbtokenizer",
        "isExtraImport": true,
        "detail": "pycocoevalcap.tokenizer.ptbtokenizer",
        "documentation": {}
    },
    {
        "label": "SummaryWriter",
        "importPath": "torch.utils.tensorboard",
        "description": "torch.utils.tensorboard",
        "isExtraImport": true,
        "detail": "torch.utils.tensorboard",
        "documentation": {}
    },
    {
        "label": "wandb",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "wandb",
        "description": "wandb",
        "detail": "wandb",
        "documentation": {}
    },
    {
        "label": "collections.abc",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "collections.abc",
        "description": "collections.abc",
        "detail": "collections.abc",
        "documentation": {}
    },
    {
        "label": "pipeline_factory",
        "importPath": "pipeline.pipeline_factory",
        "description": "pipeline.pipeline_factory",
        "isExtraImport": true,
        "detail": "pipeline.pipeline_factory",
        "documentation": {}
    },
    {
        "label": "get_dataloader",
        "kind": 2,
        "importPath": "3D-VLP.scripts.eval_scripts.caption_eval",
        "description": "3D-VLP.scripts.eval_scripts.caption_eval",
        "peekOfCode": "def get_dataloader(args, scanrefer, scanrefer_new, all_scene_list, config):\n    dataset = ScannetReferenceDataset(\n        scanrefer=scanrefer,\n        scanrefer_new=scanrefer_new,\n        scanrefer_all_scene=all_scene_list,  \n        split=\"val\",\n        name=args.dataset,\n        num_points=args.num_points, \n        use_height=(not args.no_height),\n        use_color=args.use_color, ",
        "detail": "3D-VLP.scripts.eval_scripts.caption_eval",
        "documentation": {}
    },
    {
        "label": "get_model",
        "kind": 2,
        "importPath": "3D-VLP.scripts.eval_scripts.caption_eval",
        "description": "3D-VLP.scripts.eval_scripts.caption_eval",
        "peekOfCode": "def get_model(args, dataset, device, root=CONF.PATH.OUTPUT, eval_pretrained=False):\n    # initiate model\n    input_channels = int(args.use_multiview) * 128 + int(args.use_normal) * 3 + int(args.use_color) * 3 + int(not args.no_height)\n    model = JointNet(\n        num_class=DC.num_class,\n        vocabulary=dataset.vocabulary,\n        embeddings=dataset.glove,\n        num_heading_bin=DC.num_heading_bin,\n        num_size_cluster=DC.num_size_cluster,\n        mean_size_arr=DC.mean_size_arr,",
        "detail": "3D-VLP.scripts.eval_scripts.caption_eval",
        "documentation": {}
    },
    {
        "label": "get_scannet_scene_list",
        "kind": 2,
        "importPath": "3D-VLP.scripts.eval_scripts.caption_eval",
        "description": "3D-VLP.scripts.eval_scripts.caption_eval",
        "peekOfCode": "def get_scannet_scene_list(data):\n    # scene_list = sorted([line.rstrip() for line in open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_{}.txt\".format(split)))])\n    scene_list = sorted(list(set([d[\"scene_id\"] for d in data])))\n    return scene_list\ndef get_eval_data(args):\n    if args.dataset == \"ScanRefer\":\n        scanrefer_train = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_train.json\")))\n        scanrefer_val = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_val.json\")))\n    elif args.dataset == \"ReferIt3D\":\n        scanrefer_train = json.load(open(os.path.join(CONF.PATH.DATA, \"nr3d_train.json\")))",
        "detail": "3D-VLP.scripts.eval_scripts.caption_eval",
        "documentation": {}
    },
    {
        "label": "get_eval_data",
        "kind": 2,
        "importPath": "3D-VLP.scripts.eval_scripts.caption_eval",
        "description": "3D-VLP.scripts.eval_scripts.caption_eval",
        "peekOfCode": "def get_eval_data(args):\n    if args.dataset == \"ScanRefer\":\n        scanrefer_train = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_train.json\")))\n        scanrefer_val = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_val.json\")))\n    elif args.dataset == \"ReferIt3D\":\n        scanrefer_train = json.load(open(os.path.join(CONF.PATH.DATA, \"nr3d_train.json\")))\n        scanrefer_val = json.load(open(os.path.join(CONF.PATH.DATA, \"nr3d_val.json\")))\n    else:\n        raise ValueError(\"Invalid dataset.\")\n    eval_scene_list = get_scannet_scene_list(scanrefer_train) if args.use_train else get_scannet_scene_list(scanrefer_val)",
        "detail": "3D-VLP.scripts.eval_scripts.caption_eval",
        "documentation": {}
    },
    {
        "label": "eval_caption",
        "kind": 2,
        "importPath": "3D-VLP.scripts.eval_scripts.caption_eval",
        "description": "3D-VLP.scripts.eval_scripts.caption_eval",
        "peekOfCode": "def eval_caption(args):\n    log_path = os.path.join(CONF.PATH.OUTPUT, args.folder, \"log_eval_cap.txt\")\n    log_fout = open(log_path, \"a\")\n    log_fout.write('---------------------------------------------' + \"\\n\")\n    log_fout.flush()\n    print(\"initializing...\")\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    # get eval data\n    scanrefer_eval, eval_scene_list, scanrefer_eval_new = get_eval_data(args)\n    # get dataloader",
        "detail": "3D-VLP.scripts.eval_scripts.caption_eval",
        "documentation": {}
    },
    {
        "label": "eval_detection",
        "kind": 2,
        "importPath": "3D-VLP.scripts.eval_scripts.caption_eval",
        "description": "3D-VLP.scripts.eval_scripts.caption_eval",
        "peekOfCode": "def eval_detection(args):\n    print(\"evaluate detection...\")\n    # constant\n    DC = ScannetDatasetConfig()\n    # init training dataset\n    print(\"preparing data...\")\n    # get eval data\n    scanrefer_eval, eval_scene_list, scanrefer_eval_new = get_eval_data(args)\n    # get dataloader\n    dataset, dataloader = get_dataloader(args, scanrefer_eval, scanrefer_eval_new, eval_scene_list, DC)",
        "detail": "3D-VLP.scripts.eval_scripts.caption_eval",
        "documentation": {}
    },
    {
        "label": "DC",
        "kind": 5,
        "importPath": "3D-VLP.scripts.eval_scripts.caption_eval",
        "description": "3D-VLP.scripts.eval_scripts.caption_eval",
        "peekOfCode": "DC = ScannetDatasetConfig()\ndef get_dataloader(args, scanrefer, scanrefer_new, all_scene_list, config):\n    dataset = ScannetReferenceDataset(\n        scanrefer=scanrefer,\n        scanrefer_new=scanrefer_new,\n        scanrefer_all_scene=all_scene_list,  \n        split=\"val\",\n        name=args.dataset,\n        num_points=args.num_points, \n        use_height=(not args.no_height),",
        "detail": "3D-VLP.scripts.eval_scripts.caption_eval",
        "documentation": {}
    },
    {
        "label": "get_dataloader",
        "kind": 2,
        "importPath": "3D-VLP.scripts.eval_scripts.caption_visualize",
        "description": "3D-VLP.scripts.eval_scripts.caption_visualize",
        "peekOfCode": "def get_dataloader(args, scanrefer, scanrefer_new, all_scene_list, config):\n    dataset = ScannetReferenceDataset(\n        scanrefer=scanrefer,\n        scanrefer_new=scanrefer_new,\n        scanrefer_all_scene=all_scene_list,\n        split=\"val\",\n        name=args.dataset,\n        num_points=args.num_points,\n        use_height=(not args.no_height),\n        use_color=args.use_color,",
        "detail": "3D-VLP.scripts.eval_scripts.caption_visualize",
        "documentation": {}
    },
    {
        "label": "get_model",
        "kind": 2,
        "importPath": "3D-VLP.scripts.eval_scripts.caption_visualize",
        "description": "3D-VLP.scripts.eval_scripts.caption_visualize",
        "peekOfCode": "def get_model(args, dataset, root=CONF.PATH.OUTPUT):\n    # initiate model\n    input_channels = int(args.use_multiview) * 128 + int(args.use_normal) * 3 + int(args.use_color) * 3 + int(not args.no_height)\n    model = JointNet(\n        num_class=DC.num_class,\n        vocabulary=dataset.vocabulary,\n        embeddings=dataset.glove,\n        num_heading_bin=DC.num_heading_bin,\n        num_size_cluster=DC.num_size_cluster,\n        mean_size_arr=DC.mean_size_arr,",
        "detail": "3D-VLP.scripts.eval_scripts.caption_visualize",
        "documentation": {}
    },
    {
        "label": "get_scannet_scene_list",
        "kind": 2,
        "importPath": "3D-VLP.scripts.eval_scripts.caption_visualize",
        "description": "3D-VLP.scripts.eval_scripts.caption_visualize",
        "peekOfCode": "def get_scannet_scene_list(split):\n    scene_list = sorted([line.rstrip() for line in open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_{}.txt\".format(split)))])\n    return scene_list\ndef get_eval_data(args):\n    eval_scene_list = get_scannet_scene_list(\"val\") if args.scene_id == \"-1\" else [args.scene_id]\n    scanrefer_eval = []\n    for scene_id in eval_scene_list:\n        data = deepcopy(SCANREFER_VAL[0])\n        data[\"scene_id\"] = scene_id\n        scanrefer_eval.append(data)",
        "detail": "3D-VLP.scripts.eval_scripts.caption_visualize",
        "documentation": {}
    },
    {
        "label": "get_eval_data",
        "kind": 2,
        "importPath": "3D-VLP.scripts.eval_scripts.caption_visualize",
        "description": "3D-VLP.scripts.eval_scripts.caption_visualize",
        "peekOfCode": "def get_eval_data(args):\n    eval_scene_list = get_scannet_scene_list(\"val\") if args.scene_id == \"-1\" else [args.scene_id]\n    scanrefer_eval = []\n    for scene_id in eval_scene_list:\n        data = deepcopy(SCANREFER_VAL[0])\n        data[\"scene_id\"] = scene_id\n        scanrefer_eval.append(data)\n    scanrefer_eval = []\n    scanrefer_eval_new = []\n    for scene_id in eval_scene_list:",
        "detail": "3D-VLP.scripts.eval_scripts.caption_visualize",
        "documentation": {}
    },
    {
        "label": "decode_caption",
        "kind": 2,
        "importPath": "3D-VLP.scripts.eval_scripts.caption_visualize",
        "description": "3D-VLP.scripts.eval_scripts.caption_visualize",
        "peekOfCode": "def decode_caption(raw_caption, idx2word):\n    decoded = [\"sos\"]\n    for token_idx in raw_caption:\n        token_idx = token_idx.item()\n        token = idx2word[str(token_idx)]\n        decoded.append(token)\n        if token == \"eos\": break\n    if \"eos\" not in decoded: decoded.append(\"eos\")\n    decoded = \" \".join(decoded)\n    return decoded",
        "detail": "3D-VLP.scripts.eval_scripts.caption_visualize",
        "documentation": {}
    },
    {
        "label": "write_ply",
        "kind": 2,
        "importPath": "3D-VLP.scripts.eval_scripts.caption_visualize",
        "description": "3D-VLP.scripts.eval_scripts.caption_visualize",
        "peekOfCode": "def write_ply(verts, colors, indices, output_file):\n    if colors is None:\n        colors = np.zeros_like(verts)\n    if indices is None:\n        indices = []\n    file = open(output_file, 'w')\n    file.write('ply \\n')\n    file.write('format ascii 1.0\\n')\n    file.write('element vertex {:d}\\n'.format(len(verts)))\n    file.write('property float x\\n')",
        "detail": "3D-VLP.scripts.eval_scripts.caption_visualize",
        "documentation": {}
    },
    {
        "label": "write_bbox",
        "kind": 2,
        "importPath": "3D-VLP.scripts.eval_scripts.caption_visualize",
        "description": "3D-VLP.scripts.eval_scripts.caption_visualize",
        "peekOfCode": "def write_bbox(corners, color, output_file):\n    \"\"\"\n    bbox: (cx, cy, cz, lx, ly, lz, r), center and length in three axis, the last is the rotation\n    output_file: string\n    \"\"\"\n    def create_cylinder_mesh(radius, p0, p1, stacks=10, slices=10):\n        import math\n        def compute_length_vec3(vec3):\n            return math.sqrt(vec3[0]*vec3[0] + vec3[1]*vec3[1] + vec3[2]*vec3[2])\n        def rotation(axis, angle):",
        "detail": "3D-VLP.scripts.eval_scripts.caption_visualize",
        "documentation": {}
    },
    {
        "label": "visualize",
        "kind": 2,
        "importPath": "3D-VLP.scripts.eval_scripts.caption_visualize",
        "description": "3D-VLP.scripts.eval_scripts.caption_visualize",
        "peekOfCode": "def visualize(args):\n    print(\"initializing...\")\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    # get eval data\n    scanrefer_eval, eval_scene_list, scanrefer_eval_new = get_eval_data(args)\n    # get dataloader\n    dataset, dataloader = get_dataloader(args, scanrefer_eval, scanrefer_eval_new, eval_scene_list, DC)\n    # get model\n    model = get_model(args, dataset)\n    object_id_to_object_name = {}",
        "detail": "3D-VLP.scripts.eval_scripts.caption_visualize",
        "documentation": {}
    },
    {
        "label": "SCANNET_MESH",
        "kind": 5,
        "importPath": "3D-VLP.scripts.eval_scripts.caption_visualize",
        "description": "3D-VLP.scripts.eval_scripts.caption_visualize",
        "peekOfCode": "SCANNET_MESH = os.path.join(CONF.PATH.AXIS_ALIGNED_MESH, \"{}\", \"axis_aligned_scene.ply\")\nSCANNET_AGGR = os.path.join(CONF.PATH.SCANNET_SCANS, \"{}/{}_vh_clean.aggregation.json\") # scene_id, scene_id\nSCANREFER_TRAIN = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_train.json\")))\nSCANREFER_VAL = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_val.json\")))\nSCANREFER_ORGANIZED = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_organized.json\")))\n# VOTENET_DATABASE = h5py.File(os.path.join(CONF.PATH.VOTENET_FEATURES, \"val.hdf5\"), \"r\", libver=\"latest\")\nVOTENET_DATABASE = h5py.File((\"/mnt/24t/JZ/Scan2Cap/gt_ScanRefer_features/val.hdf5\"), \"r\", libver=\"latest\")\n# constants\nDC = ScannetDatasetConfig()\ndef get_dataloader(args, scanrefer, scanrefer_new, all_scene_list, config):",
        "detail": "3D-VLP.scripts.eval_scripts.caption_visualize",
        "documentation": {}
    },
    {
        "label": "SCANNET_AGGR",
        "kind": 5,
        "importPath": "3D-VLP.scripts.eval_scripts.caption_visualize",
        "description": "3D-VLP.scripts.eval_scripts.caption_visualize",
        "peekOfCode": "SCANNET_AGGR = os.path.join(CONF.PATH.SCANNET_SCANS, \"{}/{}_vh_clean.aggregation.json\") # scene_id, scene_id\nSCANREFER_TRAIN = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_train.json\")))\nSCANREFER_VAL = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_val.json\")))\nSCANREFER_ORGANIZED = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_organized.json\")))\n# VOTENET_DATABASE = h5py.File(os.path.join(CONF.PATH.VOTENET_FEATURES, \"val.hdf5\"), \"r\", libver=\"latest\")\nVOTENET_DATABASE = h5py.File((\"/mnt/24t/JZ/Scan2Cap/gt_ScanRefer_features/val.hdf5\"), \"r\", libver=\"latest\")\n# constants\nDC = ScannetDatasetConfig()\ndef get_dataloader(args, scanrefer, scanrefer_new, all_scene_list, config):\n    dataset = ScannetReferenceDataset(",
        "detail": "3D-VLP.scripts.eval_scripts.caption_visualize",
        "documentation": {}
    },
    {
        "label": "SCANREFER_TRAIN",
        "kind": 5,
        "importPath": "3D-VLP.scripts.eval_scripts.caption_visualize",
        "description": "3D-VLP.scripts.eval_scripts.caption_visualize",
        "peekOfCode": "SCANREFER_TRAIN = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_train.json\")))\nSCANREFER_VAL = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_val.json\")))\nSCANREFER_ORGANIZED = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_organized.json\")))\n# VOTENET_DATABASE = h5py.File(os.path.join(CONF.PATH.VOTENET_FEATURES, \"val.hdf5\"), \"r\", libver=\"latest\")\nVOTENET_DATABASE = h5py.File((\"/mnt/24t/JZ/Scan2Cap/gt_ScanRefer_features/val.hdf5\"), \"r\", libver=\"latest\")\n# constants\nDC = ScannetDatasetConfig()\ndef get_dataloader(args, scanrefer, scanrefer_new, all_scene_list, config):\n    dataset = ScannetReferenceDataset(\n        scanrefer=scanrefer,",
        "detail": "3D-VLP.scripts.eval_scripts.caption_visualize",
        "documentation": {}
    },
    {
        "label": "SCANREFER_VAL",
        "kind": 5,
        "importPath": "3D-VLP.scripts.eval_scripts.caption_visualize",
        "description": "3D-VLP.scripts.eval_scripts.caption_visualize",
        "peekOfCode": "SCANREFER_VAL = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_val.json\")))\nSCANREFER_ORGANIZED = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_organized.json\")))\n# VOTENET_DATABASE = h5py.File(os.path.join(CONF.PATH.VOTENET_FEATURES, \"val.hdf5\"), \"r\", libver=\"latest\")\nVOTENET_DATABASE = h5py.File((\"/mnt/24t/JZ/Scan2Cap/gt_ScanRefer_features/val.hdf5\"), \"r\", libver=\"latest\")\n# constants\nDC = ScannetDatasetConfig()\ndef get_dataloader(args, scanrefer, scanrefer_new, all_scene_list, config):\n    dataset = ScannetReferenceDataset(\n        scanrefer=scanrefer,\n        scanrefer_new=scanrefer_new,",
        "detail": "3D-VLP.scripts.eval_scripts.caption_visualize",
        "documentation": {}
    },
    {
        "label": "SCANREFER_ORGANIZED",
        "kind": 5,
        "importPath": "3D-VLP.scripts.eval_scripts.caption_visualize",
        "description": "3D-VLP.scripts.eval_scripts.caption_visualize",
        "peekOfCode": "SCANREFER_ORGANIZED = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_organized.json\")))\n# VOTENET_DATABASE = h5py.File(os.path.join(CONF.PATH.VOTENET_FEATURES, \"val.hdf5\"), \"r\", libver=\"latest\")\nVOTENET_DATABASE = h5py.File((\"/mnt/24t/JZ/Scan2Cap/gt_ScanRefer_features/val.hdf5\"), \"r\", libver=\"latest\")\n# constants\nDC = ScannetDatasetConfig()\ndef get_dataloader(args, scanrefer, scanrefer_new, all_scene_list, config):\n    dataset = ScannetReferenceDataset(\n        scanrefer=scanrefer,\n        scanrefer_new=scanrefer_new,\n        scanrefer_all_scene=all_scene_list,",
        "detail": "3D-VLP.scripts.eval_scripts.caption_visualize",
        "documentation": {}
    },
    {
        "label": "VOTENET_DATABASE",
        "kind": 5,
        "importPath": "3D-VLP.scripts.eval_scripts.caption_visualize",
        "description": "3D-VLP.scripts.eval_scripts.caption_visualize",
        "peekOfCode": "VOTENET_DATABASE = h5py.File((\"/mnt/24t/JZ/Scan2Cap/gt_ScanRefer_features/val.hdf5\"), \"r\", libver=\"latest\")\n# constants\nDC = ScannetDatasetConfig()\ndef get_dataloader(args, scanrefer, scanrefer_new, all_scene_list, config):\n    dataset = ScannetReferenceDataset(\n        scanrefer=scanrefer,\n        scanrefer_new=scanrefer_new,\n        scanrefer_all_scene=all_scene_list,\n        split=\"val\",\n        name=args.dataset,",
        "detail": "3D-VLP.scripts.eval_scripts.caption_visualize",
        "documentation": {}
    },
    {
        "label": "DC",
        "kind": 5,
        "importPath": "3D-VLP.scripts.eval_scripts.caption_visualize",
        "description": "3D-VLP.scripts.eval_scripts.caption_visualize",
        "peekOfCode": "DC = ScannetDatasetConfig()\ndef get_dataloader(args, scanrefer, scanrefer_new, all_scene_list, config):\n    dataset = ScannetReferenceDataset(\n        scanrefer=scanrefer,\n        scanrefer_new=scanrefer_new,\n        scanrefer_all_scene=all_scene_list,\n        split=\"val\",\n        name=args.dataset,\n        num_points=args.num_points,\n        use_height=(not args.no_height),",
        "detail": "3D-VLP.scripts.eval_scripts.caption_visualize",
        "documentation": {}
    },
    {
        "label": "get_dataloader",
        "kind": 2,
        "importPath": "3D-VLP.scripts.eval_scripts.ground_eval",
        "description": "3D-VLP.scripts.eval_scripts.ground_eval",
        "peekOfCode": "def get_dataloader(args, scanrefer, scanrefer_new, all_scene_list, split, config):\n    dataset = ScannetReferenceDataset(\n        scanrefer=scanrefer,\n        scanrefer_new=scanrefer_new,\n        scanrefer_all_scene=all_scene_list, \n        split=split,\n        name=args.dataset,\n        num_points=args.num_points, \n        use_color=args.use_color, \n        use_height=(not args.no_height),",
        "detail": "3D-VLP.scripts.eval_scripts.ground_eval",
        "documentation": {}
    },
    {
        "label": "get_model",
        "kind": 2,
        "importPath": "3D-VLP.scripts.eval_scripts.ground_eval",
        "description": "3D-VLP.scripts.eval_scripts.ground_eval",
        "peekOfCode": "def get_model(args, DC, dataset):\n    # load model\n    input_channels = int(args.use_multiview) * 128 + int(args.use_normal) * 3 + int(args.use_color) * 3 + int(not args.no_height)\n    model = JointNet(\n        num_class=DC.num_class,\n        vocabulary=dataset.vocabulary,\n        embeddings=dataset.glove,\n        num_heading_bin=DC.num_heading_bin,\n        num_size_cluster=DC.num_size_cluster,\n        mean_size_arr=DC.mean_size_arr,",
        "detail": "3D-VLP.scripts.eval_scripts.ground_eval",
        "documentation": {}
    },
    {
        "label": "get_scannet_scene_list",
        "kind": 2,
        "importPath": "3D-VLP.scripts.eval_scripts.ground_eval",
        "description": "3D-VLP.scripts.eval_scripts.ground_eval",
        "peekOfCode": "def get_scannet_scene_list(split):\n    scene_list = sorted([line.rstrip() for line in open(os.path.join(CONF.PATH.SCANNET_META, \"scannetv2_{}.txt\".format(split)))])\n    return scene_list\ndef get_scanrefer(args):\n    if args.detection:\n        scene_list = get_scannet_scene_list(\"val\")\n        scanrefer = []\n        for scene_id in scene_list:\n            data = deepcopy(SCANREFER_TRAIN[0])\n            data[\"scene_id\"] = scene_id",
        "detail": "3D-VLP.scripts.eval_scripts.ground_eval",
        "documentation": {}
    },
    {
        "label": "get_scanrefer",
        "kind": 2,
        "importPath": "3D-VLP.scripts.eval_scripts.ground_eval",
        "description": "3D-VLP.scripts.eval_scripts.ground_eval",
        "peekOfCode": "def get_scanrefer(args):\n    if args.detection:\n        scene_list = get_scannet_scene_list(\"val\")\n        scanrefer = []\n        for scene_id in scene_list:\n            data = deepcopy(SCANREFER_TRAIN[0])\n            data[\"scene_id\"] = scene_id\n            scanrefer.append(data)\n    else:\n        scanrefer = SCANREFER_TRAIN if args.use_train else SCANREFER_VAL",
        "detail": "3D-VLP.scripts.eval_scripts.ground_eval",
        "documentation": {}
    },
    {
        "label": "eval_ref",
        "kind": 2,
        "importPath": "3D-VLP.scripts.eval_scripts.ground_eval",
        "description": "3D-VLP.scripts.eval_scripts.ground_eval",
        "peekOfCode": "def eval_ref(args):\n    log_path = os.path.join(CONF.PATH.OUTPUT, args.folder, \"log_eval.txt\")\n    log_fout = open(log_path, \"a\")\n    log_fout.write('---------------------------------------------' + \"\\n\")\n    log_fout.flush()\n    print(\"evaluate localization...\")\n    # constant\n    DC = ScannetDatasetConfig()\n    # init training dataset\n    print(\"preparing data...\")",
        "detail": "3D-VLP.scripts.eval_scripts.ground_eval",
        "documentation": {}
    },
    {
        "label": "eval_det",
        "kind": 2,
        "importPath": "3D-VLP.scripts.eval_scripts.ground_eval",
        "description": "3D-VLP.scripts.eval_scripts.ground_eval",
        "peekOfCode": "def eval_det(args):\n    print(\"evaluate detection...\")\n    # constant\n    DC = ScannetDatasetConfig()\n    # init training dataset\n    print(\"preparing data...\")\n    scanrefer, scene_list = get_scanrefer(args)\n    # dataloader\n    _, dataloader = get_dataloader(args, scanrefer, scene_list, \"val\", DC)\n    # model",
        "detail": "3D-VLP.scripts.eval_scripts.ground_eval",
        "documentation": {}
    },
    {
        "label": "SCANREFER_TRAIN",
        "kind": 5,
        "importPath": "3D-VLP.scripts.eval_scripts.ground_eval",
        "description": "3D-VLP.scripts.eval_scripts.ground_eval",
        "peekOfCode": "SCANREFER_TRAIN = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_train.json\")))\nSCANREFER_VAL = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_val.json\")))\n# SCANREFER_VAL = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_test.json\")))\ndef get_dataloader(args, scanrefer, scanrefer_new, all_scene_list, split, config):\n    dataset = ScannetReferenceDataset(\n        scanrefer=scanrefer,\n        scanrefer_new=scanrefer_new,\n        scanrefer_all_scene=all_scene_list, \n        split=split,\n        name=args.dataset,",
        "detail": "3D-VLP.scripts.eval_scripts.ground_eval",
        "documentation": {}
    },
    {
        "label": "SCANREFER_VAL",
        "kind": 5,
        "importPath": "3D-VLP.scripts.eval_scripts.ground_eval",
        "description": "3D-VLP.scripts.eval_scripts.ground_eval",
        "peekOfCode": "SCANREFER_VAL = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_val.json\")))\n# SCANREFER_VAL = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_test.json\")))\ndef get_dataloader(args, scanrefer, scanrefer_new, all_scene_list, split, config):\n    dataset = ScannetReferenceDataset(\n        scanrefer=scanrefer,\n        scanrefer_new=scanrefer_new,\n        scanrefer_all_scene=all_scene_list, \n        split=split,\n        name=args.dataset,\n        num_points=args.num_points, ",
        "detail": "3D-VLP.scripts.eval_scripts.ground_eval",
        "documentation": {}
    },
    {
        "label": "get_dataloader",
        "kind": 2,
        "importPath": "3D-VLP.scripts.eval_scripts.ground_predict",
        "description": "3D-VLP.scripts.eval_scripts.ground_predict",
        "peekOfCode": "def get_dataloader(args, scanrefer, scanrefer_new, all_scene_list, split, config):\n    dataset = ScannetReferenceDataset(\n        scanrefer=scanrefer,\n        scanrefer_new=scanrefer_new,\n        scanrefer_all_scene=all_scene_list,\n        split=split,\n        name=args.dataset,\n        num_points=args.num_points,\n        use_color=args.use_color,\n        use_height=(not args.no_height),",
        "detail": "3D-VLP.scripts.eval_scripts.ground_predict",
        "documentation": {}
    },
    {
        "label": "get_model",
        "kind": 2,
        "importPath": "3D-VLP.scripts.eval_scripts.ground_predict",
        "description": "3D-VLP.scripts.eval_scripts.ground_predict",
        "peekOfCode": "def get_model(args, DC, dataset):\n    # load model\n    input_channels = int(args.use_multiview) * 128 + int(args.use_normal) * 3 + int(args.use_color) * 3 + int(not args.no_height)\n    model = JointNet(\n        num_class=DC.num_class,\n        vocabulary=dataset.vocabulary,\n        embeddings=dataset.glove,\n        num_heading_bin=DC.num_heading_bin,\n        num_size_cluster=DC.num_size_cluster,\n        mean_size_arr=DC.mean_size_arr,",
        "detail": "3D-VLP.scripts.eval_scripts.ground_predict",
        "documentation": {}
    },
    {
        "label": "get_scannet_scene_list",
        "kind": 2,
        "importPath": "3D-VLP.scripts.eval_scripts.ground_predict",
        "description": "3D-VLP.scripts.eval_scripts.ground_predict",
        "peekOfCode": "def get_scannet_scene_list(split):\n    scene_list = sorted([line.rstrip() for line in open(os.path.join(CONF.PATH.SCANNET_META, \"scannetv2_{}.txt\".format(split)))])\n    return scene_list\ndef get_scanrefer(args):\n    scanrefer = SCANREFER_TEST\n    scene_list = sorted(list(set([data[\"scene_id\"] for data in scanrefer])))\n    scanrefer = [data for data in scanrefer if data[\"scene_id\"] in scene_list]\n    new_scanrefer_val = scanrefer\n    scanrefer_val_new = []\n    scanrefer_val_new_scene = []",
        "detail": "3D-VLP.scripts.eval_scripts.ground_predict",
        "documentation": {}
    },
    {
        "label": "get_scanrefer",
        "kind": 2,
        "importPath": "3D-VLP.scripts.eval_scripts.ground_predict",
        "description": "3D-VLP.scripts.eval_scripts.ground_predict",
        "peekOfCode": "def get_scanrefer(args):\n    scanrefer = SCANREFER_TEST\n    scene_list = sorted(list(set([data[\"scene_id\"] for data in scanrefer])))\n    scanrefer = [data for data in scanrefer if data[\"scene_id\"] in scene_list]\n    new_scanrefer_val = scanrefer\n    scanrefer_val_new = []\n    scanrefer_val_new_scene = []\n    scene_id = \"\"\n    for data in scanrefer:\n        # if data[\"scene_id\"] not in scanrefer_val_new:",
        "detail": "3D-VLP.scripts.eval_scripts.ground_predict",
        "documentation": {}
    },
    {
        "label": "predict",
        "kind": 2,
        "importPath": "3D-VLP.scripts.eval_scripts.ground_predict",
        "description": "3D-VLP.scripts.eval_scripts.ground_predict",
        "peekOfCode": "def predict(args):\n    print(\"predict bounding boxes...\")\n    # constant\n    DC = ScannetDatasetConfig()\n    # init training dataset\n    print(\"preparing data...\")\n    scanrefer, scene_list, scanrefer_val_new = get_scanrefer(args)\n    # dataloader\n    #_, dataloader = get_dataloader(args, scanrefer, scene_list, \"test\", DC)\n    dataset, dataloader = get_dataloader(args, scanrefer, scanrefer_val_new, scene_list, \"test\", DC)",
        "detail": "3D-VLP.scripts.eval_scripts.ground_predict",
        "documentation": {}
    },
    {
        "label": "SCANREFER_TEST",
        "kind": 5,
        "importPath": "3D-VLP.scripts.eval_scripts.ground_predict",
        "description": "3D-VLP.scripts.eval_scripts.ground_predict",
        "peekOfCode": "SCANREFER_TEST = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_test.json\")))\n# SCANREFER_TEST = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_val.json\")))\ndef get_dataloader(args, scanrefer, scanrefer_new, all_scene_list, split, config):\n    dataset = ScannetReferenceDataset(\n        scanrefer=scanrefer,\n        scanrefer_new=scanrefer_new,\n        scanrefer_all_scene=all_scene_list,\n        split=split,\n        name=args.dataset,\n        num_points=args.num_points,",
        "detail": "3D-VLP.scripts.eval_scripts.ground_predict",
        "documentation": {}
    },
    {
        "label": "get_dataloader",
        "kind": 2,
        "importPath": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "description": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "peekOfCode": "def get_dataloader(args, scanrefer, scanrefer_new, all_scene_list, split, config, augment):\n    dataset = ScannetReferenceDataset(\n        scanrefer=scanrefer,\n        scanrefer_new=scanrefer_new,\n        scanrefer_all_scene=all_scene_list,\n        split=split,\n        name=args.dataset,\n        num_points=args.num_points,\n        use_height=(not args.no_height),\n        use_color=args.use_color,",
        "detail": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "documentation": {}
    },
    {
        "label": "get_model",
        "kind": 2,
        "importPath": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "description": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "peekOfCode": "def get_model(args, DC, dataset):\n    # load model\n    input_channels = int(args.use_multiview) * 128 + int(args.use_normal) * 3 + int(args.use_color) * 3 + int(not args.no_height)\n    model = JointNet(\n        num_class=DC.num_class,\n        vocabulary=dataset.vocabulary,\n        embeddings=dataset.glove,\n        num_heading_bin=DC.num_heading_bin,\n        num_size_cluster=DC.num_size_cluster,\n        mean_size_arr=DC.mean_size_arr,",
        "detail": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "documentation": {}
    },
    {
        "label": "get_scanrefer",
        "kind": 2,
        "importPath": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "description": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "peekOfCode": "def get_scanrefer(args):\n    scanrefer = SCANREFER_TRAIN if args.use_train else SCANREFER_VAL\n    all_scene_list = sorted(list(set([data[\"scene_id\"] for data in scanrefer])))\n    if args.scene_id:\n        assert args.scene_id in all_scene_list, \"The scene_id is not found\"\n        scene_list = [args.scene_id]\n    else:\n        scene_list = sorted(list(set([data[\"scene_id\"] for data in scanrefer])))\n    scanrefer = [data for data in scanrefer if data[\"scene_id\"] in scene_list]\n    new_scanrefer = []",
        "detail": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "documentation": {}
    },
    {
        "label": "write_ply",
        "kind": 2,
        "importPath": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "description": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "peekOfCode": "def write_ply(verts, colors, indices, output_file):\n    if colors is None:\n        colors = np.zeros_like(verts)\n    if indices is None:\n        indices = []\n    file = open(output_file, 'w')\n    file.write('ply \\n')\n    file.write('format ascii 1.0\\n')\n    file.write('element vertex {:d}\\n'.format(len(verts)))\n    file.write('property float x\\n')",
        "detail": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "documentation": {}
    },
    {
        "label": "write_bbox",
        "kind": 2,
        "importPath": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "description": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "peekOfCode": "def write_bbox(bbox, mode, output_file):\n    \"\"\"\n    bbox: (cx, cy, cz, lx, ly, lz, r), center and length in three axis, the last is the rotation\n    output_file: string\n    \"\"\"\n    def create_cylinder_mesh(radius, p0, p1, stacks=10, slices=10):\n        import math\n        def compute_length_vec3(vec3):\n            return math.sqrt(vec3[0]*vec3[0] + vec3[1]*vec3[1] + vec3[2]*vec3[2])\n        def rotation(axis, angle):",
        "detail": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "documentation": {}
    },
    {
        "label": "read_mesh",
        "kind": 2,
        "importPath": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "description": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "peekOfCode": "def read_mesh(filename):\n    \"\"\" read XYZ for each vertex.\n    \"\"\"\n    assert os.path.isfile(filename)\n    with open(filename, 'rb') as f:\n        plydata = PlyData.read(f)\n        num_verts = plydata['vertex'].count\n        vertices = np.zeros(shape=[num_verts, 6], dtype=np.float32)\n        vertices[:,0] = plydata['vertex'].data['x']\n        vertices[:,1] = plydata['vertex'].data['y']",
        "detail": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "documentation": {}
    },
    {
        "label": "export_mesh",
        "kind": 2,
        "importPath": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "description": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "peekOfCode": "def export_mesh(vertices, faces):\n    new_vertices = []\n    for i in range(vertices.shape[0]):\n        new_vertices.append(\n            (\n                vertices[i][0],\n                vertices[i][1],\n                vertices[i][2],\n                vertices[i][3],\n                vertices[i][4],",
        "detail": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "documentation": {}
    },
    {
        "label": "align_mesh",
        "kind": 2,
        "importPath": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "description": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "peekOfCode": "def align_mesh(scene_id):\n    if args.use_complete:\n        vertices, faces = read_mesh(SCANNET_MESH_COMPLETE.format(scene_id, scene_id))\n    else:\n        vertices, faces = read_mesh(SCANNET_MESH.format(scene_id, scene_id))\n    for line in open(SCANNET_META.format(scene_id, scene_id)).readlines():\n        if 'axisAlignment' in line:\n            axis_align_matrix = np.array([float(x) for x in line.rstrip().strip('axisAlignment = ').split(' ')]).reshape((4, 4))\n            break\n    # align",
        "detail": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "documentation": {}
    },
    {
        "label": "dump_results",
        "kind": 2,
        "importPath": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "description": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "peekOfCode": "def dump_results(args, scanrefer, data, config):\n    dump_dir = os.path.join(CONF.PATH.OUTPUT, args.folder, \"vis\")\n    os.makedirs(dump_dir, exist_ok=True)\n    # from inputs\n    ids = data['scan_idx'].detach().cpu().numpy()\n    point_clouds = data['point_clouds'].cpu().numpy()\n    batch_size = point_clouds.shape[0]\n    pcl_color = data[\"pcl_color\"].detach().cpu().numpy()\n    if args.use_color:\n        pcl_color = (pcl_color * 256 + MEAN_COLOR_RGB).astype(np.int64)",
        "detail": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "documentation": {}
    },
    {
        "label": "visualize",
        "kind": 2,
        "importPath": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "description": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "peekOfCode": "def visualize(args):\n    # init training dataset\n    print(\"preparing data...\")\n    scanrefer, scene_list, new_scanrefer = get_scanrefer(args)\n    # dataloader\n    dataset, dataloader = get_dataloader(args, scanrefer, new_scanrefer, scene_list, \"val\", DC, False)\n    # model\n    model = get_model(args, DC, dataset)\n    # config\n    POST_DICT = {",
        "detail": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "documentation": {}
    },
    {
        "label": "#SCANNET_ROOT",
        "kind": 5,
        "importPath": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "description": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "peekOfCode": "#SCANNET_ROOT = \"/mnt/canis/Datasets/ScanNet/public/v2/scans/\" # TODO point this to your scannet data\n#SCANNET_ROOT = \"/data4/caidaigang/caidaigang/model/scanrefer/data/scannet/scans/\"  #29\nSCANNET_ROOT = \"/mnt/24t/JZ/3DJCG/data/scannet/scans/\"  #30\nSCANNET_MESH = os.path.join(SCANNET_ROOT, \"{}/{}_vh_clean_2.ply\") # scene_id, scene_id\nSCANNET_MESH_COMPLETE = os.path.join(SCANNET_ROOT, \"{}/{}_vh_clean.ply\") # scene_id, scene_id\nSCANNET_META = os.path.join(SCANNET_ROOT, \"{}/{}.txt\") # scene_id, scene_id \nSCANREFER_TRAIN = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_train.json\")))\nSCANREFER_VAL = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_val.json\")))\n# constants\nMEAN_COLOR_RGB = np.array([109.8, 97.2, 83.8])",
        "detail": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "documentation": {}
    },
    {
        "label": "#SCANNET_ROOT",
        "kind": 5,
        "importPath": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "description": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "peekOfCode": "#SCANNET_ROOT = \"/data4/caidaigang/caidaigang/model/scanrefer/data/scannet/scans/\"  #29\nSCANNET_ROOT = \"/mnt/24t/JZ/3DJCG/data/scannet/scans/\"  #30\nSCANNET_MESH = os.path.join(SCANNET_ROOT, \"{}/{}_vh_clean_2.ply\") # scene_id, scene_id\nSCANNET_MESH_COMPLETE = os.path.join(SCANNET_ROOT, \"{}/{}_vh_clean.ply\") # scene_id, scene_id\nSCANNET_META = os.path.join(SCANNET_ROOT, \"{}/{}.txt\") # scene_id, scene_id \nSCANREFER_TRAIN = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_train.json\")))\nSCANREFER_VAL = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_val.json\")))\n# constants\nMEAN_COLOR_RGB = np.array([109.8, 97.2, 83.8])\nDC = ScannetDatasetConfig()",
        "detail": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "documentation": {}
    },
    {
        "label": "SCANNET_ROOT",
        "kind": 5,
        "importPath": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "description": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "peekOfCode": "SCANNET_ROOT = \"/mnt/24t/JZ/3DJCG/data/scannet/scans/\"  #30\nSCANNET_MESH = os.path.join(SCANNET_ROOT, \"{}/{}_vh_clean_2.ply\") # scene_id, scene_id\nSCANNET_MESH_COMPLETE = os.path.join(SCANNET_ROOT, \"{}/{}_vh_clean.ply\") # scene_id, scene_id\nSCANNET_META = os.path.join(SCANNET_ROOT, \"{}/{}.txt\") # scene_id, scene_id \nSCANREFER_TRAIN = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_train.json\")))\nSCANREFER_VAL = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_val.json\")))\n# constants\nMEAN_COLOR_RGB = np.array([109.8, 97.2, 83.8])\nDC = ScannetDatasetConfig()\ndef get_dataloader(args, scanrefer, scanrefer_new, all_scene_list, split, config, augment):",
        "detail": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "documentation": {}
    },
    {
        "label": "SCANNET_MESH",
        "kind": 5,
        "importPath": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "description": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "peekOfCode": "SCANNET_MESH = os.path.join(SCANNET_ROOT, \"{}/{}_vh_clean_2.ply\") # scene_id, scene_id\nSCANNET_MESH_COMPLETE = os.path.join(SCANNET_ROOT, \"{}/{}_vh_clean.ply\") # scene_id, scene_id\nSCANNET_META = os.path.join(SCANNET_ROOT, \"{}/{}.txt\") # scene_id, scene_id \nSCANREFER_TRAIN = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_train.json\")))\nSCANREFER_VAL = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_val.json\")))\n# constants\nMEAN_COLOR_RGB = np.array([109.8, 97.2, 83.8])\nDC = ScannetDatasetConfig()\ndef get_dataloader(args, scanrefer, scanrefer_new, all_scene_list, split, config, augment):\n    dataset = ScannetReferenceDataset(",
        "detail": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "documentation": {}
    },
    {
        "label": "SCANNET_MESH_COMPLETE",
        "kind": 5,
        "importPath": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "description": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "peekOfCode": "SCANNET_MESH_COMPLETE = os.path.join(SCANNET_ROOT, \"{}/{}_vh_clean.ply\") # scene_id, scene_id\nSCANNET_META = os.path.join(SCANNET_ROOT, \"{}/{}.txt\") # scene_id, scene_id \nSCANREFER_TRAIN = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_train.json\")))\nSCANREFER_VAL = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_val.json\")))\n# constants\nMEAN_COLOR_RGB = np.array([109.8, 97.2, 83.8])\nDC = ScannetDatasetConfig()\ndef get_dataloader(args, scanrefer, scanrefer_new, all_scene_list, split, config, augment):\n    dataset = ScannetReferenceDataset(\n        scanrefer=scanrefer,",
        "detail": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "documentation": {}
    },
    {
        "label": "SCANNET_META",
        "kind": 5,
        "importPath": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "description": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "peekOfCode": "SCANNET_META = os.path.join(SCANNET_ROOT, \"{}/{}.txt\") # scene_id, scene_id \nSCANREFER_TRAIN = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_train.json\")))\nSCANREFER_VAL = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_val.json\")))\n# constants\nMEAN_COLOR_RGB = np.array([109.8, 97.2, 83.8])\nDC = ScannetDatasetConfig()\ndef get_dataloader(args, scanrefer, scanrefer_new, all_scene_list, split, config, augment):\n    dataset = ScannetReferenceDataset(\n        scanrefer=scanrefer,\n        scanrefer_new=scanrefer_new,",
        "detail": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "documentation": {}
    },
    {
        "label": "SCANREFER_TRAIN",
        "kind": 5,
        "importPath": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "description": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "peekOfCode": "SCANREFER_TRAIN = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_train.json\")))\nSCANREFER_VAL = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_val.json\")))\n# constants\nMEAN_COLOR_RGB = np.array([109.8, 97.2, 83.8])\nDC = ScannetDatasetConfig()\ndef get_dataloader(args, scanrefer, scanrefer_new, all_scene_list, split, config, augment):\n    dataset = ScannetReferenceDataset(\n        scanrefer=scanrefer,\n        scanrefer_new=scanrefer_new,\n        scanrefer_all_scene=all_scene_list,",
        "detail": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "documentation": {}
    },
    {
        "label": "SCANREFER_VAL",
        "kind": 5,
        "importPath": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "description": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "peekOfCode": "SCANREFER_VAL = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_val.json\")))\n# constants\nMEAN_COLOR_RGB = np.array([109.8, 97.2, 83.8])\nDC = ScannetDatasetConfig()\ndef get_dataloader(args, scanrefer, scanrefer_new, all_scene_list, split, config, augment):\n    dataset = ScannetReferenceDataset(\n        scanrefer=scanrefer,\n        scanrefer_new=scanrefer_new,\n        scanrefer_all_scene=all_scene_list,\n        split=split,",
        "detail": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "documentation": {}
    },
    {
        "label": "MEAN_COLOR_RGB",
        "kind": 5,
        "importPath": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "description": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "peekOfCode": "MEAN_COLOR_RGB = np.array([109.8, 97.2, 83.8])\nDC = ScannetDatasetConfig()\ndef get_dataloader(args, scanrefer, scanrefer_new, all_scene_list, split, config, augment):\n    dataset = ScannetReferenceDataset(\n        scanrefer=scanrefer,\n        scanrefer_new=scanrefer_new,\n        scanrefer_all_scene=all_scene_list,\n        split=split,\n        name=args.dataset,\n        num_points=args.num_points,",
        "detail": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "documentation": {}
    },
    {
        "label": "DC",
        "kind": 5,
        "importPath": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "description": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "peekOfCode": "DC = ScannetDatasetConfig()\ndef get_dataloader(args, scanrefer, scanrefer_new, all_scene_list, split, config, augment):\n    dataset = ScannetReferenceDataset(\n        scanrefer=scanrefer,\n        scanrefer_new=scanrefer_new,\n        scanrefer_all_scene=all_scene_list,\n        split=split,\n        name=args.dataset,\n        num_points=args.num_points,\n        use_height=(not args.no_height),",
        "detail": "3D-VLP.scripts.eval_scripts.ground_visualize",
        "documentation": {}
    },
    {
        "label": "get_dataloader",
        "kind": 2,
        "importPath": "3D-VLP.scripts.eval_scripts.qa_eval",
        "description": "3D-VLP.scripts.eval_scripts.qa_eval",
        "peekOfCode": "def get_dataloader(args, scanqa, all_scene_list, split, config):\n    answer_vocab_path = os.path.join(CONF.PATH.OUTPUT, args.folder, \"answer_vocab.json\")\n    answer_counter = json.load(open(answer_vocab_path))\n    answer_cands = sorted(answer_counter.keys())\n    config.num_answers = len(answer_cands)\n    print(\"using {} answers\".format(config.num_answers))\n    if 'bert-' in args.tokenizer_name:\n        from transformers import AutoTokenizer\n        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n        tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name)",
        "detail": "3D-VLP.scripts.eval_scripts.qa_eval",
        "documentation": {}
    },
    {
        "label": "get_model",
        "kind": 2,
        "importPath": "3D-VLP.scripts.eval_scripts.qa_eval",
        "description": "3D-VLP.scripts.eval_scripts.qa_eval",
        "peekOfCode": "def get_model(args, config):\n    # load tokenizer model\n    if \"bert-\" in args.tokenizer_name:\n        from transformers import AutoConfig\n        bert_model_name = args.tokenizer_name\n        bert_config = AutoConfig.from_pretrained(bert_model_name)\n        if hasattr(bert_config, \"hidden_size\"):\n            lang_emb_size = bert_config.hidden_size\n        else:\n            # for distllbert",
        "detail": "3D-VLP.scripts.eval_scripts.qa_eval",
        "documentation": {}
    },
    {
        "label": "get_scannet_scene_list",
        "kind": 2,
        "importPath": "3D-VLP.scripts.eval_scripts.qa_eval",
        "description": "3D-VLP.scripts.eval_scripts.qa_eval",
        "peekOfCode": "def get_scannet_scene_list(split):\n    scene_list = sorted(\n        [line.rstrip() for line in open(os.path.join(CONF.PATH.SCANNET_META, \"scannetv2_{}.txt\".format(split)))])\n    return scene_list\ndef get_scanqa(args):\n    if args.detection:\n        scene_list = get_scannet_scene_list(\"val\")\n        scanqa = []\n        for scene_id in scene_list:\n            data = deepcopy(SCANQA_TRAIN[0])",
        "detail": "3D-VLP.scripts.eval_scripts.qa_eval",
        "documentation": {}
    },
    {
        "label": "get_scanqa",
        "kind": 2,
        "importPath": "3D-VLP.scripts.eval_scripts.qa_eval",
        "description": "3D-VLP.scripts.eval_scripts.qa_eval",
        "peekOfCode": "def get_scanqa(args):\n    if args.detection:\n        scene_list = get_scannet_scene_list(\"val\")\n        scanqa = []\n        for scene_id in scene_list:\n            data = deepcopy(SCANQA_TRAIN[0])\n            data[\"scene_id\"] = scene_id\n            scanqa.append(data)\n    else:\n        SCANQA_VAL = json.load(open(os.path.join(CONF.PATH.SCANQA, project_name + \"_val.json\")))",
        "detail": "3D-VLP.scripts.eval_scripts.qa_eval",
        "documentation": {}
    },
    {
        "label": "eval_qa",
        "kind": 2,
        "importPath": "3D-VLP.scripts.eval_scripts.qa_eval",
        "description": "3D-VLP.scripts.eval_scripts.qa_eval",
        "peekOfCode": "def eval_qa(args):\n    print(\"evaluate localization...\")\n    # constant\n    DC = ScannetQADatasetConfig()\n    # init training dataset\n    print(\"preparing data...\")\n    scanqa, scene_list = get_scanqa(args)\n    # dataloader\n    _, dataloader = get_dataloader(args, scanqa, scene_list, \"val\", DC)\n    # model",
        "detail": "3D-VLP.scripts.eval_scripts.qa_eval",
        "documentation": {}
    },
    {
        "label": "eval_det",
        "kind": 2,
        "importPath": "3D-VLP.scripts.eval_scripts.qa_eval",
        "description": "3D-VLP.scripts.eval_scripts.qa_eval",
        "peekOfCode": "def eval_det(args):\n    print(\"evaluate detection...\")\n    # constant\n    DC = ScannetQADatasetConfig()\n    # init training dataset\n    print(\"preparing data...\")\n    scanqa, scene_list = get_scanqa(args)\n    # dataloader\n    dataset, dataloader = get_dataloader(args, scanqa, scene_list, \"val\", DC)\n    scanqa = dataset.scanqa",
        "detail": "3D-VLP.scripts.eval_scripts.qa_eval",
        "documentation": {}
    },
    {
        "label": "project_name",
        "kind": 5,
        "importPath": "3D-VLP.scripts.eval_scripts.qa_eval",
        "description": "3D-VLP.scripts.eval_scripts.qa_eval",
        "peekOfCode": "project_name = \"ScanQA_v1.0\"\nSCANQA_TRAIN = json.load(open(os.path.join(CONF.PATH.SCANQA, project_name + \"_train.json\")))\ndef get_dataloader(args, scanqa, all_scene_list, split, config):\n    answer_vocab_path = os.path.join(CONF.PATH.OUTPUT, args.folder, \"answer_vocab.json\")\n    answer_counter = json.load(open(answer_vocab_path))\n    answer_cands = sorted(answer_counter.keys())\n    config.num_answers = len(answer_cands)\n    print(\"using {} answers\".format(config.num_answers))\n    if 'bert-' in args.tokenizer_name:\n        from transformers import AutoTokenizer",
        "detail": "3D-VLP.scripts.eval_scripts.qa_eval",
        "documentation": {}
    },
    {
        "label": "SCANQA_TRAIN",
        "kind": 5,
        "importPath": "3D-VLP.scripts.eval_scripts.qa_eval",
        "description": "3D-VLP.scripts.eval_scripts.qa_eval",
        "peekOfCode": "SCANQA_TRAIN = json.load(open(os.path.join(CONF.PATH.SCANQA, project_name + \"_train.json\")))\ndef get_dataloader(args, scanqa, all_scene_list, split, config):\n    answer_vocab_path = os.path.join(CONF.PATH.OUTPUT, args.folder, \"answer_vocab.json\")\n    answer_counter = json.load(open(answer_vocab_path))\n    answer_cands = sorted(answer_counter.keys())\n    config.num_answers = len(answer_cands)\n    print(\"using {} answers\".format(config.num_answers))\n    if 'bert-' in args.tokenizer_name:\n        from transformers import AutoTokenizer\n        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"",
        "detail": "3D-VLP.scripts.eval_scripts.qa_eval",
        "documentation": {}
    },
    {
        "label": "get_dataloader",
        "kind": 2,
        "importPath": "3D-VLP.scripts.eval_scripts.qa_predict",
        "description": "3D-VLP.scripts.eval_scripts.qa_predict",
        "peekOfCode": "def get_dataloader(args, scanqa, all_scene_list, split, config):\n    answer_vocab_path = os.path.join(CONF.PATH.OUTPUT, args.folder, \"answer_vocab.json\")\n    answer_counter = json.load(open(answer_vocab_path))\n    answer_cands = sorted(answer_counter.keys())\n    config.num_answers = len(answer_cands)    \n    print(\"using {} answers\".format(config.num_answers))\n    if 'bert-' in args.tokenizer_name: \n        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n        tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name)\n    else:",
        "detail": "3D-VLP.scripts.eval_scripts.qa_predict",
        "documentation": {}
    },
    {
        "label": "get_model",
        "kind": 2,
        "importPath": "3D-VLP.scripts.eval_scripts.qa_predict",
        "description": "3D-VLP.scripts.eval_scripts.qa_predict",
        "peekOfCode": "def get_model(args, config):\n    # load tokenizer model\n    if \"bert-\" in args.tokenizer_name:\n        bert_model_name = args.tokenizer_name\n        bert_config = AutoConfig.from_pretrained(bert_model_name)\n        if hasattr(bert_config, \"hidden_size\"):\n            lang_emb_size = bert_config.hidden_size\n        else:\n            # for distllbert\n            lang_emb_size = bert_config.dim",
        "detail": "3D-VLP.scripts.eval_scripts.qa_predict",
        "documentation": {}
    },
    {
        "label": "get_scanqa",
        "kind": 2,
        "importPath": "3D-VLP.scripts.eval_scripts.qa_predict",
        "description": "3D-VLP.scripts.eval_scripts.qa_predict",
        "peekOfCode": "def get_scanqa(args):\n    scanqa = json.load(open(os.path.join(CONF.PATH.SCANQA, project_name + \"_\"+args.test_type+\".json\")))\n    scene_list = sorted(list(set([data[\"scene_id\"] for data in scanqa])))\n    scanqa = [data for data in scanqa if data[\"scene_id\"] in scene_list]\n    return scanqa, scene_list\ndef predict(args):\n    print(\"predict bounding boxes...\")\n    # constant\n    DC = ScannetDatasetConfig()\n    # init training dataset",
        "detail": "3D-VLP.scripts.eval_scripts.qa_predict",
        "documentation": {}
    },
    {
        "label": "predict",
        "kind": 2,
        "importPath": "3D-VLP.scripts.eval_scripts.qa_predict",
        "description": "3D-VLP.scripts.eval_scripts.qa_predict",
        "peekOfCode": "def predict(args):\n    print(\"predict bounding boxes...\")\n    # constant\n    DC = ScannetDatasetConfig()\n    # init training dataset\n    print(\"preparing data...\")\n    scanqa, scene_list = get_scanqa(args)\n    # dataloader\n    _, dataloader = get_dataloader(args, scanqa, scene_list, \"test\", DC)\n    dataset = dataloader.dataset",
        "detail": "3D-VLP.scripts.eval_scripts.qa_predict",
        "documentation": {}
    },
    {
        "label": "project_name",
        "kind": 5,
        "importPath": "3D-VLP.scripts.eval_scripts.qa_predict",
        "description": "3D-VLP.scripts.eval_scripts.qa_predict",
        "peekOfCode": "project_name = \"ScanQA_v1.0\"\ndef get_dataloader(args, scanqa, all_scene_list, split, config):\n    answer_vocab_path = os.path.join(CONF.PATH.OUTPUT, args.folder, \"answer_vocab.json\")\n    answer_counter = json.load(open(answer_vocab_path))\n    answer_cands = sorted(answer_counter.keys())\n    config.num_answers = len(answer_cands)    \n    print(\"using {} answers\".format(config.num_answers))\n    if 'bert-' in args.tokenizer_name: \n        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n        tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name)",
        "detail": "3D-VLP.scripts.eval_scripts.qa_predict",
        "documentation": {}
    },
    {
        "label": "EnetDataset",
        "kind": 6,
        "importPath": "3D-VLP.scripts.multiview_compute.compute_multiview_features",
        "description": "3D-VLP.scripts.multiview_compute.compute_multiview_features",
        "peekOfCode": "class EnetDataset(Dataset):\n    def __init__(self):\n        self._init_resources()\n    def __len__(self):\n        return len(self.data)\n    def __getitem__(self, idx):\n        scene_id, frame_id = self.data[idx]\n        image = self._load_image(SCANNET_FRAME_PATH.format(scene_id, \"color\", \"{}.jpg\".format(frame_id)), [328, 256])\n        return scene_id, frame_id, image\n    def _init_resources(self):",
        "detail": "3D-VLP.scripts.multiview_compute.compute_multiview_features",
        "documentation": {}
    },
    {
        "label": "create_enet",
        "kind": 2,
        "importPath": "3D-VLP.scripts.multiview_compute.compute_multiview_features",
        "description": "3D-VLP.scripts.multiview_compute.compute_multiview_features",
        "peekOfCode": "def create_enet():\n    enet_fixed, enet_trainable, _ = create_enet_for_3d(41, ENET_PATH, 21)\n    enet = nn.Sequential(\n        enet_fixed,\n        enet_trainable\n    ).cuda()\n    enet.eval()\n    for param in enet.parameters():\n        param.requires_grad = False\n    return enet",
        "detail": "3D-VLP.scripts.multiview_compute.compute_multiview_features",
        "documentation": {}
    },
    {
        "label": "SCANNET_FRAME_ROOT",
        "kind": 5,
        "importPath": "3D-VLP.scripts.multiview_compute.compute_multiview_features",
        "description": "3D-VLP.scripts.multiview_compute.compute_multiview_features",
        "peekOfCode": "SCANNET_FRAME_ROOT = CONF.SCANNET_FRAMES\nSCANNET_FRAME_PATH = os.path.join(SCANNET_FRAME_ROOT, \"{}\") # name of the file\nSCANNET_LIST = CONF.SCANNETV2_LIST\nENET_PATH = CONF.ENET_WEIGHTS\nENET_FEATURE_ROOT = CONF.ENET_FEATURES_SUBROOT\nENET_FEATURE_PATH = CONF.ENET_FEATURES_PATH\nclass EnetDataset(Dataset):\n    def __init__(self):\n        self._init_resources()\n    def __len__(self):",
        "detail": "3D-VLP.scripts.multiview_compute.compute_multiview_features",
        "documentation": {}
    },
    {
        "label": "SCANNET_FRAME_PATH",
        "kind": 5,
        "importPath": "3D-VLP.scripts.multiview_compute.compute_multiview_features",
        "description": "3D-VLP.scripts.multiview_compute.compute_multiview_features",
        "peekOfCode": "SCANNET_FRAME_PATH = os.path.join(SCANNET_FRAME_ROOT, \"{}\") # name of the file\nSCANNET_LIST = CONF.SCANNETV2_LIST\nENET_PATH = CONF.ENET_WEIGHTS\nENET_FEATURE_ROOT = CONF.ENET_FEATURES_SUBROOT\nENET_FEATURE_PATH = CONF.ENET_FEATURES_PATH\nclass EnetDataset(Dataset):\n    def __init__(self):\n        self._init_resources()\n    def __len__(self):\n        return len(self.data)",
        "detail": "3D-VLP.scripts.multiview_compute.compute_multiview_features",
        "documentation": {}
    },
    {
        "label": "SCANNET_LIST",
        "kind": 5,
        "importPath": "3D-VLP.scripts.multiview_compute.compute_multiview_features",
        "description": "3D-VLP.scripts.multiview_compute.compute_multiview_features",
        "peekOfCode": "SCANNET_LIST = CONF.SCANNETV2_LIST\nENET_PATH = CONF.ENET_WEIGHTS\nENET_FEATURE_ROOT = CONF.ENET_FEATURES_SUBROOT\nENET_FEATURE_PATH = CONF.ENET_FEATURES_PATH\nclass EnetDataset(Dataset):\n    def __init__(self):\n        self._init_resources()\n    def __len__(self):\n        return len(self.data)\n    def __getitem__(self, idx):",
        "detail": "3D-VLP.scripts.multiview_compute.compute_multiview_features",
        "documentation": {}
    },
    {
        "label": "ENET_PATH",
        "kind": 5,
        "importPath": "3D-VLP.scripts.multiview_compute.compute_multiview_features",
        "description": "3D-VLP.scripts.multiview_compute.compute_multiview_features",
        "peekOfCode": "ENET_PATH = CONF.ENET_WEIGHTS\nENET_FEATURE_ROOT = CONF.ENET_FEATURES_SUBROOT\nENET_FEATURE_PATH = CONF.ENET_FEATURES_PATH\nclass EnetDataset(Dataset):\n    def __init__(self):\n        self._init_resources()\n    def __len__(self):\n        return len(self.data)\n    def __getitem__(self, idx):\n        scene_id, frame_id = self.data[idx]",
        "detail": "3D-VLP.scripts.multiview_compute.compute_multiview_features",
        "documentation": {}
    },
    {
        "label": "ENET_FEATURE_ROOT",
        "kind": 5,
        "importPath": "3D-VLP.scripts.multiview_compute.compute_multiview_features",
        "description": "3D-VLP.scripts.multiview_compute.compute_multiview_features",
        "peekOfCode": "ENET_FEATURE_ROOT = CONF.ENET_FEATURES_SUBROOT\nENET_FEATURE_PATH = CONF.ENET_FEATURES_PATH\nclass EnetDataset(Dataset):\n    def __init__(self):\n        self._init_resources()\n    def __len__(self):\n        return len(self.data)\n    def __getitem__(self, idx):\n        scene_id, frame_id = self.data[idx]\n        image = self._load_image(SCANNET_FRAME_PATH.format(scene_id, \"color\", \"{}.jpg\".format(frame_id)), [328, 256])",
        "detail": "3D-VLP.scripts.multiview_compute.compute_multiview_features",
        "documentation": {}
    },
    {
        "label": "ENET_FEATURE_PATH",
        "kind": 5,
        "importPath": "3D-VLP.scripts.multiview_compute.compute_multiview_features",
        "description": "3D-VLP.scripts.multiview_compute.compute_multiview_features",
        "peekOfCode": "ENET_FEATURE_PATH = CONF.ENET_FEATURES_PATH\nclass EnetDataset(Dataset):\n    def __init__(self):\n        self._init_resources()\n    def __len__(self):\n        return len(self.data)\n    def __getitem__(self, idx):\n        scene_id, frame_id = self.data[idx]\n        image = self._load_image(SCANNET_FRAME_PATH.format(scene_id, \"color\", \"{}.jpg\".format(frame_id)), [328, 256])\n        return scene_id, frame_id, image",
        "detail": "3D-VLP.scripts.multiview_compute.compute_multiview_features",
        "documentation": {}
    },
    {
        "label": "get_scene_list",
        "kind": 2,
        "importPath": "3D-VLP.scripts.multiview_compute.project_multiview_features",
        "description": "3D-VLP.scripts.multiview_compute.project_multiview_features",
        "peekOfCode": "def get_scene_list():\n    with open(SCANNET_LIST, 'r') as f:\n        return sorted(list(set(f.read().splitlines())))\ndef to_tensor(arr):\n    return torch.Tensor(arr).cuda()\ndef resize_crop_image(image, new_image_dims):\n    image_dims = [image.shape[1], image.shape[0]]\n    if image_dims == new_image_dims:\n        return image\n    resize_width = int(math.floor(new_image_dims[1] * float(image_dims[0]) / float(image_dims[1])))",
        "detail": "3D-VLP.scripts.multiview_compute.project_multiview_features",
        "documentation": {}
    },
    {
        "label": "to_tensor",
        "kind": 2,
        "importPath": "3D-VLP.scripts.multiview_compute.project_multiview_features",
        "description": "3D-VLP.scripts.multiview_compute.project_multiview_features",
        "peekOfCode": "def to_tensor(arr):\n    return torch.Tensor(arr).cuda()\ndef resize_crop_image(image, new_image_dims):\n    image_dims = [image.shape[1], image.shape[0]]\n    if image_dims == new_image_dims:\n        return image\n    resize_width = int(math.floor(new_image_dims[1] * float(image_dims[0]) / float(image_dims[1])))\n    image = transforms.Resize([new_image_dims[1], resize_width], interpolation=Image.NEAREST)(Image.fromarray(image))\n    image = transforms.CenterCrop([new_image_dims[1], new_image_dims[0]])(image)\n    image = np.array(image)",
        "detail": "3D-VLP.scripts.multiview_compute.project_multiview_features",
        "documentation": {}
    },
    {
        "label": "resize_crop_image",
        "kind": 2,
        "importPath": "3D-VLP.scripts.multiview_compute.project_multiview_features",
        "description": "3D-VLP.scripts.multiview_compute.project_multiview_features",
        "peekOfCode": "def resize_crop_image(image, new_image_dims):\n    image_dims = [image.shape[1], image.shape[0]]\n    if image_dims == new_image_dims:\n        return image\n    resize_width = int(math.floor(new_image_dims[1] * float(image_dims[0]) / float(image_dims[1])))\n    image = transforms.Resize([new_image_dims[1], resize_width], interpolation=Image.NEAREST)(Image.fromarray(image))\n    image = transforms.CenterCrop([new_image_dims[1], new_image_dims[0]])(image)\n    image = np.array(image)\n    return image\ndef load_image(file, image_dims):",
        "detail": "3D-VLP.scripts.multiview_compute.project_multiview_features",
        "documentation": {}
    },
    {
        "label": "load_image",
        "kind": 2,
        "importPath": "3D-VLP.scripts.multiview_compute.project_multiview_features",
        "description": "3D-VLP.scripts.multiview_compute.project_multiview_features",
        "peekOfCode": "def load_image(file, image_dims):\n    image = imread(file)\n    # preprocess\n    image = resize_crop_image(image, image_dims)\n    if len(image.shape) == 3: # color image\n        image =  np.transpose(image, [2, 0, 1])  # move feature to front\n        image = transforms.Normalize(mean=[0.496342, 0.466664, 0.440796], std=[0.277856, 0.28623, 0.291129])(torch.Tensor(image.astype(np.float32) / 255.0))\n    elif len(image.shape) == 2: # label image\n#         image = np.expand_dims(image, 0)\n        pass",
        "detail": "3D-VLP.scripts.multiview_compute.project_multiview_features",
        "documentation": {}
    },
    {
        "label": "load_pose",
        "kind": 2,
        "importPath": "3D-VLP.scripts.multiview_compute.project_multiview_features",
        "description": "3D-VLP.scripts.multiview_compute.project_multiview_features",
        "peekOfCode": "def load_pose(filename):\n    lines = open(filename).read().splitlines()\n    assert len(lines) == 4\n    lines = [[x[0],x[1],x[2],x[3]] for x in (x.split(\" \") for x in lines)]\n    return np.asarray(lines).astype(np.float32)\ndef load_depth(file, image_dims):\n    depth_image = imread(file)\n    # preprocess\n    depth_image = resize_crop_image(depth_image, image_dims)\n    depth_image = depth_image.astype(np.float32) / 1000.0",
        "detail": "3D-VLP.scripts.multiview_compute.project_multiview_features",
        "documentation": {}
    },
    {
        "label": "load_depth",
        "kind": 2,
        "importPath": "3D-VLP.scripts.multiview_compute.project_multiview_features",
        "description": "3D-VLP.scripts.multiview_compute.project_multiview_features",
        "peekOfCode": "def load_depth(file, image_dims):\n    depth_image = imread(file)\n    # preprocess\n    depth_image = resize_crop_image(depth_image, image_dims)\n    depth_image = depth_image.astype(np.float32) / 1000.0\n    return depth_image\ndef get_scene_data(scene_list):\n    scene_data = {}\n    for scene_id in scene_list:\n        # load the original vertices, not the axis-aligned ones",
        "detail": "3D-VLP.scripts.multiview_compute.project_multiview_features",
        "documentation": {}
    },
    {
        "label": "get_scene_data",
        "kind": 2,
        "importPath": "3D-VLP.scripts.multiview_compute.project_multiview_features",
        "description": "3D-VLP.scripts.multiview_compute.project_multiview_features",
        "peekOfCode": "def get_scene_data(scene_list):\n    scene_data = {}\n    for scene_id in scene_list:\n        # load the original vertices, not the axis-aligned ones\n        scene_data[scene_id] = np.load(os.path.join(SCANNET_DATA, scene_id)+\"_vert.npy\")[:, :3]\n    return scene_data\ndef compute_projection(points, depth, camera_to_world):\n    \"\"\"\n        :param points: tensor containing all points of the point cloud (num_points, 3)\n        :param depth: depth map (size: proj_image)",
        "detail": "3D-VLP.scripts.multiview_compute.project_multiview_features",
        "documentation": {}
    },
    {
        "label": "compute_projection",
        "kind": 2,
        "importPath": "3D-VLP.scripts.multiview_compute.project_multiview_features",
        "description": "3D-VLP.scripts.multiview_compute.project_multiview_features",
        "peekOfCode": "def compute_projection(points, depth, camera_to_world):\n    \"\"\"\n        :param points: tensor containing all points of the point cloud (num_points, 3)\n        :param depth: depth map (size: proj_image)\n        :param camera_to_world: camera pose (4, 4)\n        :return indices_3d (array with point indices that correspond to a pixel),\n        :return indices_2d (array with pixel indices that correspond to a point)\n        note:\n            the first digit of indices represents the number of relevant points\n            the rest digits are for the projection mapping",
        "detail": "3D-VLP.scripts.multiview_compute.project_multiview_features",
        "documentation": {}
    },
    {
        "label": "SCANNET_LIST",
        "kind": 5,
        "importPath": "3D-VLP.scripts.multiview_compute.project_multiview_features",
        "description": "3D-VLP.scripts.multiview_compute.project_multiview_features",
        "peekOfCode": "SCANNET_LIST = CONF.SCANNETV2_LIST\nSCANNET_DATA = CONF.PATH.SCANNET_DATA\nSCANNET_FRAME_ROOT = CONF.SCANNET_FRAMES\nSCANNET_FRAME_PATH = os.path.join(SCANNET_FRAME_ROOT, \"{}\") # name of the file\nENET_FEATURE_PATH = CONF.ENET_FEATURES_PATH\nENET_FEATURE_DATABASE = CONF.MULTIVIEW\n# projection\nINTRINSICS = [[37.01983, 0, 20, 0],[0, 38.52470, 15.5, 0],[0, 0, 1, 0],[0, 0, 0, 1]]\nPROJECTOR = ProjectionHelper(INTRINSICS, 0.1, 4.0, [41, 32], 0.05)\ndef get_scene_list():",
        "detail": "3D-VLP.scripts.multiview_compute.project_multiview_features",
        "documentation": {}
    },
    {
        "label": "SCANNET_DATA",
        "kind": 5,
        "importPath": "3D-VLP.scripts.multiview_compute.project_multiview_features",
        "description": "3D-VLP.scripts.multiview_compute.project_multiview_features",
        "peekOfCode": "SCANNET_DATA = CONF.PATH.SCANNET_DATA\nSCANNET_FRAME_ROOT = CONF.SCANNET_FRAMES\nSCANNET_FRAME_PATH = os.path.join(SCANNET_FRAME_ROOT, \"{}\") # name of the file\nENET_FEATURE_PATH = CONF.ENET_FEATURES_PATH\nENET_FEATURE_DATABASE = CONF.MULTIVIEW\n# projection\nINTRINSICS = [[37.01983, 0, 20, 0],[0, 38.52470, 15.5, 0],[0, 0, 1, 0],[0, 0, 0, 1]]\nPROJECTOR = ProjectionHelper(INTRINSICS, 0.1, 4.0, [41, 32], 0.05)\ndef get_scene_list():\n    with open(SCANNET_LIST, 'r') as f:",
        "detail": "3D-VLP.scripts.multiview_compute.project_multiview_features",
        "documentation": {}
    },
    {
        "label": "SCANNET_FRAME_ROOT",
        "kind": 5,
        "importPath": "3D-VLP.scripts.multiview_compute.project_multiview_features",
        "description": "3D-VLP.scripts.multiview_compute.project_multiview_features",
        "peekOfCode": "SCANNET_FRAME_ROOT = CONF.SCANNET_FRAMES\nSCANNET_FRAME_PATH = os.path.join(SCANNET_FRAME_ROOT, \"{}\") # name of the file\nENET_FEATURE_PATH = CONF.ENET_FEATURES_PATH\nENET_FEATURE_DATABASE = CONF.MULTIVIEW\n# projection\nINTRINSICS = [[37.01983, 0, 20, 0],[0, 38.52470, 15.5, 0],[0, 0, 1, 0],[0, 0, 0, 1]]\nPROJECTOR = ProjectionHelper(INTRINSICS, 0.1, 4.0, [41, 32], 0.05)\ndef get_scene_list():\n    with open(SCANNET_LIST, 'r') as f:\n        return sorted(list(set(f.read().splitlines())))",
        "detail": "3D-VLP.scripts.multiview_compute.project_multiview_features",
        "documentation": {}
    },
    {
        "label": "SCANNET_FRAME_PATH",
        "kind": 5,
        "importPath": "3D-VLP.scripts.multiview_compute.project_multiview_features",
        "description": "3D-VLP.scripts.multiview_compute.project_multiview_features",
        "peekOfCode": "SCANNET_FRAME_PATH = os.path.join(SCANNET_FRAME_ROOT, \"{}\") # name of the file\nENET_FEATURE_PATH = CONF.ENET_FEATURES_PATH\nENET_FEATURE_DATABASE = CONF.MULTIVIEW\n# projection\nINTRINSICS = [[37.01983, 0, 20, 0],[0, 38.52470, 15.5, 0],[0, 0, 1, 0],[0, 0, 0, 1]]\nPROJECTOR = ProjectionHelper(INTRINSICS, 0.1, 4.0, [41, 32], 0.05)\ndef get_scene_list():\n    with open(SCANNET_LIST, 'r') as f:\n        return sorted(list(set(f.read().splitlines())))\ndef to_tensor(arr):",
        "detail": "3D-VLP.scripts.multiview_compute.project_multiview_features",
        "documentation": {}
    },
    {
        "label": "ENET_FEATURE_PATH",
        "kind": 5,
        "importPath": "3D-VLP.scripts.multiview_compute.project_multiview_features",
        "description": "3D-VLP.scripts.multiview_compute.project_multiview_features",
        "peekOfCode": "ENET_FEATURE_PATH = CONF.ENET_FEATURES_PATH\nENET_FEATURE_DATABASE = CONF.MULTIVIEW\n# projection\nINTRINSICS = [[37.01983, 0, 20, 0],[0, 38.52470, 15.5, 0],[0, 0, 1, 0],[0, 0, 0, 1]]\nPROJECTOR = ProjectionHelper(INTRINSICS, 0.1, 4.0, [41, 32], 0.05)\ndef get_scene_list():\n    with open(SCANNET_LIST, 'r') as f:\n        return sorted(list(set(f.read().splitlines())))\ndef to_tensor(arr):\n    return torch.Tensor(arr).cuda()",
        "detail": "3D-VLP.scripts.multiview_compute.project_multiview_features",
        "documentation": {}
    },
    {
        "label": "ENET_FEATURE_DATABASE",
        "kind": 5,
        "importPath": "3D-VLP.scripts.multiview_compute.project_multiview_features",
        "description": "3D-VLP.scripts.multiview_compute.project_multiview_features",
        "peekOfCode": "ENET_FEATURE_DATABASE = CONF.MULTIVIEW\n# projection\nINTRINSICS = [[37.01983, 0, 20, 0],[0, 38.52470, 15.5, 0],[0, 0, 1, 0],[0, 0, 0, 1]]\nPROJECTOR = ProjectionHelper(INTRINSICS, 0.1, 4.0, [41, 32], 0.05)\ndef get_scene_list():\n    with open(SCANNET_LIST, 'r') as f:\n        return sorted(list(set(f.read().splitlines())))\ndef to_tensor(arr):\n    return torch.Tensor(arr).cuda()\ndef resize_crop_image(image, new_image_dims):",
        "detail": "3D-VLP.scripts.multiview_compute.project_multiview_features",
        "documentation": {}
    },
    {
        "label": "INTRINSICS",
        "kind": 5,
        "importPath": "3D-VLP.scripts.multiview_compute.project_multiview_features",
        "description": "3D-VLP.scripts.multiview_compute.project_multiview_features",
        "peekOfCode": "INTRINSICS = [[37.01983, 0, 20, 0],[0, 38.52470, 15.5, 0],[0, 0, 1, 0],[0, 0, 0, 1]]\nPROJECTOR = ProjectionHelper(INTRINSICS, 0.1, 4.0, [41, 32], 0.05)\ndef get_scene_list():\n    with open(SCANNET_LIST, 'r') as f:\n        return sorted(list(set(f.read().splitlines())))\ndef to_tensor(arr):\n    return torch.Tensor(arr).cuda()\ndef resize_crop_image(image, new_image_dims):\n    image_dims = [image.shape[1], image.shape[0]]\n    if image_dims == new_image_dims:",
        "detail": "3D-VLP.scripts.multiview_compute.project_multiview_features",
        "documentation": {}
    },
    {
        "label": "PROJECTOR",
        "kind": 5,
        "importPath": "3D-VLP.scripts.multiview_compute.project_multiview_features",
        "description": "3D-VLP.scripts.multiview_compute.project_multiview_features",
        "peekOfCode": "PROJECTOR = ProjectionHelper(INTRINSICS, 0.1, 4.0, [41, 32], 0.05)\ndef get_scene_list():\n    with open(SCANNET_LIST, 'r') as f:\n        return sorted(list(set(f.read().splitlines())))\ndef to_tensor(arr):\n    return torch.Tensor(arr).cuda()\ndef resize_crop_image(image, new_image_dims):\n    image_dims = [image.shape[1], image.shape[0]]\n    if image_dims == new_image_dims:\n        return image",
        "detail": "3D-VLP.scripts.multiview_compute.project_multiview_features",
        "documentation": {}
    },
    {
        "label": "get_nyu40_labels",
        "kind": 2,
        "importPath": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "description": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "peekOfCode": "def get_nyu40_labels():\n    labels = [\"unannotated\"]\n    labels += pd.read_csv(NYU40_LABELS)[\"nyu40class\"].tolist()\n    return labels\ndef get_prediction_to_raw():\n    labels = get_nyu40_labels()\n    mapping = {i: label for i, label in enumerate(labels)}\n    return mapping\ndef get_nyu_to_scannet():\n    nyu_idx_to_nyu_label = get_prediction_to_raw()",
        "detail": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "documentation": {}
    },
    {
        "label": "get_prediction_to_raw",
        "kind": 2,
        "importPath": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "description": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "peekOfCode": "def get_prediction_to_raw():\n    labels = get_nyu40_labels()\n    mapping = {i: label for i, label in enumerate(labels)}\n    return mapping\ndef get_nyu_to_scannet():\n    nyu_idx_to_nyu_label = get_prediction_to_raw()\n    scannet_label_to_scannet_idx = {label: i for i, label in enumerate(SCANNET_LABELS)}\n    # mapping\n    nyu_to_scannet = {}\n    for nyu_idx in range(41):",
        "detail": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "documentation": {}
    },
    {
        "label": "get_nyu_to_scannet",
        "kind": 2,
        "importPath": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "description": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "peekOfCode": "def get_nyu_to_scannet():\n    nyu_idx_to_nyu_label = get_prediction_to_raw()\n    scannet_label_to_scannet_idx = {label: i for i, label in enumerate(SCANNET_LABELS)}\n    # mapping\n    nyu_to_scannet = {}\n    for nyu_idx in range(41):\n        nyu_label = nyu_idx_to_nyu_label[nyu_idx]\n        if nyu_label in scannet_label_to_scannet_idx.keys():\n            scannet_idx = scannet_label_to_scannet_idx[nyu_label]\n        else:",
        "detail": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "documentation": {}
    },
    {
        "label": "create_color_palette",
        "kind": 2,
        "importPath": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "description": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "peekOfCode": "def create_color_palette():\n    return {\n        \"unannotated\": (0, 0, 0),\n        \"floor\": (152, 223, 138),\n        \"wall\": (174, 199, 232),\n        \"cabinet\": (31, 119, 180),\n        \"bed\": (255, 187, 120),\n        \"chair\": (188, 189, 34),\n        \"sofa\": (140, 86, 75),\n        \"table\": (255, 152, 150),",
        "detail": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "documentation": {}
    },
    {
        "label": "get_scene_list",
        "kind": 2,
        "importPath": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "description": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "peekOfCode": "def get_scene_list(args):\n    if args.scene_id == \"-1\":\n        with open(SCANNET_LIST, 'r') as f:\n            return sorted(list(set(f.read().splitlines())))\n    else:\n        return [args.scene_id]\ndef to_tensor(arr):\n    return torch.Tensor(arr).cuda()\ndef resize_crop_image(image, new_image_dims):\n    image_dims = [image.shape[1], image.shape[0]]",
        "detail": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "documentation": {}
    },
    {
        "label": "to_tensor",
        "kind": 2,
        "importPath": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "description": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "peekOfCode": "def to_tensor(arr):\n    return torch.Tensor(arr).cuda()\ndef resize_crop_image(image, new_image_dims):\n    image_dims = [image.shape[1], image.shape[0]]\n    if image_dims == new_image_dims:\n        return image\n    resize_width = int(math.floor(new_image_dims[1] * float(image_dims[0]) / float(image_dims[1])))\n    image = transforms.Resize([new_image_dims[1], resize_width], interpolation=Image.NEAREST)(Image.fromarray(image))\n    image = transforms.CenterCrop([new_image_dims[1], new_image_dims[0]])(image)\n    image = np.array(image)",
        "detail": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "documentation": {}
    },
    {
        "label": "resize_crop_image",
        "kind": 2,
        "importPath": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "description": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "peekOfCode": "def resize_crop_image(image, new_image_dims):\n    image_dims = [image.shape[1], image.shape[0]]\n    if image_dims == new_image_dims:\n        return image\n    resize_width = int(math.floor(new_image_dims[1] * float(image_dims[0]) / float(image_dims[1])))\n    image = transforms.Resize([new_image_dims[1], resize_width], interpolation=Image.NEAREST)(Image.fromarray(image))\n    image = transforms.CenterCrop([new_image_dims[1], new_image_dims[0]])(image)\n    image = np.array(image)\n    return image\ndef load_image(file, image_dims):",
        "detail": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "documentation": {}
    },
    {
        "label": "load_image",
        "kind": 2,
        "importPath": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "description": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "peekOfCode": "def load_image(file, image_dims):\n    image = imread(file)\n    # preprocess\n    image = resize_crop_image(image, image_dims)\n    if len(image.shape) == 3: # color image\n        image =  np.transpose(image, [2, 0, 1])  # move feature to front\n        image = transforms.Normalize(mean=[0.496342, 0.466664, 0.440796], std=[0.277856, 0.28623, 0.291129])(torch.Tensor(image.astype(np.float32) / 255.0))\n    elif len(image.shape) == 2: # label image\n#         image = np.expand_dims(image, 0)\n        pass",
        "detail": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "documentation": {}
    },
    {
        "label": "load_pose",
        "kind": 2,
        "importPath": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "description": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "peekOfCode": "def load_pose(filename):\n    lines = open(filename).read().splitlines()\n    assert len(lines) == 4\n    lines = [[x[0],x[1],x[2],x[3]] for x in (x.split(\" \") for x in lines)]\n    return np.asarray(lines).astype(np.float32)\ndef load_depth(file, image_dims):\n    depth_image = imread(file)\n    # preprocess\n    depth_image = resize_crop_image(depth_image, image_dims)\n    depth_image = depth_image.astype(np.float32) / 1000.0",
        "detail": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "documentation": {}
    },
    {
        "label": "load_depth",
        "kind": 2,
        "importPath": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "description": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "peekOfCode": "def load_depth(file, image_dims):\n    depth_image = imread(file)\n    # preprocess\n    depth_image = resize_crop_image(depth_image, image_dims)\n    depth_image = depth_image.astype(np.float32) / 1000.0\n    return depth_image\ndef visualize(coords, labels):\n    palette = create_color_palette()\n    nyu_to_scannet = get_nyu_to_scannet()\n    vertex = []",
        "detail": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "documentation": {}
    },
    {
        "label": "visualize",
        "kind": 2,
        "importPath": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "description": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "peekOfCode": "def visualize(coords, labels):\n    palette = create_color_palette()\n    nyu_to_scannet = get_nyu_to_scannet()\n    vertex = []\n    for i in range(coords.shape[0]):\n        vertex.append(\n            (\n                coords[i][0],\n                coords[i][1],\n                coords[i][2],",
        "detail": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "documentation": {}
    },
    {
        "label": "get_scene_data",
        "kind": 2,
        "importPath": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "description": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "peekOfCode": "def get_scene_data(scene_list):\n    scene_data = {}\n    for scene_id in scene_list:\n        scene_data[scene_id] = {}\n        scene_data[scene_id] = np.load(os.path.join(SCANNET_DATA, scene_id)+\"_vert.npy\")[:, :3]\n    return scene_data\ndef compute_projection(points, depth, camera_to_world):\n    \"\"\"\n        :param points: tensor containing all points of the point cloud (num_points, 3)\n        :param depth: depth map (size: proj_image)",
        "detail": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "documentation": {}
    },
    {
        "label": "compute_projection",
        "kind": 2,
        "importPath": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "description": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "peekOfCode": "def compute_projection(points, depth, camera_to_world):\n    \"\"\"\n        :param points: tensor containing all points of the point cloud (num_points, 3)\n        :param depth: depth map (size: proj_image)\n        :param camera_to_world: camera pose (4, 4)\n        :return indices_3d (array with point indices that correspond to a pixel),\n        :return indices_2d (array with pixel indices that correspond to a point)\n        note:\n            the first digit of indices represents the number of relevant points\n            the rest digits are for the projection mapping",
        "detail": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "documentation": {}
    },
    {
        "label": "create_enet",
        "kind": 2,
        "importPath": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "description": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "peekOfCode": "def create_enet():\n    enet_fixed, enet_trainable, enet_classifier = create_enet_for_3d(41, ENET_PATH, 21)\n    enet = nn.Sequential(\n        enet_fixed,\n        enet_trainable,\n        enet_classifier\n    ).cuda()\n    enet.eval()\n    for param in enet.parameters():\n        param.requires_grad = False",
        "detail": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "documentation": {}
    },
    {
        "label": "SCANNET_LIST",
        "kind": 5,
        "importPath": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "description": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "peekOfCode": "SCANNET_LIST = CONF.SCANNETV2_LIST\nSCANNET_DATA = CONF.PATH.SCANNET_DATA\nSCANNET_FRAME_ROOT = CONF.SCANNET_FRAMES\nSCANNET_FRAME_PATH = os.path.join(SCANNET_FRAME_ROOT, \"{}\") # name of the file\nENET_FEATURE_PATH = CONF.ENET_FEATURES_PATH\nENET_FEATURE_DATABASE = CONF.MULTIVIEW\n# projection\nINTRINSICS = [[37.01983, 0, 20, 0],[0, 38.52470, 15.5, 0],[0, 0, 1, 0],[0, 0, 0, 1]]\nPROJECTOR = ProjectionHelper(INTRINSICS, 0.1, 4.0, [41, 32], 0.05)\nENET_PATH = CONF.ENET_WEIGHTS",
        "detail": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "documentation": {}
    },
    {
        "label": "SCANNET_DATA",
        "kind": 5,
        "importPath": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "description": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "peekOfCode": "SCANNET_DATA = CONF.PATH.SCANNET_DATA\nSCANNET_FRAME_ROOT = CONF.SCANNET_FRAMES\nSCANNET_FRAME_PATH = os.path.join(SCANNET_FRAME_ROOT, \"{}\") # name of the file\nENET_FEATURE_PATH = CONF.ENET_FEATURES_PATH\nENET_FEATURE_DATABASE = CONF.MULTIVIEW\n# projection\nINTRINSICS = [[37.01983, 0, 20, 0],[0, 38.52470, 15.5, 0],[0, 0, 1, 0],[0, 0, 0, 1]]\nPROJECTOR = ProjectionHelper(INTRINSICS, 0.1, 4.0, [41, 32], 0.05)\nENET_PATH = CONF.ENET_WEIGHTS\nENET_GT_PATH = SCANNET_FRAME_PATH",
        "detail": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "documentation": {}
    },
    {
        "label": "SCANNET_FRAME_ROOT",
        "kind": 5,
        "importPath": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "description": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "peekOfCode": "SCANNET_FRAME_ROOT = CONF.SCANNET_FRAMES\nSCANNET_FRAME_PATH = os.path.join(SCANNET_FRAME_ROOT, \"{}\") # name of the file\nENET_FEATURE_PATH = CONF.ENET_FEATURES_PATH\nENET_FEATURE_DATABASE = CONF.MULTIVIEW\n# projection\nINTRINSICS = [[37.01983, 0, 20, 0],[0, 38.52470, 15.5, 0],[0, 0, 1, 0],[0, 0, 0, 1]]\nPROJECTOR = ProjectionHelper(INTRINSICS, 0.1, 4.0, [41, 32], 0.05)\nENET_PATH = CONF.ENET_WEIGHTS\nENET_GT_PATH = SCANNET_FRAME_PATH\nNYU40_LABELS = CONF.NYU40_LABELS",
        "detail": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "documentation": {}
    },
    {
        "label": "SCANNET_FRAME_PATH",
        "kind": 5,
        "importPath": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "description": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "peekOfCode": "SCANNET_FRAME_PATH = os.path.join(SCANNET_FRAME_ROOT, \"{}\") # name of the file\nENET_FEATURE_PATH = CONF.ENET_FEATURES_PATH\nENET_FEATURE_DATABASE = CONF.MULTIVIEW\n# projection\nINTRINSICS = [[37.01983, 0, 20, 0],[0, 38.52470, 15.5, 0],[0, 0, 1, 0],[0, 0, 0, 1]]\nPROJECTOR = ProjectionHelper(INTRINSICS, 0.1, 4.0, [41, 32], 0.05)\nENET_PATH = CONF.ENET_WEIGHTS\nENET_GT_PATH = SCANNET_FRAME_PATH\nNYU40_LABELS = CONF.NYU40_LABELS\nSCANNET_LABELS = ['unannotated', 'wall', 'floor', 'chair', 'table', 'desk', 'bed', 'bookshelf', 'sofa', 'sink', 'bathtub', 'toilet', 'curtain', 'counter', 'door', 'window', 'shower curtain', 'refridgerator', 'picture', 'cabinet', 'otherfurniture']",
        "detail": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "documentation": {}
    },
    {
        "label": "ENET_FEATURE_PATH",
        "kind": 5,
        "importPath": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "description": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "peekOfCode": "ENET_FEATURE_PATH = CONF.ENET_FEATURES_PATH\nENET_FEATURE_DATABASE = CONF.MULTIVIEW\n# projection\nINTRINSICS = [[37.01983, 0, 20, 0],[0, 38.52470, 15.5, 0],[0, 0, 1, 0],[0, 0, 0, 1]]\nPROJECTOR = ProjectionHelper(INTRINSICS, 0.1, 4.0, [41, 32], 0.05)\nENET_PATH = CONF.ENET_WEIGHTS\nENET_GT_PATH = SCANNET_FRAME_PATH\nNYU40_LABELS = CONF.NYU40_LABELS\nSCANNET_LABELS = ['unannotated', 'wall', 'floor', 'chair', 'table', 'desk', 'bed', 'bookshelf', 'sofa', 'sink', 'bathtub', 'toilet', 'curtain', 'counter', 'door', 'window', 'shower curtain', 'refridgerator', 'picture', 'cabinet', 'otherfurniture']\nPC_LABEL_ROOT = os.path.join(CONF.PATH.OUTPUT, \"projections\")",
        "detail": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "documentation": {}
    },
    {
        "label": "ENET_FEATURE_DATABASE",
        "kind": 5,
        "importPath": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "description": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "peekOfCode": "ENET_FEATURE_DATABASE = CONF.MULTIVIEW\n# projection\nINTRINSICS = [[37.01983, 0, 20, 0],[0, 38.52470, 15.5, 0],[0, 0, 1, 0],[0, 0, 0, 1]]\nPROJECTOR = ProjectionHelper(INTRINSICS, 0.1, 4.0, [41, 32], 0.05)\nENET_PATH = CONF.ENET_WEIGHTS\nENET_GT_PATH = SCANNET_FRAME_PATH\nNYU40_LABELS = CONF.NYU40_LABELS\nSCANNET_LABELS = ['unannotated', 'wall', 'floor', 'chair', 'table', 'desk', 'bed', 'bookshelf', 'sofa', 'sink', 'bathtub', 'toilet', 'curtain', 'counter', 'door', 'window', 'shower curtain', 'refridgerator', 'picture', 'cabinet', 'otherfurniture']\nPC_LABEL_ROOT = os.path.join(CONF.PATH.OUTPUT, \"projections\")\nPC_LABEL_PATH = os.path.join(PC_LABEL_ROOT, \"{}.ply\")",
        "detail": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "documentation": {}
    },
    {
        "label": "INTRINSICS",
        "kind": 5,
        "importPath": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "description": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "peekOfCode": "INTRINSICS = [[37.01983, 0, 20, 0],[0, 38.52470, 15.5, 0],[0, 0, 1, 0],[0, 0, 0, 1]]\nPROJECTOR = ProjectionHelper(INTRINSICS, 0.1, 4.0, [41, 32], 0.05)\nENET_PATH = CONF.ENET_WEIGHTS\nENET_GT_PATH = SCANNET_FRAME_PATH\nNYU40_LABELS = CONF.NYU40_LABELS\nSCANNET_LABELS = ['unannotated', 'wall', 'floor', 'chair', 'table', 'desk', 'bed', 'bookshelf', 'sofa', 'sink', 'bathtub', 'toilet', 'curtain', 'counter', 'door', 'window', 'shower curtain', 'refridgerator', 'picture', 'cabinet', 'otherfurniture']\nPC_LABEL_ROOT = os.path.join(CONF.PATH.OUTPUT, \"projections\")\nPC_LABEL_PATH = os.path.join(PC_LABEL_ROOT, \"{}.ply\")\ndef get_nyu40_labels():\n    labels = [\"unannotated\"]",
        "detail": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "documentation": {}
    },
    {
        "label": "PROJECTOR",
        "kind": 5,
        "importPath": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "description": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "peekOfCode": "PROJECTOR = ProjectionHelper(INTRINSICS, 0.1, 4.0, [41, 32], 0.05)\nENET_PATH = CONF.ENET_WEIGHTS\nENET_GT_PATH = SCANNET_FRAME_PATH\nNYU40_LABELS = CONF.NYU40_LABELS\nSCANNET_LABELS = ['unannotated', 'wall', 'floor', 'chair', 'table', 'desk', 'bed', 'bookshelf', 'sofa', 'sink', 'bathtub', 'toilet', 'curtain', 'counter', 'door', 'window', 'shower curtain', 'refridgerator', 'picture', 'cabinet', 'otherfurniture']\nPC_LABEL_ROOT = os.path.join(CONF.PATH.OUTPUT, \"projections\")\nPC_LABEL_PATH = os.path.join(PC_LABEL_ROOT, \"{}.ply\")\ndef get_nyu40_labels():\n    labels = [\"unannotated\"]\n    labels += pd.read_csv(NYU40_LABELS)[\"nyu40class\"].tolist()",
        "detail": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "documentation": {}
    },
    {
        "label": "ENET_PATH",
        "kind": 5,
        "importPath": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "description": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "peekOfCode": "ENET_PATH = CONF.ENET_WEIGHTS\nENET_GT_PATH = SCANNET_FRAME_PATH\nNYU40_LABELS = CONF.NYU40_LABELS\nSCANNET_LABELS = ['unannotated', 'wall', 'floor', 'chair', 'table', 'desk', 'bed', 'bookshelf', 'sofa', 'sink', 'bathtub', 'toilet', 'curtain', 'counter', 'door', 'window', 'shower curtain', 'refridgerator', 'picture', 'cabinet', 'otherfurniture']\nPC_LABEL_ROOT = os.path.join(CONF.PATH.OUTPUT, \"projections\")\nPC_LABEL_PATH = os.path.join(PC_LABEL_ROOT, \"{}.ply\")\ndef get_nyu40_labels():\n    labels = [\"unannotated\"]\n    labels += pd.read_csv(NYU40_LABELS)[\"nyu40class\"].tolist()\n    return labels",
        "detail": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "documentation": {}
    },
    {
        "label": "ENET_GT_PATH",
        "kind": 5,
        "importPath": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "description": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "peekOfCode": "ENET_GT_PATH = SCANNET_FRAME_PATH\nNYU40_LABELS = CONF.NYU40_LABELS\nSCANNET_LABELS = ['unannotated', 'wall', 'floor', 'chair', 'table', 'desk', 'bed', 'bookshelf', 'sofa', 'sink', 'bathtub', 'toilet', 'curtain', 'counter', 'door', 'window', 'shower curtain', 'refridgerator', 'picture', 'cabinet', 'otherfurniture']\nPC_LABEL_ROOT = os.path.join(CONF.PATH.OUTPUT, \"projections\")\nPC_LABEL_PATH = os.path.join(PC_LABEL_ROOT, \"{}.ply\")\ndef get_nyu40_labels():\n    labels = [\"unannotated\"]\n    labels += pd.read_csv(NYU40_LABELS)[\"nyu40class\"].tolist()\n    return labels\ndef get_prediction_to_raw():",
        "detail": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "documentation": {}
    },
    {
        "label": "NYU40_LABELS",
        "kind": 5,
        "importPath": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "description": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "peekOfCode": "NYU40_LABELS = CONF.NYU40_LABELS\nSCANNET_LABELS = ['unannotated', 'wall', 'floor', 'chair', 'table', 'desk', 'bed', 'bookshelf', 'sofa', 'sink', 'bathtub', 'toilet', 'curtain', 'counter', 'door', 'window', 'shower curtain', 'refridgerator', 'picture', 'cabinet', 'otherfurniture']\nPC_LABEL_ROOT = os.path.join(CONF.PATH.OUTPUT, \"projections\")\nPC_LABEL_PATH = os.path.join(PC_LABEL_ROOT, \"{}.ply\")\ndef get_nyu40_labels():\n    labels = [\"unannotated\"]\n    labels += pd.read_csv(NYU40_LABELS)[\"nyu40class\"].tolist()\n    return labels\ndef get_prediction_to_raw():\n    labels = get_nyu40_labels()",
        "detail": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "documentation": {}
    },
    {
        "label": "SCANNET_LABELS",
        "kind": 5,
        "importPath": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "description": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "peekOfCode": "SCANNET_LABELS = ['unannotated', 'wall', 'floor', 'chair', 'table', 'desk', 'bed', 'bookshelf', 'sofa', 'sink', 'bathtub', 'toilet', 'curtain', 'counter', 'door', 'window', 'shower curtain', 'refridgerator', 'picture', 'cabinet', 'otherfurniture']\nPC_LABEL_ROOT = os.path.join(CONF.PATH.OUTPUT, \"projections\")\nPC_LABEL_PATH = os.path.join(PC_LABEL_ROOT, \"{}.ply\")\ndef get_nyu40_labels():\n    labels = [\"unannotated\"]\n    labels += pd.read_csv(NYU40_LABELS)[\"nyu40class\"].tolist()\n    return labels\ndef get_prediction_to_raw():\n    labels = get_nyu40_labels()\n    mapping = {i: label for i, label in enumerate(labels)}",
        "detail": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "documentation": {}
    },
    {
        "label": "PC_LABEL_ROOT",
        "kind": 5,
        "importPath": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "description": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "peekOfCode": "PC_LABEL_ROOT = os.path.join(CONF.PATH.OUTPUT, \"projections\")\nPC_LABEL_PATH = os.path.join(PC_LABEL_ROOT, \"{}.ply\")\ndef get_nyu40_labels():\n    labels = [\"unannotated\"]\n    labels += pd.read_csv(NYU40_LABELS)[\"nyu40class\"].tolist()\n    return labels\ndef get_prediction_to_raw():\n    labels = get_nyu40_labels()\n    mapping = {i: label for i, label in enumerate(labels)}\n    return mapping",
        "detail": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "documentation": {}
    },
    {
        "label": "PC_LABEL_PATH",
        "kind": 5,
        "importPath": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "description": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "peekOfCode": "PC_LABEL_PATH = os.path.join(PC_LABEL_ROOT, \"{}.ply\")\ndef get_nyu40_labels():\n    labels = [\"unannotated\"]\n    labels += pd.read_csv(NYU40_LABELS)[\"nyu40class\"].tolist()\n    return labels\ndef get_prediction_to_raw():\n    labels = get_nyu40_labels()\n    mapping = {i: label for i, label in enumerate(labels)}\n    return mapping\ndef get_nyu_to_scannet():",
        "detail": "3D-VLP.scripts.multiview_compute.project_multiview_labels",
        "documentation": {}
    },
    {
        "label": "get_dataloader",
        "kind": 2,
        "importPath": "3D-VLP.scripts.train_scripts.train_ft",
        "description": "3D-VLP.scripts.train_scripts.train_ft",
        "peekOfCode": "def get_dataloader(args, scanrefer, scanrefer_new, all_scene_list, split, config, augment, shuffle=True,\n                   scan2cad_rotation=None):\n    dataset = ScannetReferenceDataset(\n        scanrefer=scanrefer,\n        scanrefer_new=scanrefer_new,\n        scanrefer_all_scene=all_scene_list,\n        split=split,\n        name=args.dataset,\n        num_points=args.num_points,\n        use_height=(not args.no_height),",
        "detail": "3D-VLP.scripts.train_scripts.train_ft",
        "documentation": {}
    },
    {
        "label": "get_model",
        "kind": 2,
        "importPath": "3D-VLP.scripts.train_scripts.train_ft",
        "description": "3D-VLP.scripts.train_scripts.train_ft",
        "peekOfCode": "def get_model(args, dataset, device):\n    # initiate model\n    input_channels = int(args.use_multiview) * 128 + int(args.use_normal) * 3 + int(args.use_color) * 3 + int(\n        not args.no_height)\n    model = JointNet(\n        num_class=DC.num_class,\n        vocabulary=dataset.vocabulary,\n        embeddings=dataset.glove,\n        num_heading_bin=DC.num_heading_bin,\n        num_size_cluster=DC.num_size_cluster,",
        "detail": "3D-VLP.scripts.train_scripts.train_ft",
        "documentation": {}
    },
    {
        "label": "get_num_params",
        "kind": 2,
        "importPath": "3D-VLP.scripts.train_scripts.train_ft",
        "description": "3D-VLP.scripts.train_scripts.train_ft",
        "peekOfCode": "def get_num_params(model):\n    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n    num_params = int(sum([np.prod(p.size()) for p in model_parameters]))\n    return num_params\ndef get_solver(args, dataset, dataloader):\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    model = get_model(args, dataset[\"train\"], device)\n    # TODO\n    \"\"\"\n    weight_dict = {",
        "detail": "3D-VLP.scripts.train_scripts.train_ft",
        "documentation": {}
    },
    {
        "label": "get_solver",
        "kind": 2,
        "importPath": "3D-VLP.scripts.train_scripts.train_ft",
        "description": "3D-VLP.scripts.train_scripts.train_ft",
        "peekOfCode": "def get_solver(args, dataset, dataloader):\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    model = get_model(args, dataset[\"train\"], device)\n    # TODO\n    \"\"\"\n    weight_dict = {\n        'backbone_net': {'lr': 0.0001},\n        'vgen': {'lr': 0.0001},\n        'proposal': {'lr': 0.0001},\n        'detr': {'lr': 0.0001},",
        "detail": "3D-VLP.scripts.train_scripts.train_ft",
        "documentation": {}
    },
    {
        "label": "save_info",
        "kind": 2,
        "importPath": "3D-VLP.scripts.train_scripts.train_ft",
        "description": "3D-VLP.scripts.train_scripts.train_ft",
        "peekOfCode": "def save_info(args, root, num_params, dataset):\n    info = {}\n    for key, value in vars(args).items():\n        info[key] = value\n    info[\"num_train\"] = len(dataset[\"train\"])\n    info[\"num_eval_train\"] = len(dataset[\"eval\"][\"train\"])\n    info[\"num_eval_val\"] = len(dataset[\"eval\"][\"val\"])\n    info[\"num_train_scenes\"] = len(dataset[\"train\"].scene_list)\n    info[\"num_eval_train_scenes\"] = len(dataset[\"eval\"][\"train\"].scene_list)\n    info[\"num_eval_val_scenes\"] = len(dataset[\"eval\"][\"val\"].scene_list)",
        "detail": "3D-VLP.scripts.train_scripts.train_ft",
        "documentation": {}
    },
    {
        "label": "get_scannet_scene_list",
        "kind": 2,
        "importPath": "3D-VLP.scripts.train_scripts.train_ft",
        "description": "3D-VLP.scripts.train_scripts.train_ft",
        "peekOfCode": "def get_scannet_scene_list(split):\n    scene_list = sorted(\n        [line.rstrip() for line in open(os.path.join(CONF.PATH.SCANNET_META, \"scannetv2_{}.txt\".format(split)))])\n    return scene_list\ndef get_scanrefer(args):\n    if args.dataset == \"ScanRefer\":\n        scanrefer_train = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_train.json\")))\n        scanrefer_eval_train = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_train.json\")))\n        scanrefer_eval_val = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_val.json\")))\n    elif args.dataset == \"ReferIt3D\":",
        "detail": "3D-VLP.scripts.train_scripts.train_ft",
        "documentation": {}
    },
    {
        "label": "get_scanrefer",
        "kind": 2,
        "importPath": "3D-VLP.scripts.train_scripts.train_ft",
        "description": "3D-VLP.scripts.train_scripts.train_ft",
        "peekOfCode": "def get_scanrefer(args):\n    if args.dataset == \"ScanRefer\":\n        scanrefer_train = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_train.json\")))\n        scanrefer_eval_train = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_train.json\")))\n        scanrefer_eval_val = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_val.json\")))\n    elif args.dataset == \"ReferIt3D\":\n        scanrefer_train = json.load(open(os.path.join(CONF.PATH.DATA, \"nr3d_train.json\")))\n        scanrefer_eval_train = json.load(open(os.path.join(CONF.PATH.DATA, \"nr3d_train.json\")))\n        scanrefer_eval_val = json.load(open(os.path.join(CONF.PATH.DATA, \"nr3d_val.json\")))\n    else:",
        "detail": "3D-VLP.scripts.train_scripts.train_ft",
        "documentation": {}
    },
    {
        "label": "train",
        "kind": 2,
        "importPath": "3D-VLP.scripts.train_scripts.train_ft",
        "description": "3D-VLP.scripts.train_scripts.train_ft",
        "peekOfCode": "def train(args):\n    # init training dataset\n    print(\"preparing data...\")\n    scanrefer_train, scanrefer_eval_train, scanrefer_eval_val, scanrefer_eval_val2, all_scene_list, scanrefer_train_new, scanrefer_eval_train_new, scanrefer_eval_val_new, scanrefer_eval_val_new2 = get_scanrefer(\n        args)\n    # eval_train_dataset\n    # dataloader\n    train_dataset, train_dataloader = get_dataloader(args, scanrefer_train, scanrefer_train_new, all_scene_list,\n                                                     \"train\", DC, True, SCAN2CAD_ROTATION)\n    eval_train_dataset, eval_train_dataloader = get_dataloader(args, scanrefer_eval_train, scanrefer_eval_train_new,",
        "detail": "3D-VLP.scripts.train_scripts.train_ft",
        "documentation": {}
    },
    {
        "label": "SCANREFER_TRAIN",
        "kind": 5,
        "importPath": "3D-VLP.scripts.train_scripts.train_ft",
        "description": "3D-VLP.scripts.train_scripts.train_ft",
        "peekOfCode": "SCANREFER_TRAIN = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_train.json\")))\nSCANREFER_VAL = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_val.json\")))\nSCAN2CAD_ROTATION = json.load(open(os.path.join(CONF.PATH.SCAN2CAD, \"scannet_instance_rotations.json\")))\n# constants\nDC = ScannetDatasetConfig()\ndef get_dataloader(args, scanrefer, scanrefer_new, all_scene_list, split, config, augment, shuffle=True,\n                   scan2cad_rotation=None):\n    dataset = ScannetReferenceDataset(\n        scanrefer=scanrefer,\n        scanrefer_new=scanrefer_new,",
        "detail": "3D-VLP.scripts.train_scripts.train_ft",
        "documentation": {}
    },
    {
        "label": "SCANREFER_VAL",
        "kind": 5,
        "importPath": "3D-VLP.scripts.train_scripts.train_ft",
        "description": "3D-VLP.scripts.train_scripts.train_ft",
        "peekOfCode": "SCANREFER_VAL = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_val.json\")))\nSCAN2CAD_ROTATION = json.load(open(os.path.join(CONF.PATH.SCAN2CAD, \"scannet_instance_rotations.json\")))\n# constants\nDC = ScannetDatasetConfig()\ndef get_dataloader(args, scanrefer, scanrefer_new, all_scene_list, split, config, augment, shuffle=True,\n                   scan2cad_rotation=None):\n    dataset = ScannetReferenceDataset(\n        scanrefer=scanrefer,\n        scanrefer_new=scanrefer_new,\n        scanrefer_all_scene=all_scene_list,",
        "detail": "3D-VLP.scripts.train_scripts.train_ft",
        "documentation": {}
    },
    {
        "label": "SCAN2CAD_ROTATION",
        "kind": 5,
        "importPath": "3D-VLP.scripts.train_scripts.train_ft",
        "description": "3D-VLP.scripts.train_scripts.train_ft",
        "peekOfCode": "SCAN2CAD_ROTATION = json.load(open(os.path.join(CONF.PATH.SCAN2CAD, \"scannet_instance_rotations.json\")))\n# constants\nDC = ScannetDatasetConfig()\ndef get_dataloader(args, scanrefer, scanrefer_new, all_scene_list, split, config, augment, shuffle=True,\n                   scan2cad_rotation=None):\n    dataset = ScannetReferenceDataset(\n        scanrefer=scanrefer,\n        scanrefer_new=scanrefer_new,\n        scanrefer_all_scene=all_scene_list,\n        split=split,",
        "detail": "3D-VLP.scripts.train_scripts.train_ft",
        "documentation": {}
    },
    {
        "label": "DC",
        "kind": 5,
        "importPath": "3D-VLP.scripts.train_scripts.train_ft",
        "description": "3D-VLP.scripts.train_scripts.train_ft",
        "peekOfCode": "DC = ScannetDatasetConfig()\ndef get_dataloader(args, scanrefer, scanrefer_new, all_scene_list, split, config, augment, shuffle=True,\n                   scan2cad_rotation=None):\n    dataset = ScannetReferenceDataset(\n        scanrefer=scanrefer,\n        scanrefer_new=scanrefer_new,\n        scanrefer_all_scene=all_scene_list,\n        split=split,\n        name=args.dataset,\n        num_points=args.num_points,",
        "detail": "3D-VLP.scripts.train_scripts.train_ft",
        "documentation": {}
    },
    {
        "label": "parse_option",
        "kind": 2,
        "importPath": "3D-VLP.scripts.train_scripts.train_ft_qa",
        "description": "3D-VLP.scripts.train_scripts.train_ft_qa",
        "peekOfCode": "def parse_option():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--debug\", action=\"store_true\", help=\"debugging mode\")\n    parser.add_argument(\"--tag\", type=str, help=\"tag for the training, e.g. XYZ_COLOR\", default=\"QA\")\n    parser.add_argument(\"--gpu\", type=str, help=\"gpu\", default=\"0\")\n    # Training\n    parser.add_argument(\"--cur_criterion\", type=str, default=\"answer_acc_at1\", help=\"data augmentation type\")\n    parser.add_argument(\"--batch_size\", type=int, help=\"batch size\", default=16)\n    parser.add_argument(\"--epoch\", type=int, help=\"number of epochs\", default=30)\n    parser.add_argument(\"--verbose\", type=int, help=\"iterations of showing verbose\", default=10)",
        "detail": "3D-VLP.scripts.train_scripts.train_ft_qa",
        "documentation": {}
    },
    {
        "label": "get_answer_cands",
        "kind": 2,
        "importPath": "3D-VLP.scripts.train_scripts.train_ft_qa",
        "description": "3D-VLP.scripts.train_scripts.train_ft_qa",
        "peekOfCode": "def get_answer_cands(args, scanqa):\n    answer_counter = sum([data[\"answers\"] for data in scanqa[\"train\"]], [])\n    answer_counter = collections.Counter(sorted(answer_counter))\n    num_all_answers = len(answer_counter)\n    answer_max_size = args.answer_max_size\n    if answer_max_size < 0:\n        answer_max_size = len(answer_counter)\n    answer_counter = dict([x for x in answer_counter.most_common()[:answer_max_size] if x[1] >= args.answer_min_freq])\n    print(\"using {} answers out of {} ones\".format(len(answer_counter), num_all_answers))    \n    answer_cands = sorted(answer_counter.keys())",
        "detail": "3D-VLP.scripts.train_scripts.train_ft_qa",
        "documentation": {}
    },
    {
        "label": "get_dataloader",
        "kind": 2,
        "importPath": "3D-VLP.scripts.train_scripts.train_ft_qa",
        "description": "3D-VLP.scripts.train_scripts.train_ft_qa",
        "peekOfCode": "def get_dataloader(args, scanqa, all_scene_list, split, config, augment):\n    answer_cands, answer_counter = get_answer_cands(args, scanqa)\n    config.num_answers = len(answer_cands)\n    if 'bert-' in args.tokenizer_name: \n        from transformers import AutoTokenizer\n        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n        tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name)\n    else:\n        tokenizer = None\n    dataset = ScannetQADataset(",
        "detail": "3D-VLP.scripts.train_scripts.train_ft_qa",
        "documentation": {}
    },
    {
        "label": "get_model",
        "kind": 2,
        "importPath": "3D-VLP.scripts.train_scripts.train_ft_qa",
        "description": "3D-VLP.scripts.train_scripts.train_ft_qa",
        "peekOfCode": "def get_model(args, config):\n    if \"bert-\" in args.tokenizer_name:\n        from transformers import AutoConfig\n        bert_model_name = args.tokenizer_name\n        bert_config = AutoConfig.from_pretrained(bert_model_name)\n        if hasattr(bert_config, \"hidden_size\"):\n            lang_emb_size = bert_config.hidden_size\n        else:\n            # for distllbert\n            lang_emb_size = bert_config.dim",
        "detail": "3D-VLP.scripts.train_scripts.train_ft_qa",
        "documentation": {}
    },
    {
        "label": "get_num_params",
        "kind": 2,
        "importPath": "3D-VLP.scripts.train_scripts.train_ft_qa",
        "description": "3D-VLP.scripts.train_scripts.train_ft_qa",
        "peekOfCode": "def get_num_params(model):\n    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n    num_params = int(sum([np.prod(p.size()) for p in model_parameters]))\n    return num_params\ndef get_solver(args, dataloader):\n    model = get_model(args, DC)\n    #wandb.watch(model, log_freq=100)\n    # lr 0.0005\n    weight_dict = {\n        'backbone_net': {'lr': 0.0001},",
        "detail": "3D-VLP.scripts.train_scripts.train_ft_qa",
        "documentation": {}
    },
    {
        "label": "get_solver",
        "kind": 2,
        "importPath": "3D-VLP.scripts.train_scripts.train_ft_qa",
        "description": "3D-VLP.scripts.train_scripts.train_ft_qa",
        "peekOfCode": "def get_solver(args, dataloader):\n    model = get_model(args, DC)\n    #wandb.watch(model, log_freq=100)\n    # lr 0.0005\n    weight_dict = {\n        'backbone_net': {'lr': 0.0001},\n        'vgen': {'lr': 0.0001},\n        'proposal': {'lr': 0.0001},\n        'relation': {'lr': 0.0001}\n    }",
        "detail": "3D-VLP.scripts.train_scripts.train_ft_qa",
        "documentation": {}
    },
    {
        "label": "save_info",
        "kind": 2,
        "importPath": "3D-VLP.scripts.train_scripts.train_ft_qa",
        "description": "3D-VLP.scripts.train_scripts.train_ft_qa",
        "peekOfCode": "def save_info(args, root, num_params, train_dataset, val_dataset):\n    info = {}\n    for key, value in vars(args).items():\n        info[key] = value\n    info[\"num_train\"] = len(train_dataset)\n    info[\"num_val\"] = len(val_dataset)\n    info[\"num_train_scenes\"] = len(train_dataset.scene_list)\n    info[\"num_val_scenes\"] = len(val_dataset.scene_list)\n    info[\"num_params\"] = num_params\n    with open(os.path.join(root, \"info.json\"), \"w\") as f:",
        "detail": "3D-VLP.scripts.train_scripts.train_ft_qa",
        "documentation": {}
    },
    {
        "label": "get_scannet_scene_list",
        "kind": 2,
        "importPath": "3D-VLP.scripts.train_scripts.train_ft_qa",
        "description": "3D-VLP.scripts.train_scripts.train_ft_qa",
        "peekOfCode": "def get_scannet_scene_list(split):\n    scene_list = sorted([line.rstrip() for line in open(os.path.join(CONF.PATH.SCANNET_META, \"scannetv2_{}.txt\".format(split)))])\n    return scene_list\ndef get_scanqa(scanqa_train, scanqa_val, train_num_scenes, val_num_scenes):\n    # get initial scene list\n    train_scene_list = sorted(list(set([data[\"scene_id\"] for data in scanqa_train])))\n    val_scene_list = sorted(list(set([data[\"scene_id\"] for data in scanqa_val])))\n    # set train_num_scenes\n    if train_num_scenes <= -1: \n        train_num_scenes = len(train_scene_list)",
        "detail": "3D-VLP.scripts.train_scripts.train_ft_qa",
        "documentation": {}
    },
    {
        "label": "get_scanqa",
        "kind": 2,
        "importPath": "3D-VLP.scripts.train_scripts.train_ft_qa",
        "description": "3D-VLP.scripts.train_scripts.train_ft_qa",
        "peekOfCode": "def get_scanqa(scanqa_train, scanqa_val, train_num_scenes, val_num_scenes):\n    # get initial scene list\n    train_scene_list = sorted(list(set([data[\"scene_id\"] for data in scanqa_train])))\n    val_scene_list = sorted(list(set([data[\"scene_id\"] for data in scanqa_val])))\n    # set train_num_scenes\n    if train_num_scenes <= -1: \n        train_num_scenes = len(train_scene_list)\n    else:\n        assert len(train_scene_list) >= train_num_scenes\n    # slice train_scene_list",
        "detail": "3D-VLP.scripts.train_scripts.train_ft_qa",
        "documentation": {}
    },
    {
        "label": "train",
        "kind": 2,
        "importPath": "3D-VLP.scripts.train_scripts.train_ft_qa",
        "description": "3D-VLP.scripts.train_scripts.train_ft_qa",
        "peekOfCode": "def train(args):\n    # WandB init    \n    # wandb.init(project=project_name, config=args)\n    # init training dataset\n    print(\"preparing data...\")\n    scanqa_train, scanqa_val, all_scene_list = get_scanqa(SCANQA_TRAIN, SCANQA_VAL, args.train_num_scenes, args.val_num_scenes)\n    scanqa = {\n        \"train\": scanqa_train,\n        \"val\": scanqa_val\n    }",
        "detail": "3D-VLP.scripts.train_scripts.train_ft_qa",
        "documentation": {}
    },
    {
        "label": "project_name",
        "kind": 5,
        "importPath": "3D-VLP.scripts.train_scripts.train_ft_qa",
        "description": "3D-VLP.scripts.train_scripts.train_ft_qa",
        "peekOfCode": "project_name = \"ScanQA_v1.0\"\nSCANQA_TRAIN = json.load(open(os.path.join(CONF.PATH.SCANQA, project_name + \"_train.json\"))) \nSCANQA_VAL = json.load(open(os.path.join(CONF.PATH.SCANQA, project_name + \"_val.json\")))\n# constants\nDC = ScannetQADatasetConfig()\ndef parse_option():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--debug\", action=\"store_true\", help=\"debugging mode\")\n    parser.add_argument(\"--tag\", type=str, help=\"tag for the training, e.g. XYZ_COLOR\", default=\"QA\")\n    parser.add_argument(\"--gpu\", type=str, help=\"gpu\", default=\"0\")",
        "detail": "3D-VLP.scripts.train_scripts.train_ft_qa",
        "documentation": {}
    },
    {
        "label": "SCANQA_TRAIN",
        "kind": 5,
        "importPath": "3D-VLP.scripts.train_scripts.train_ft_qa",
        "description": "3D-VLP.scripts.train_scripts.train_ft_qa",
        "peekOfCode": "SCANQA_TRAIN = json.load(open(os.path.join(CONF.PATH.SCANQA, project_name + \"_train.json\"))) \nSCANQA_VAL = json.load(open(os.path.join(CONF.PATH.SCANQA, project_name + \"_val.json\")))\n# constants\nDC = ScannetQADatasetConfig()\ndef parse_option():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--debug\", action=\"store_true\", help=\"debugging mode\")\n    parser.add_argument(\"--tag\", type=str, help=\"tag for the training, e.g. XYZ_COLOR\", default=\"QA\")\n    parser.add_argument(\"--gpu\", type=str, help=\"gpu\", default=\"0\")\n    # Training",
        "detail": "3D-VLP.scripts.train_scripts.train_ft_qa",
        "documentation": {}
    },
    {
        "label": "SCANQA_VAL",
        "kind": 5,
        "importPath": "3D-VLP.scripts.train_scripts.train_ft_qa",
        "description": "3D-VLP.scripts.train_scripts.train_ft_qa",
        "peekOfCode": "SCANQA_VAL = json.load(open(os.path.join(CONF.PATH.SCANQA, project_name + \"_val.json\")))\n# constants\nDC = ScannetQADatasetConfig()\ndef parse_option():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--debug\", action=\"store_true\", help=\"debugging mode\")\n    parser.add_argument(\"--tag\", type=str, help=\"tag for the training, e.g. XYZ_COLOR\", default=\"QA\")\n    parser.add_argument(\"--gpu\", type=str, help=\"gpu\", default=\"0\")\n    # Training\n    parser.add_argument(\"--cur_criterion\", type=str, default=\"answer_acc_at1\", help=\"data augmentation type\")",
        "detail": "3D-VLP.scripts.train_scripts.train_ft_qa",
        "documentation": {}
    },
    {
        "label": "DC",
        "kind": 5,
        "importPath": "3D-VLP.scripts.train_scripts.train_ft_qa",
        "description": "3D-VLP.scripts.train_scripts.train_ft_qa",
        "peekOfCode": "DC = ScannetQADatasetConfig()\ndef parse_option():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--debug\", action=\"store_true\", help=\"debugging mode\")\n    parser.add_argument(\"--tag\", type=str, help=\"tag for the training, e.g. XYZ_COLOR\", default=\"QA\")\n    parser.add_argument(\"--gpu\", type=str, help=\"gpu\", default=\"0\")\n    # Training\n    parser.add_argument(\"--cur_criterion\", type=str, default=\"answer_acc_at1\", help=\"data augmentation type\")\n    parser.add_argument(\"--batch_size\", type=int, help=\"batch size\", default=16)\n    parser.add_argument(\"--epoch\", type=int, help=\"number of epochs\", default=30)",
        "detail": "3D-VLP.scripts.train_scripts.train_ft_qa",
        "documentation": {}
    },
    {
        "label": "get_dataloader",
        "kind": 2,
        "importPath": "3D-VLP.scripts.train_scripts.train_pretrain",
        "description": "3D-VLP.scripts.train_scripts.train_pretrain",
        "peekOfCode": "def get_dataloader(args, scanrefer, scanrefer_new, all_scene_list, split, config, augment, shuffle=True,\n                   scan2cad_rotation=None):\n    dataset = ScannetReferenceDataset(\n        scanrefer=scanrefer,\n        scanrefer_new=scanrefer_new,\n        scanrefer_all_scene=all_scene_list,\n        split=split,\n        name=args.dataset,\n        num_points=args.num_points,\n        use_height=(not args.no_height),",
        "detail": "3D-VLP.scripts.train_scripts.train_pretrain",
        "documentation": {}
    },
    {
        "label": "get_model",
        "kind": 2,
        "importPath": "3D-VLP.scripts.train_scripts.train_pretrain",
        "description": "3D-VLP.scripts.train_scripts.train_pretrain",
        "peekOfCode": "def get_model(args, dataset, device):\n    # initiate model\n    print('multiview', args.use_multiview)\n    input_channels = int(args.use_multiview) * 128 + int(args.use_normal) * 3 + int(args.use_color) * 3 + int(\n        not args.no_height)\n    print('input channels', input_channels)\n    model = JointNet(\n        num_class=DC.num_class,\n        num_heading_bin=DC.num_heading_bin,\n        num_size_cluster=DC.num_size_cluster,",
        "detail": "3D-VLP.scripts.train_scripts.train_pretrain",
        "documentation": {}
    },
    {
        "label": "get_num_params",
        "kind": 2,
        "importPath": "3D-VLP.scripts.train_scripts.train_pretrain",
        "description": "3D-VLP.scripts.train_scripts.train_pretrain",
        "peekOfCode": "def get_num_params(model):\n    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n    num_params = int(sum([np.prod(p.size()) for p in model_parameters]))\n    return num_params\ndef get_solver(args, dataset, dataloader):\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    model = get_model(args, dataset[\"train\"], device)\n    # TODO\n    # weight_dict = {\n    #     'backbone_net': {'lr': 0.0001},",
        "detail": "3D-VLP.scripts.train_scripts.train_pretrain",
        "documentation": {}
    },
    {
        "label": "get_solver",
        "kind": 2,
        "importPath": "3D-VLP.scripts.train_scripts.train_pretrain",
        "description": "3D-VLP.scripts.train_scripts.train_pretrain",
        "peekOfCode": "def get_solver(args, dataset, dataloader):\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    model = get_model(args, dataset[\"train\"], device)\n    # TODO\n    # weight_dict = {\n    #     'backbone_net': {'lr': 0.0001},\n    #     'vgen': {'lr': 0.0001},\n    #     'proposal': {'lr': 0.0001},\n    #     'lang': {'lr': 0.0005},\n    #     'relation': {'lr': 0.0005},",
        "detail": "3D-VLP.scripts.train_scripts.train_pretrain",
        "documentation": {}
    },
    {
        "label": "save_info",
        "kind": 2,
        "importPath": "3D-VLP.scripts.train_scripts.train_pretrain",
        "description": "3D-VLP.scripts.train_scripts.train_pretrain",
        "peekOfCode": "def save_info(args, root, num_params, dataset):\n    info = {}\n    for key, value in vars(args).items():\n        info[key] = value\n    info[\"num_train\"] = len(dataset[\"train\"])\n    info[\"num_eval_train\"] = len(dataset[\"eval\"][\"train\"])\n    info[\"num_eval_val\"] = len(dataset[\"eval\"][\"val\"])\n    info[\"num_train_scenes\"] = len(dataset[\"train\"].scene_list)\n    info[\"num_eval_train_scenes\"] = len(dataset[\"eval\"][\"train\"].scene_list)\n    info[\"num_eval_val_scenes\"] = len(dataset[\"eval\"][\"val\"].scene_list)",
        "detail": "3D-VLP.scripts.train_scripts.train_pretrain",
        "documentation": {}
    },
    {
        "label": "get_scannet_scene_list",
        "kind": 2,
        "importPath": "3D-VLP.scripts.train_scripts.train_pretrain",
        "description": "3D-VLP.scripts.train_scripts.train_pretrain",
        "peekOfCode": "def get_scannet_scene_list(split):\n    scene_list = sorted(\n        [line.rstrip() for line in open(os.path.join(CONF.PATH.SCANNET_META, \"scannetv2_{}.txt\".format(split)))])\n    return scene_list\ndef get_scanrefer(args):\n    if args.dataset == \"ScanRefer\":\n        scanrefer_train = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_train.json\")))\n        scanrefer_eval_train = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_train.json\")))\n        scanrefer_eval_val = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_val.json\")))\n    elif args.dataset == \"ReferIt3D\":",
        "detail": "3D-VLP.scripts.train_scripts.train_pretrain",
        "documentation": {}
    },
    {
        "label": "get_scanrefer",
        "kind": 2,
        "importPath": "3D-VLP.scripts.train_scripts.train_pretrain",
        "description": "3D-VLP.scripts.train_scripts.train_pretrain",
        "peekOfCode": "def get_scanrefer(args):\n    if args.dataset == \"ScanRefer\":\n        scanrefer_train = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_train.json\")))\n        scanrefer_eval_train = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_train.json\")))\n        scanrefer_eval_val = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_val.json\")))\n    elif args.dataset == \"ReferIt3D\":\n        scanrefer_train = json.load(open(os.path.join(CONF.PATH.DATA, \"nr3d_train.json\")))\n        scanrefer_eval_train = json.load(open(os.path.join(CONF.PATH.DATA, \"nr3d_train.json\")))\n        scanrefer_eval_val = json.load(open(os.path.join(CONF.PATH.DATA, \"nr3d_val.json\")))\n    else:",
        "detail": "3D-VLP.scripts.train_scripts.train_pretrain",
        "documentation": {}
    },
    {
        "label": "train",
        "kind": 2,
        "importPath": "3D-VLP.scripts.train_scripts.train_pretrain",
        "description": "3D-VLP.scripts.train_scripts.train_pretrain",
        "peekOfCode": "def train(args):\n    # init training dataset\n    print(\"preparing data...\")\n    scanrefer_train, scanrefer_eval_train, scanrefer_eval_val, scanrefer_eval_val2, all_scene_list, scanrefer_train_new, scanrefer_eval_train_new, scanrefer_eval_val_new, scanrefer_eval_val_new2 = get_scanrefer(\n        args)\n    # eval_train_dataset\n    # dataloader\n    train_dataset, train_dataloader = get_dataloader(args, scanrefer_train, scanrefer_train_new, all_scene_list,\n                                                     \"train\", DC, True, SCAN2CAD_ROTATION)\n    eval_train_dataset, eval_train_dataloader = get_dataloader(args, scanrefer_eval_train, scanrefer_eval_train_new,",
        "detail": "3D-VLP.scripts.train_scripts.train_pretrain",
        "documentation": {}
    },
    {
        "label": "SCANREFER_TRAIN",
        "kind": 5,
        "importPath": "3D-VLP.scripts.train_scripts.train_pretrain",
        "description": "3D-VLP.scripts.train_scripts.train_pretrain",
        "peekOfCode": "SCANREFER_TRAIN = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_train.json\")))\nSCANREFER_VAL = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_val.json\")))\nSCAN2CAD_ROTATION = json.load(open(os.path.join(CONF.PATH.SCAN2CAD, \"scannet_instance_rotations.json\")))\n# constants\nDC = ScannetDatasetConfig()\ndef get_dataloader(args, scanrefer, scanrefer_new, all_scene_list, split, config, augment, shuffle=True,\n                   scan2cad_rotation=None):\n    dataset = ScannetReferenceDataset(\n        scanrefer=scanrefer,\n        scanrefer_new=scanrefer_new,",
        "detail": "3D-VLP.scripts.train_scripts.train_pretrain",
        "documentation": {}
    },
    {
        "label": "SCANREFER_VAL",
        "kind": 5,
        "importPath": "3D-VLP.scripts.train_scripts.train_pretrain",
        "description": "3D-VLP.scripts.train_scripts.train_pretrain",
        "peekOfCode": "SCANREFER_VAL = json.load(open(os.path.join(CONF.PATH.DATA, \"ScanRefer_filtered_val.json\")))\nSCAN2CAD_ROTATION = json.load(open(os.path.join(CONF.PATH.SCAN2CAD, \"scannet_instance_rotations.json\")))\n# constants\nDC = ScannetDatasetConfig()\ndef get_dataloader(args, scanrefer, scanrefer_new, all_scene_list, split, config, augment, shuffle=True,\n                   scan2cad_rotation=None):\n    dataset = ScannetReferenceDataset(\n        scanrefer=scanrefer,\n        scanrefer_new=scanrefer_new,\n        scanrefer_all_scene=all_scene_list,",
        "detail": "3D-VLP.scripts.train_scripts.train_pretrain",
        "documentation": {}
    },
    {
        "label": "SCAN2CAD_ROTATION",
        "kind": 5,
        "importPath": "3D-VLP.scripts.train_scripts.train_pretrain",
        "description": "3D-VLP.scripts.train_scripts.train_pretrain",
        "peekOfCode": "SCAN2CAD_ROTATION = json.load(open(os.path.join(CONF.PATH.SCAN2CAD, \"scannet_instance_rotations.json\")))\n# constants\nDC = ScannetDatasetConfig()\ndef get_dataloader(args, scanrefer, scanrefer_new, all_scene_list, split, config, augment, shuffle=True,\n                   scan2cad_rotation=None):\n    dataset = ScannetReferenceDataset(\n        scanrefer=scanrefer,\n        scanrefer_new=scanrefer_new,\n        scanrefer_all_scene=all_scene_list,\n        split=split,",
        "detail": "3D-VLP.scripts.train_scripts.train_pretrain",
        "documentation": {}
    },
    {
        "label": "DC",
        "kind": 5,
        "importPath": "3D-VLP.scripts.train_scripts.train_pretrain",
        "description": "3D-VLP.scripts.train_scripts.train_pretrain",
        "peekOfCode": "DC = ScannetDatasetConfig()\ndef get_dataloader(args, scanrefer, scanrefer_new, all_scene_list, split, config, augment, shuffle=True,\n                   scan2cad_rotation=None):\n    dataset = ScannetReferenceDataset(\n        scanrefer=scanrefer,\n        scanrefer_new=scanrefer_new,\n        scanrefer_all_scene=all_scene_list,\n        split=split,\n        name=args.dataset,\n        num_points=args.num_points,",
        "detail": "3D-VLP.scripts.train_scripts.train_pretrain",
        "documentation": {}
    },
    {
        "label": "AdamW",
        "kind": 6,
        "importPath": "3D-VLP.scripts.utils.AdamW",
        "description": "3D-VLP.scripts.utils.AdamW",
        "peekOfCode": "class AdamW(Optimizer):\n    r\"\"\"Implements AdamW algorithm.\n    The original Adam algorithm was proposed in `Adam: A Method for Stochastic Optimization`_.\n    The AdamW variant was proposed in `Decoupled Weight Decay Regularization`_.\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float, optional): learning rate (default: 1e-3)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square (default: (0.9, 0.999))",
        "detail": "3D-VLP.scripts.utils.AdamW",
        "documentation": {}
    },
    {
        "label": "set_params_lr_dict",
        "kind": 2,
        "importPath": "3D-VLP.scripts.utils.script_utils",
        "description": "3D-VLP.scripts.utils.script_utils",
        "peekOfCode": "def set_params_lr_dict(model, base_lr, weight_decay, weight_dict, output=True):\n    if output:  # output parms dict\n        print('Set params dict lr!', weight_dict, 'Base:', base_lr, weight_decay)\n    params_dict = weight_dict\n    assert 'Default' not in params_dict.keys(), 'KEY \\'Default\\' should not in weight_dict(automantic set)'\n    params_dict['Default'] = {}\n    for name in params_dict.keys():\n        params_dict[name]['params'] = []\n    for name, param in model.named_parameters():\n        result_key = 'Default'",
        "detail": "3D-VLP.scripts.utils.script_utils",
        "documentation": {}
    },
    {
        "label": "COLORS",
        "kind": 5,
        "importPath": "3D-VLP.scripts.colors",
        "description": "3D-VLP.scripts.colors",
        "peekOfCode": "COLORS = [\n    [174, 199, 232],\t\t# wall\n    [152, 223, 138],\t\t# floor\n    [31, 119, 180], \t\t# cabinet\n    [255, 187, 120],\t\t# bed\n    [188, 189, 34], \t\t# chair\n    [140, 86, 75],  \t\t# sofa\n    [255, 152, 150],\t\t# table\n    [214, 39, 40],  \t\t# door\n    [197, 176, 213],\t\t# window",
        "detail": "3D-VLP.scripts.colors",
        "documentation": {}
    },
    {
        "label": "output_bounding_box",
        "kind": 2,
        "importPath": "3D-VLP.scripts.core_vision_utils",
        "description": "3D-VLP.scripts.core_vision_utils",
        "peekOfCode": "def output_bounding_box(id_, point, color, heat, file_out, output_face=True, output_line=True): # color=(1,1,1),p=1: red\n    # print(p, '<< heatmap')\n    p = heat\n    if point.shape[-1] == 6:  # convert center-offset to new_point\n        pc, sz = point[:3], point[3:] / 2\n        point = np.zeros([8, 3])\n        off = [[1, 1, 1, 1, -1, -1, -1, -1], \\\n               [1, 1, -1, -1, 1, 1, -1, -1], \\\n               [1, -1, 1, -1, 1, -1, 1, -1]]\n        for i in range(8):",
        "detail": "3D-VLP.scripts.core_vision_utils",
        "documentation": {}
    },
    {
        "label": "save_bbox_heatmap",
        "kind": 2,
        "importPath": "3D-VLP.scripts.core_vision_utils",
        "description": "3D-VLP.scripts.core_vision_utils",
        "peekOfCode": "def save_bbox_heatmap(bboxes, heatmap, save_base = os.getcwd()+'/heatmap_result', save_name='', kth_input=None, color=[1, 0, 0]): # save_name: text\n    #print(\"bboxes\", bboxes.shape)\n    #print(\"heatmap\", heatmap.shape)\n    #color = color.cpu().numpy()\n    save_base = os.path.join(save_base, save_name)\n    print(save_base, flush=True)\n    if not os.path.exists(save_base):\n        os.makedirs(save_base)\n    if kth_input is not None:\n        for idx in range(heatmap.shape[0]):  # idx: object",
        "detail": "3D-VLP.scripts.core_vision_utils",
        "documentation": {}
    },
    {
        "label": "save_bbox_heatmap_cross",
        "kind": 2,
        "importPath": "3D-VLP.scripts.core_vision_utils",
        "description": "3D-VLP.scripts.core_vision_utils",
        "peekOfCode": "def save_bbox_heatmap_cross(bboxes, heatmap, save_base = os.getcwd()+'/heatmap_result_cross', save_name='', kth_input=None, color=[1, 0, 0]): # save_name: text\n    #print(\"bboxes\", bboxes.shape)\n    #print(\"heatmap\", heatmap.shape)\n    #color = color.cpu().numpy()\n    save_base = os.path.join(save_base, save_name)\n    print(save_base, flush=True)\n    if not os.path.exists(save_base):\n        os.makedirs(save_base)\n    heatmap = torch.mean(heatmap, dim=0, keepdim=True).transpose(0, 1)  # (32, 1, proposal)\n    for idx in range(heatmap.shape[0]):  # idx: object",
        "detail": "3D-VLP.scripts.core_vision_utils",
        "documentation": {}
    },
    {
        "label": "polygon_clip",
        "kind": 2,
        "importPath": "3D-VLP.utils.box_util",
        "description": "3D-VLP.utils.box_util",
        "peekOfCode": "def polygon_clip(subjectPolygon, clipPolygon):\n   \"\"\" Clip a polygon with another polygon.\n   Ref: https://rosettacode.org/wiki/Sutherland-Hodgman_polygon_clipping#Python\n   Args:\n     subjectPolygon: a list of (x,y) 2d points, any polygon.\n     clipPolygon: a list of (x,y) 2d points, has to be *convex*\n   Note:\n     **points have to be counter-clockwise ordered**\n   Return:\n     a list of (x,y) vertex point for the intersection polygon.",
        "detail": "3D-VLP.utils.box_util",
        "documentation": {}
    },
    {
        "label": "poly_area",
        "kind": 2,
        "importPath": "3D-VLP.utils.box_util",
        "description": "3D-VLP.utils.box_util",
        "peekOfCode": "def poly_area(x,y):\n    \"\"\" Ref: http://stackoverflow.com/questions/24467972/calculate-area-of-polygon-given-x-y-coordinates \"\"\"\n    return 0.5*np.abs(np.dot(x,np.roll(y,1))-np.dot(y,np.roll(x,1)))\ndef poly_area_batch(x,y):\n    \"\"\" Ref: http://stackoverflow.com/questions/24467972/calculate-area-of-polygon-given-x-y-coordinates \"\"\"    \n    return 0.5 * np.abs(np.matmul(np.expand_dims(x, axis=1), np.roll(np.expand_dims(y, axis=2), 1, axis=1)) \\\n        - np.matmul(np.expand_dims(y, axis=1), np.roll(np.expand_dims(x, axis=2), 1, axis=1))).squeeze(axis=(1,2))\ndef convex_hull_intersection(p1, p2):\n    \"\"\" Compute area of two convex hull's intersection area.\n        p1,p2 are a list of (x,y) tuples of hull vertices.",
        "detail": "3D-VLP.utils.box_util",
        "documentation": {}
    },
    {
        "label": "poly_area_batch",
        "kind": 2,
        "importPath": "3D-VLP.utils.box_util",
        "description": "3D-VLP.utils.box_util",
        "peekOfCode": "def poly_area_batch(x,y):\n    \"\"\" Ref: http://stackoverflow.com/questions/24467972/calculate-area-of-polygon-given-x-y-coordinates \"\"\"    \n    return 0.5 * np.abs(np.matmul(np.expand_dims(x, axis=1), np.roll(np.expand_dims(y, axis=2), 1, axis=1)) \\\n        - np.matmul(np.expand_dims(y, axis=1), np.roll(np.expand_dims(x, axis=2), 1, axis=1))).squeeze(axis=(1,2))\ndef convex_hull_intersection(p1, p2):\n    \"\"\" Compute area of two convex hull's intersection area.\n        p1,p2 are a list of (x,y) tuples of hull vertices.\n        return a list of (x,y) for the intersection and its volume\n    \"\"\"\n    inter_p = polygon_clip(p1,p2)",
        "detail": "3D-VLP.utils.box_util",
        "documentation": {}
    },
    {
        "label": "convex_hull_intersection",
        "kind": 2,
        "importPath": "3D-VLP.utils.box_util",
        "description": "3D-VLP.utils.box_util",
        "peekOfCode": "def convex_hull_intersection(p1, p2):\n    \"\"\" Compute area of two convex hull's intersection area.\n        p1,p2 are a list of (x,y) tuples of hull vertices.\n        return a list of (x,y) for the intersection and its volume\n    \"\"\"\n    inter_p = polygon_clip(p1,p2)\n    if inter_p is not None:\n        hull_inter = ConvexHull(inter_p)\n        return inter_p, hull_inter.volume\n    else:",
        "detail": "3D-VLP.utils.box_util",
        "documentation": {}
    },
    {
        "label": "box3d_vol",
        "kind": 2,
        "importPath": "3D-VLP.utils.box_util",
        "description": "3D-VLP.utils.box_util",
        "peekOfCode": "def box3d_vol(corners):\n    ''' corners: (8,3) no assumption on axis direction '''\n    a = np.sqrt(np.sum((corners[0,:] - corners[1,:])**2))\n    b = np.sqrt(np.sum((corners[1,:] - corners[2,:])**2))\n    c = np.sqrt(np.sum((corners[0,:] - corners[4,:])**2))\n    return a*b*c\ndef is_clockwise(p):\n    x = p[:,0]\n    y = p[:,1]\n    return np.dot(x,np.roll(y,1))-np.dot(y,np.roll(x,1)) > 0",
        "detail": "3D-VLP.utils.box_util",
        "documentation": {}
    },
    {
        "label": "is_clockwise",
        "kind": 2,
        "importPath": "3D-VLP.utils.box_util",
        "description": "3D-VLP.utils.box_util",
        "peekOfCode": "def is_clockwise(p):\n    x = p[:,0]\n    y = p[:,1]\n    return np.dot(x,np.roll(y,1))-np.dot(y,np.roll(x,1)) > 0\ndef box3d_iou(corners1, corners2):\n    ''' Compute 3D bounding box IoU.\n    Input:\n        corners1: numpy array (8,3), assume up direction is Z\n        corners2: numpy array (8,3), assume up direction is Z\n    Output:",
        "detail": "3D-VLP.utils.box_util",
        "documentation": {}
    },
    {
        "label": "box3d_iou",
        "kind": 2,
        "importPath": "3D-VLP.utils.box_util",
        "description": "3D-VLP.utils.box_util",
        "peekOfCode": "def box3d_iou(corners1, corners2):\n    ''' Compute 3D bounding box IoU.\n    Input:\n        corners1: numpy array (8,3), assume up direction is Z\n        corners2: numpy array (8,3), assume up direction is Z\n    Output:\n        iou: 3D bounding box IoU\n    '''\n    # # corner points are in counter clockwise order\n    # rect1 = [(corners1[i,0], corners1[i,2]) for i in range(3,-1,-1)]",
        "detail": "3D-VLP.utils.box_util",
        "documentation": {}
    },
    {
        "label": "get_box3d_min_max",
        "kind": 2,
        "importPath": "3D-VLP.utils.box_util",
        "description": "3D-VLP.utils.box_util",
        "peekOfCode": "def get_box3d_min_max(corner):\n    ''' Compute min and max coordinates for 3D bounding box\n        Note: only for axis-aligned bounding boxes\n    Input:\n        corners: numpy array (8,3), assume up direction is Z (batch of N samples)\n    Output:\n        box_min_max: an array for min and max coordinates of 3D bounding box IoU\n    '''\n    min_coord = corner.min(axis=0)\n    max_coord = corner.max(axis=0)",
        "detail": "3D-VLP.utils.box_util",
        "documentation": {}
    },
    {
        "label": "box3d_iou_batch",
        "kind": 2,
        "importPath": "3D-VLP.utils.box_util",
        "description": "3D-VLP.utils.box_util",
        "peekOfCode": "def box3d_iou_batch(corners1, corners2):\n    ''' Compute 3D bounding box IoU.\n        Note: only for axis-aligned bounding boxes\n    Input:\n        corners1: numpy array (N,8,3), assume up direction is Z (batch of N samples)\n        corners2: numpy array (N,8,3), assume up direction is Z (batch of N samples)\n    Output:\n        iou: an array of 3D bounding box IoU\n    '''\n    x_min_1, x_max_1, y_min_1, y_max_1, z_min_1, z_max_1 = get_box3d_min_max_batch(corners1)",
        "detail": "3D-VLP.utils.box_util",
        "documentation": {}
    },
    {
        "label": "box3d_iou_batch_tensor",
        "kind": 2,
        "importPath": "3D-VLP.utils.box_util",
        "description": "3D-VLP.utils.box_util",
        "peekOfCode": "def box3d_iou_batch_tensor(corners1, corners2):\n    ''' Compute 3D bounding box IoU.\n        Note: only for axis-aligned bounding boxes\n    Input:\n        corners1: PyTorch tensor (N,8,3), assume up direction is Z (batch of N samples)\n        corners2: PyTorch tensor (N,8,3), assume up direction is Z (batch of N samples)\n    Output:\n        iou: an tensor of 3D bounding box IoU (N)\n    '''\n    x_min_1, x_max_1, y_min_1, y_max_1, z_min_1, z_max_1 = get_box3d_min_max_batch_tensor(corners1)",
        "detail": "3D-VLP.utils.box_util",
        "documentation": {}
    },
    {
        "label": "get_box3d_min_max_batch",
        "kind": 2,
        "importPath": "3D-VLP.utils.box_util",
        "description": "3D-VLP.utils.box_util",
        "peekOfCode": "def get_box3d_min_max_batch(corner):\n    ''' Compute min and max coordinates for 3D bounding box\n        Note: only for axis-aligned bounding boxes\n    Input:\n        corners: numpy array (N,8,3), assume up direction is Z (batch of N samples)\n    Output:\n        box_min_max: an array for min and max coordinates of 3D bounding box IoU\n    '''\n    min_coord = corner.min(axis=1)\n    max_coord = corner.max(axis=1)",
        "detail": "3D-VLP.utils.box_util",
        "documentation": {}
    },
    {
        "label": "get_box3d_min_max_batch_tensor",
        "kind": 2,
        "importPath": "3D-VLP.utils.box_util",
        "description": "3D-VLP.utils.box_util",
        "peekOfCode": "def get_box3d_min_max_batch_tensor(corner):\n    ''' Compute min and max coordinates for 3D bounding box\n        Note: only for axis-aligned bounding boxes\n    Input:\n        corners: PyTorch tensor (N,8,3), assume up direction is Z (batch of N samples)\n    Output:\n        box_min_max: an tensor for min and max coordinates of 3D bounding box IoU\n    '''\n    min_coord, _ = corner.min(dim=1)\n    max_coord, _ = corner.max(dim=1)",
        "detail": "3D-VLP.utils.box_util",
        "documentation": {}
    },
    {
        "label": "get_iou",
        "kind": 2,
        "importPath": "3D-VLP.utils.box_util",
        "description": "3D-VLP.utils.box_util",
        "peekOfCode": "def get_iou(bb1, bb2):\n    \"\"\"\n    Calculate the Intersection over Union (IoU) of two 2D bounding boxes.\n    Parameters\n    ----------\n    bb1 : dict\n        Keys: {'x1', 'x2', 'y1', 'y2'}\n        The (x1, y1) position is at the top left corner,\n        the (x2, y2) position is at the bottom right corner\n    bb2 : dict",
        "detail": "3D-VLP.utils.box_util",
        "documentation": {}
    },
    {
        "label": "box2d_iou",
        "kind": 2,
        "importPath": "3D-VLP.utils.box_util",
        "description": "3D-VLP.utils.box_util",
        "peekOfCode": "def box2d_iou(box1, box2):\n    ''' Compute 2D bounding box IoU.\n    Input:\n        box1: tuple of (xmin,ymin,xmax,ymax)\n        box2: tuple of (xmin,ymin,xmax,ymax)\n    Output:\n        iou: 2D IoU scalar\n    '''\n    return get_iou({'x1':box1[0], 'y1':box1[1], 'x2':box1[2], 'y2':box1[3]}, \\\n        {'x1':box2[0], 'y1':box2[1], 'x2':box2[2], 'y2':box2[3]})",
        "detail": "3D-VLP.utils.box_util",
        "documentation": {}
    },
    {
        "label": "roty",
        "kind": 2,
        "importPath": "3D-VLP.utils.box_util",
        "description": "3D-VLP.utils.box_util",
        "peekOfCode": "def roty(t):\n    \"\"\"Rotation about the y-axis.\"\"\"\n    c = np.cos(t)\n    s = np.sin(t)\n    return np.array([[c,  0,  s],\n                    [0,  1,  0],\n                    [-s, 0,  c]])\ndef roty_batch(t):\n    \"\"\"Rotation about the y-axis.\n    t: (x1,x2,...xn)",
        "detail": "3D-VLP.utils.box_util",
        "documentation": {}
    },
    {
        "label": "roty_batch",
        "kind": 2,
        "importPath": "3D-VLP.utils.box_util",
        "description": "3D-VLP.utils.box_util",
        "peekOfCode": "def roty_batch(t):\n    \"\"\"Rotation about the y-axis.\n    t: (x1,x2,...xn)\n    return: (x1,x2,...,xn,3,3)\n    \"\"\"\n    input_shape = t.shape\n    output = np.zeros(tuple(list(input_shape)+[3,3]))\n    c = np.cos(t)\n    s = np.sin(t)\n    output[...,0,0] = c",
        "detail": "3D-VLP.utils.box_util",
        "documentation": {}
    },
    {
        "label": "get_3d_box",
        "kind": 2,
        "importPath": "3D-VLP.utils.box_util",
        "description": "3D-VLP.utils.box_util",
        "peekOfCode": "def get_3d_box(box_size, heading_angle, center):\n    ''' box_size is array(l,w,h), heading_angle is radius clockwise from pos x axis, center is xyz of box center\n        output (8,3) array for 3D box cornders\n        Similar to utils/compute_orientation_3d\n    '''\n    R = roty(heading_angle)  # Actually we should rotz\n    l,w,h = box_size\n    # x_corners = [l/2,l/2,-l/2,-l/2,l/2,l/2,-l/2,-l/2]\n    # y_corners = [h/2,h/2,h/2,h/2,-h/2,-h/2,-h/2,-h/2]\n    # z_corners = [w/2,-w/2,-w/2,w/2,w/2,-w/2,-w/2,w/2]",
        "detail": "3D-VLP.utils.box_util",
        "documentation": {}
    },
    {
        "label": "get_3d_box_batch",
        "kind": 2,
        "importPath": "3D-VLP.utils.box_util",
        "description": "3D-VLP.utils.box_util",
        "peekOfCode": "def get_3d_box_batch(box_size, heading_angle, center):\n    ''' box_size: [x1,x2,...,xn,3]\n        heading_angle: [x1,x2,...,xn]\n        center: [x1,x2,...,xn,3]\n    Return:\n        [x1,x3,...,xn,8,3]\n    '''\n    input_shape = heading_angle.shape\n    R = roty_batch(heading_angle)  # Actually we should rotz\n    l = np.expand_dims(box_size[...,0], -1) # [x1,...,xn,1]",
        "detail": "3D-VLP.utils.box_util",
        "documentation": {}
    },
    {
        "label": "roty_batch_pytorch",
        "kind": 2,
        "importPath": "3D-VLP.utils.box_util",
        "description": "3D-VLP.utils.box_util",
        "peekOfCode": "def roty_batch_pytorch(t):\n    \"\"\"\n    Rotation about z-axis\n    :param t:\n    :return:\n    \"\"\"\n    input_shape = t.shape\n    output = torch.zeros(tuple(list(input_shape)+[3,3])).to(t.device)\n    c = torch.cos(t)\n    s = torch.sin(t)",
        "detail": "3D-VLP.utils.box_util",
        "documentation": {}
    },
    {
        "label": "rotz_batch_pytorch",
        "kind": 2,
        "importPath": "3D-VLP.utils.box_util",
        "description": "3D-VLP.utils.box_util",
        "peekOfCode": "def rotz_batch_pytorch(t):\n    \"\"\"\n    Rotation about z-axis\n    :param t:\n    :return:\n    \"\"\"\n    input_shape = t.shape\n    output = torch.zeros(tuple(list(input_shape)+[3,3])).to(t.device)\n    c = torch.cos(t)\n    s = torch.sin(t)",
        "detail": "3D-VLP.utils.box_util",
        "documentation": {}
    },
    {
        "label": "dist_init",
        "kind": 2,
        "importPath": "3D-VLP.utils.dist",
        "description": "3D-VLP.utils.dist",
        "peekOfCode": "def dist_init(args):\n    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n        args.rank = int(os.environ[\"RANK\"])\n        args.world_size = int(os.environ['WORLD_SIZE'])\n        args.gpu = int(os.environ['LOCAL_RANK'])\n        args.dist_url = 'env://'\n        os.environ['LOCAL_SIZE'] = str(torch.cuda.device_count())\n        print('RANK in os.environ and WORLD_SIZE in os.environ')\n    elif 'SLURM_PROCID' in os.environ:\n        proc_id = int(os.environ['SLURM_PROCID'])",
        "detail": "3D-VLP.utils.dist",
        "documentation": {}
    },
    {
        "label": "get_eta",
        "kind": 2,
        "importPath": "3D-VLP.utils.eta",
        "description": "3D-VLP.utils.eta",
        "peekOfCode": "def get_eta(start, end, extra, num_left):\n    exe_s = end - start\n    eta_s = (exe_s + extra) * num_left\n    eta = {'h': 0, 'm': 0, 's': 0}\n    if eta_s < 60:\n        eta['s'] = int(eta_s)\n    elif eta_s >= 60 and eta_s < 3600:\n        eta['m'] = int(eta_s / 60)\n        eta['s'] = int(eta_s % 60)\n    else:",
        "detail": "3D-VLP.utils.eta",
        "documentation": {}
    },
    {
        "label": "decode_eta",
        "kind": 2,
        "importPath": "3D-VLP.utils.eta",
        "description": "3D-VLP.utils.eta",
        "peekOfCode": "def decode_eta(eta_sec):\n    eta = {'h': 0, 'm': 0, 's': 0}\n    if eta_sec < 60:\n        eta['s'] = int(eta_sec)\n    elif eta_sec >= 60 and eta_sec < 3600:\n        eta['m'] = int(eta_sec / 60)\n        eta['s'] = int(eta_sec % 60)\n    else:\n        eta['h'] = int(eta_sec / (60 * 60))\n        eta['m'] = int(eta_sec % (60 * 60) / 60)",
        "detail": "3D-VLP.utils.eta",
        "documentation": {}
    },
    {
        "label": "voc_ap",
        "kind": 2,
        "importPath": "3D-VLP.utils.eval_det",
        "description": "3D-VLP.utils.eval_det",
        "peekOfCode": "def voc_ap(rec, prec, use_07_metric=False):\n    \"\"\" ap = voc_ap(rec, prec, [use_07_metric])\n    Compute VOC AP given precision and recall.\n    If use_07_metric is true, uses the\n    VOC 07 11 point method (default:False).\n    \"\"\"\n    if use_07_metric:\n        # 11 point metric\n        ap = 0.\n        for t in np.arange(0., 1.1, 0.1):",
        "detail": "3D-VLP.utils.eval_det",
        "documentation": {}
    },
    {
        "label": "get_iou",
        "kind": 2,
        "importPath": "3D-VLP.utils.eval_det",
        "description": "3D-VLP.utils.eval_det",
        "peekOfCode": "def get_iou(bb1, bb2):\n    \"\"\" Compute IoU of two bounding boxes.\n        ** Define your bod IoU function HERE **\n    \"\"\"\n    #pass\n    iou3d = calc_iou(bb1, bb2)\n    return iou3d\nfrom utils.box_util import box3d_iou\ndef get_iou_obb(bb1,bb2):\n    iou3d = box3d_iou(bb1,bb2)",
        "detail": "3D-VLP.utils.eval_det",
        "documentation": {}
    },
    {
        "label": "get_iou_obb",
        "kind": 2,
        "importPath": "3D-VLP.utils.eval_det",
        "description": "3D-VLP.utils.eval_det",
        "peekOfCode": "def get_iou_obb(bb1,bb2):\n    iou3d = box3d_iou(bb1,bb2)\n    return iou3d\ndef get_iou_main(get_iou_func, args):\n    return get_iou_func(*args)\ndef eval_det_cls(pred, gt, ovthresh=0.25, use_07_metric=False, get_iou_func=get_iou):\n    \"\"\" Generic functions to compute precision/recall for object detection\n        for a single class.\n        Input:\n            pred: map of {img_id: [(bbox, score)]} where bbox is numpy array",
        "detail": "3D-VLP.utils.eval_det",
        "documentation": {}
    },
    {
        "label": "get_iou_main",
        "kind": 2,
        "importPath": "3D-VLP.utils.eval_det",
        "description": "3D-VLP.utils.eval_det",
        "peekOfCode": "def get_iou_main(get_iou_func, args):\n    return get_iou_func(*args)\ndef eval_det_cls(pred, gt, ovthresh=0.25, use_07_metric=False, get_iou_func=get_iou):\n    \"\"\" Generic functions to compute precision/recall for object detection\n        for a single class.\n        Input:\n            pred: map of {img_id: [(bbox, score)]} where bbox is numpy array\n            gt: map of {img_id: [bbox]}\n            ovthresh: scalar, iou threshold\n            use_07_metric: bool, if True use VOC07 11 point method",
        "detail": "3D-VLP.utils.eval_det",
        "documentation": {}
    },
    {
        "label": "eval_det_cls",
        "kind": 2,
        "importPath": "3D-VLP.utils.eval_det",
        "description": "3D-VLP.utils.eval_det",
        "peekOfCode": "def eval_det_cls(pred, gt, ovthresh=0.25, use_07_metric=False, get_iou_func=get_iou):\n    \"\"\" Generic functions to compute precision/recall for object detection\n        for a single class.\n        Input:\n            pred: map of {img_id: [(bbox, score)]} where bbox is numpy array\n            gt: map of {img_id: [bbox]}\n            ovthresh: scalar, iou threshold\n            use_07_metric: bool, if True use VOC07 11 point method\n        Output:\n            rec: numpy array of length nd",
        "detail": "3D-VLP.utils.eval_det",
        "documentation": {}
    },
    {
        "label": "eval_det_cls_wrapper",
        "kind": 2,
        "importPath": "3D-VLP.utils.eval_det",
        "description": "3D-VLP.utils.eval_det",
        "peekOfCode": "def eval_det_cls_wrapper(arguments):\n    pred, gt, ovthresh, use_07_metric, get_iou_func = arguments\n    rec, prec, ap = eval_det_cls(pred, gt, ovthresh, use_07_metric, get_iou_func)\n    return (rec, prec, ap)\ndef eval_det(pred_all, gt_all, ovthresh=0.25, use_07_metric=False, get_iou_func=get_iou):\n    \"\"\" Generic functions to compute precision/recall for object detection\n        for multiple classes.\n        Input:\n            pred_all: map of {img_id: [(classname, bbox, score)]}\n            gt_all: map of {img_id: [(classname, bbox)]}",
        "detail": "3D-VLP.utils.eval_det",
        "documentation": {}
    },
    {
        "label": "eval_det",
        "kind": 2,
        "importPath": "3D-VLP.utils.eval_det",
        "description": "3D-VLP.utils.eval_det",
        "peekOfCode": "def eval_det(pred_all, gt_all, ovthresh=0.25, use_07_metric=False, get_iou_func=get_iou):\n    \"\"\" Generic functions to compute precision/recall for object detection\n        for multiple classes.\n        Input:\n            pred_all: map of {img_id: [(classname, bbox, score)]}\n            gt_all: map of {img_id: [(classname, bbox)]}\n            ovthresh: scalar, iou threshold\n            use_07_metric: bool, if true use VOC07 11 point method\n        Output:\n            rec: {classname: rec}",
        "detail": "3D-VLP.utils.eval_det",
        "documentation": {}
    },
    {
        "label": "eval_det_multiprocessing",
        "kind": 2,
        "importPath": "3D-VLP.utils.eval_det",
        "description": "3D-VLP.utils.eval_det",
        "peekOfCode": "def eval_det_multiprocessing(pred_all, gt_all, ovthresh=0.25, use_07_metric=False, get_iou_func=get_iou):\n    \"\"\" Generic functions to compute precision/recall for object detection\n        for multiple classes.\n        Input:\n            pred_all: map of {img_id: [(classname, bbox, score)]}\n            gt_all: map of {img_id: [(classname, bbox)]}\n            ovthresh: scalar, iou threshold\n            use_07_metric: bool, if true use VOC07 11 point method\n        Output:\n            rec: {classname: rec}",
        "detail": "3D-VLP.utils.eval_det",
        "documentation": {}
    },
    {
        "label": "BASE_DIR",
        "kind": 5,
        "importPath": "3D-VLP.utils.eval_det",
        "description": "3D-VLP.utils.eval_det",
        "peekOfCode": "BASE_DIR = os.path.dirname(os.path.abspath(__file__))\nfrom utils.metric_util import calc_iou # axis-aligned 3D box IoU\ndef get_iou(bb1, bb2):\n    \"\"\" Compute IoU of two bounding boxes.\n        ** Define your bod IoU function HERE **\n    \"\"\"\n    #pass\n    iou3d = calc_iou(bb1, bb2)\n    return iou3d\nfrom utils.box_util import box3d_iou",
        "detail": "3D-VLP.utils.eval_det",
        "documentation": {}
    },
    {
        "label": "multi_scene_precision_recall",
        "kind": 2,
        "importPath": "3D-VLP.utils.metric_util",
        "description": "3D-VLP.utils.metric_util",
        "peekOfCode": "def multi_scene_precision_recall(labels, pred, iou_thresh, conf_thresh, label_mask, pred_mask=None):\n    '''\n    Args:\n        labels: (B, N, 6)\n        pred: (B, M, 6)\n        iou_thresh: scalar\n        conf_thresh: scalar\n        label_mask: (B, N,) with values in 0 or 1 to indicate which GT boxes to consider.\n        pred_mask: (B, M,) with values in 0 or 1 to indicate which PRED boxes to consider.\n    Returns:",
        "detail": "3D-VLP.utils.metric_util",
        "documentation": {}
    },
    {
        "label": "single_scene_precision_recall",
        "kind": 2,
        "importPath": "3D-VLP.utils.metric_util",
        "description": "3D-VLP.utils.metric_util",
        "peekOfCode": "def single_scene_precision_recall(labels, pred, iou_thresh, conf_thresh):\n    \"\"\"Compute P and R for predicted bounding boxes. Ignores classes!\n    Args:\n        labels: (N x bbox) ground-truth bounding boxes (6 dims) \n        pred: (M x (bbox + conf)) predicted bboxes with confidence and maybe classification\n    Returns:\n        TP, FP, FN\n    \"\"\"\n    # for each pred box with high conf (C), compute IoU with all gt boxes. \n    # TP = number of times IoU > th ; FP = C - TP ",
        "detail": "3D-VLP.utils.metric_util",
        "documentation": {}
    },
    {
        "label": "precision_recall",
        "kind": 2,
        "importPath": "3D-VLP.utils.metric_util",
        "description": "3D-VLP.utils.metric_util",
        "peekOfCode": "def precision_recall(TP, FP, FN):\n    Prec = 1.0 * TP / (TP + FP) if TP+FP>0 else 0\n    Rec = 1.0 * TP / (TP + FN)\n    return Prec, Rec\ndef calc_iou(box_a, box_b):\n    \"\"\"Computes IoU of two axis aligned bboxes.\n    Args:\n        box_a, box_b: 6D of center and lengths        \n    Returns:\n        iou",
        "detail": "3D-VLP.utils.metric_util",
        "documentation": {}
    },
    {
        "label": "calc_iou",
        "kind": 2,
        "importPath": "3D-VLP.utils.metric_util",
        "description": "3D-VLP.utils.metric_util",
        "peekOfCode": "def calc_iou(box_a, box_b):\n    \"\"\"Computes IoU of two axis aligned bboxes.\n    Args:\n        box_a, box_b: 6D of center and lengths        \n    Returns:\n        iou\n    \"\"\"        \n    max_a = box_a[0:3] + box_a[3:6]/2\n    max_b = box_b[0:3] + box_b[3:6]/2    \n    min_max = np.array([max_a, max_b]).min(0)",
        "detail": "3D-VLP.utils.metric_util",
        "documentation": {}
    },
    {
        "label": "BASE_DIR",
        "kind": 5,
        "importPath": "3D-VLP.utils.metric_util",
        "description": "3D-VLP.utils.metric_util",
        "peekOfCode": "BASE_DIR = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(BASE_DIR)\nimport numpy as np\n# Mesh IO\nimport trimesh\n# ----------------------------------------\n# Precision and Recall\n# ----------------------------------------\ndef multi_scene_precision_recall(labels, pred, iou_thresh, conf_thresh, label_mask, pred_mask=None):\n    '''",
        "detail": "3D-VLP.utils.metric_util",
        "documentation": {}
    },
    {
        "label": "set_seed",
        "kind": 2,
        "importPath": "3D-VLP.utils.misc",
        "description": "3D-VLP.utils.misc",
        "peekOfCode": "def set_seed(seed, n_gpu):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if n_gpu > 0:\n        torch.cuda.manual_seed_all(seed)\ndef get_rank():\n    if not dist.is_available():\n        return 0\n    if not dist.is_initialized():",
        "detail": "3D-VLP.utils.misc",
        "documentation": {}
    },
    {
        "label": "get_rank",
        "kind": 2,
        "importPath": "3D-VLP.utils.misc",
        "description": "3D-VLP.utils.misc",
        "peekOfCode": "def get_rank():\n    if not dist.is_available():\n        return 0\n    if not dist.is_initialized():\n        return 0\n    return dist.get_rank()\ndef is_main_process():\n    return get_rank() == 0\ndef get_world_size():\n    if not dist.is_available():",
        "detail": "3D-VLP.utils.misc",
        "documentation": {}
    },
    {
        "label": "is_main_process",
        "kind": 2,
        "importPath": "3D-VLP.utils.misc",
        "description": "3D-VLP.utils.misc",
        "peekOfCode": "def is_main_process():\n    return get_rank() == 0\ndef get_world_size():\n    if not dist.is_available():\n        return 1\n    if not dist.is_initialized():\n        return 1\n    return dist.get_world_size()\ndef get_rank():\n    if not dist.is_available():",
        "detail": "3D-VLP.utils.misc",
        "documentation": {}
    },
    {
        "label": "get_world_size",
        "kind": 2,
        "importPath": "3D-VLP.utils.misc",
        "description": "3D-VLP.utils.misc",
        "peekOfCode": "def get_world_size():\n    if not dist.is_available():\n        return 1\n    if not dist.is_initialized():\n        return 1\n    return dist.get_world_size()\ndef get_rank():\n    if not dist.is_available():\n        return 0\n    if not dist.is_initialized():",
        "detail": "3D-VLP.utils.misc",
        "documentation": {}
    },
    {
        "label": "get_rank",
        "kind": 2,
        "importPath": "3D-VLP.utils.misc",
        "description": "3D-VLP.utils.misc",
        "peekOfCode": "def get_rank():\n    if not dist.is_available():\n        return 0\n    if not dist.is_initialized():\n        return 0\n    return dist.get_rank()\ndef overwrite_config(args, past_args):\n    for k, v in past_args.items():\n        if hasattr(args, k): # skip if args has past_args\n            continue",
        "detail": "3D-VLP.utils.misc",
        "documentation": {}
    },
    {
        "label": "overwrite_config",
        "kind": 2,
        "importPath": "3D-VLP.utils.misc",
        "description": "3D-VLP.utils.misc",
        "peekOfCode": "def overwrite_config(args, past_args):\n    for k, v in past_args.items():\n        if hasattr(args, k): # skip if args has past_args\n            continue\n        setattr(args, k, v)\n    return args",
        "detail": "3D-VLP.utils.misc",
        "documentation": {}
    },
    {
        "label": "nms_2d",
        "kind": 2,
        "importPath": "3D-VLP.utils.nms",
        "description": "3D-VLP.utils.nms",
        "peekOfCode": "def nms_2d(boxes, overlap_threshold):\n    x1 = boxes[:,0]\n    y1 = boxes[:,1]\n    x2 = boxes[:,2]\n    y2 = boxes[:,3]\n    score = boxes[:,4]\n    area = (x2-x1)*(y2-y1)\n    I = np.argsort(score)\n    pick = []\n    while (I.size!=0):",
        "detail": "3D-VLP.utils.nms",
        "documentation": {}
    },
    {
        "label": "nms_2d_faster",
        "kind": 2,
        "importPath": "3D-VLP.utils.nms",
        "description": "3D-VLP.utils.nms",
        "peekOfCode": "def nms_2d_faster(boxes, overlap_threshold, old_type=False):\n    x1 = boxes[:,0]\n    y1 = boxes[:,1]\n    x2 = boxes[:,2]\n    y2 = boxes[:,3]\n    score = boxes[:,4]\n    area = (x2-x1)*(y2-y1)\n    I = np.argsort(score)\n    pick = []\n    while (I.size!=0):",
        "detail": "3D-VLP.utils.nms",
        "documentation": {}
    },
    {
        "label": "nms_3d_faster",
        "kind": 2,
        "importPath": "3D-VLP.utils.nms",
        "description": "3D-VLP.utils.nms",
        "peekOfCode": "def nms_3d_faster(boxes, overlap_threshold, old_type=False):\n    x1 = boxes[:,0]\n    y1 = boxes[:,1]\n    z1 = boxes[:,2]\n    x2 = boxes[:,3]\n    y2 = boxes[:,4]\n    z2 = boxes[:,5]\n    score = boxes[:,6]\n    area = (x2-x1)*(y2-y1)*(z2-z1)\n    I = np.argsort(score)",
        "detail": "3D-VLP.utils.nms",
        "documentation": {}
    },
    {
        "label": "nms_3d_faster_samecls",
        "kind": 2,
        "importPath": "3D-VLP.utils.nms",
        "description": "3D-VLP.utils.nms",
        "peekOfCode": "def nms_3d_faster_samecls(boxes, overlap_threshold, old_type=False):\n    x1 = boxes[:,0]\n    y1 = boxes[:,1]\n    z1 = boxes[:,2]\n    x2 = boxes[:,3]\n    y2 = boxes[:,4]\n    z2 = boxes[:,5]\n    score = boxes[:,6]\n    cls = boxes[:,7]\n    area = (x2-x1)*(y2-y1)*(z2-z1)",
        "detail": "3D-VLP.utils.nms",
        "documentation": {}
    },
    {
        "label": "nms_3d_faster_samecls_torch",
        "kind": 2,
        "importPath": "3D-VLP.utils.nms",
        "description": "3D-VLP.utils.nms",
        "peekOfCode": "def nms_3d_faster_samecls_torch(boxes, overlap_threshold, old_type=False):\n    x1 = boxes[:,0]\n    y1 = boxes[:,1]\n    z1 = boxes[:,2]\n    x2 = boxes[:,3]\n    y2 = boxes[:,4]\n    z2 = boxes[:,5]\n    score = boxes[:,6]\n    cls = boxes[:,7]\n    area = (x2-x1)*(y2-y1)*(z2-z1)",
        "detail": "3D-VLP.utils.nms",
        "documentation": {}
    },
    {
        "label": "nms_crnr_dist",
        "kind": 2,
        "importPath": "3D-VLP.utils.nms",
        "description": "3D-VLP.utils.nms",
        "peekOfCode": "def nms_crnr_dist(boxes, conf, overlap_threshold):\n    I = np.argsort(conf)\n    pick = []\n    while (I.size!=0):\n        last = I.size\n        i = I[-1]\n        pick.append(i)        \n        scores = []\n        for ind in I[:-1]:\n            scores.append(bbox_corner_dist_measure(boxes[i,:], boxes[ind, :]))",
        "detail": "3D-VLP.utils.nms",
        "documentation": {}
    },
    {
        "label": "huber_loss",
        "kind": 2,
        "importPath": "3D-VLP.utils.nn_distance",
        "description": "3D-VLP.utils.nn_distance",
        "peekOfCode": "def huber_loss(error, delta=1.0):\n    \"\"\"\n    Args:\n        error: Torch tensor (d1,d2,...,dk)\n    Returns:\n        loss: Torch tensor (d1,d2,...,dk)\n    x = error = pred - gt or dist(pred,gt)\n    0.5 * |x|^2                 if |x|<=d\n    0.5 * d^2 + d * (|x|-d)     if |x|>d\n    Ref: https://github.com/charlesq34/frustum-pointnets/blob/master/models/model_util.py",
        "detail": "3D-VLP.utils.nn_distance",
        "documentation": {}
    },
    {
        "label": "nn_distance",
        "kind": 2,
        "importPath": "3D-VLP.utils.nn_distance",
        "description": "3D-VLP.utils.nn_distance",
        "peekOfCode": "def nn_distance(pc1, pc2, l1smooth=False, delta=1.0, l1=False):\n    \"\"\"\n    Input:\n        pc1: (B,N,C) torch tensor\n        pc2: (B,M,C) torch tensor\n        l1smooth: bool, whether to use l1smooth loss\n        delta: scalar, the delta used in l1smooth loss\n    Output:\n        dist1: (B,N) torch float32 tensor\n        idx1: (B,N) torch int64 tensor",
        "detail": "3D-VLP.utils.nn_distance",
        "documentation": {}
    },
    {
        "label": "knn_distance",
        "kind": 2,
        "importPath": "3D-VLP.utils.nn_distance",
        "description": "3D-VLP.utils.nn_distance",
        "peekOfCode": "def knn_distance(pc1, pc2, l1smooth=False, delta=1.0, l1=False, k=1):\n    \"\"\"\n    Input:\n        pc1: (B,N,C) torch tensor\n        pc2: (B,M,C) torch tensor\n        l1smooth: bool, whether to use l1smooth loss\n        delta: scalar, the delta used in l1smooth loss\n    Output:\n        dist1: (B,N) torch float32 tensor\n        idx1: (B,N) torch int64 tensor",
        "detail": "3D-VLP.utils.nn_distance",
        "documentation": {}
    },
    {
        "label": "demo_nn_distance",
        "kind": 2,
        "importPath": "3D-VLP.utils.nn_distance",
        "description": "3D-VLP.utils.nn_distance",
        "peekOfCode": "def demo_nn_distance():\n    np.random.seed(0)\n    pc1arr = np.random.random((1,5,3))\n    pc2arr = np.random.random((1,6,3))\n    pc1 = torch.from_numpy(pc1arr.astype(np.float32))\n    pc2 = torch.from_numpy(pc2arr.astype(np.float32))\n    dist1, idx1, dist2, idx2 = nn_distance(pc1, pc2)\n    print(dist1)\n    print(idx1)\n    dist = np.zeros((5,6))",
        "detail": "3D-VLP.utils.nn_distance",
        "documentation": {}
    },
    {
        "label": "random_sampling",
        "kind": 2,
        "importPath": "3D-VLP.utils.pc_utils",
        "description": "3D-VLP.utils.pc_utils",
        "peekOfCode": "def random_sampling(pc, num_sample, replace=None, return_choices=False):\n    \"\"\" Input is NxC, output is num_samplexC\n    \"\"\"\n    if replace is None: replace = (pc.shape[0]<num_sample)\n    choices = np.random.choice(pc.shape[0], num_sample, replace=replace)\n    if return_choices:\n        return pc[choices], choices\n    else:\n        return pc[choices]\n# ----------------------------------------",
        "detail": "3D-VLP.utils.pc_utils",
        "documentation": {}
    },
    {
        "label": "point_cloud_to_volume_batch",
        "kind": 2,
        "importPath": "3D-VLP.utils.pc_utils",
        "description": "3D-VLP.utils.pc_utils",
        "peekOfCode": "def point_cloud_to_volume_batch(point_clouds, vsize=12, radius=1.0, flatten=True):\n    \"\"\" Input is BxNx3 batch of point cloud\n        Output is Bx(vsize^3)\n    \"\"\"\n    vol_list = []\n    for b in range(point_clouds.shape[0]):\n        vol = point_cloud_to_volume(np.squeeze(point_clouds[b,:,:]), vsize, radius)\n        if flatten:\n            vol_list.append(vol.flatten())\n        else:",
        "detail": "3D-VLP.utils.pc_utils",
        "documentation": {}
    },
    {
        "label": "point_cloud_to_volume",
        "kind": 2,
        "importPath": "3D-VLP.utils.pc_utils",
        "description": "3D-VLP.utils.pc_utils",
        "peekOfCode": "def point_cloud_to_volume(points, vsize, radius=1.0):\n    \"\"\" input is Nx3 points.\n        output is vsize*vsize*vsize\n        assumes points are in range [-radius, radius]\n    \"\"\"\n    vol = np.zeros((vsize,vsize,vsize))\n    voxel = 2*radius/float(vsize)\n    locations = (points + radius)/voxel\n    locations = locations.astype(int)\n    vol[locations[:,0],locations[:,1],locations[:,2]] = 1.0",
        "detail": "3D-VLP.utils.pc_utils",
        "documentation": {}
    },
    {
        "label": "volume_to_point_cloud",
        "kind": 2,
        "importPath": "3D-VLP.utils.pc_utils",
        "description": "3D-VLP.utils.pc_utils",
        "peekOfCode": "def volume_to_point_cloud(vol):\n    \"\"\" vol is occupancy grid (value = 0 or 1) of size vsize*vsize*vsize\n        return Nx3 numpy array.\n    \"\"\"\n    vsize = vol.shape[0]\n    assert(vol.shape[1] == vsize and vol.shape[1] == vsize)\n    points = []\n    for a in range(vsize):\n        for b in range(vsize):\n            for c in range(vsize):",
        "detail": "3D-VLP.utils.pc_utils",
        "documentation": {}
    },
    {
        "label": "point_cloud_to_volume_v2_batch",
        "kind": 2,
        "importPath": "3D-VLP.utils.pc_utils",
        "description": "3D-VLP.utils.pc_utils",
        "peekOfCode": "def point_cloud_to_volume_v2_batch(point_clouds, vsize=12, radius=1.0, num_sample=128):\n    \"\"\" Input is BxNx3 a batch of point cloud\n        Output is BxVxVxVxnum_samplex3\n        Added on Feb 19\n    \"\"\"\n    vol_list = []\n    for b in range(point_clouds.shape[0]):\n        vol = point_cloud_to_volume_v2(point_clouds[b,:,:], vsize, radius, num_sample)\n        vol_list.append(np.expand_dims(vol, 0))\n    return np.concatenate(vol_list, 0)",
        "detail": "3D-VLP.utils.pc_utils",
        "documentation": {}
    },
    {
        "label": "point_cloud_to_volume_v2",
        "kind": 2,
        "importPath": "3D-VLP.utils.pc_utils",
        "description": "3D-VLP.utils.pc_utils",
        "peekOfCode": "def point_cloud_to_volume_v2(points, vsize, radius=1.0, num_sample=128):\n    \"\"\" input is Nx3 points\n        output is vsize*vsize*vsize*num_sample*3\n        assumes points are in range [-radius, radius]\n        samples num_sample points in each voxel, if there are less than\n        num_sample points, replicate the points\n        Added on Feb 19\n    \"\"\"\n    vol = np.zeros((vsize,vsize,vsize,num_sample,3))\n    voxel = 2*radius/float(vsize)",
        "detail": "3D-VLP.utils.pc_utils",
        "documentation": {}
    },
    {
        "label": "point_cloud_to_image_batch",
        "kind": 2,
        "importPath": "3D-VLP.utils.pc_utils",
        "description": "3D-VLP.utils.pc_utils",
        "peekOfCode": "def point_cloud_to_image_batch(point_clouds, imgsize, radius=1.0, num_sample=128):\n    \"\"\" Input is BxNx3 a batch of point cloud\n        Output is BxIxIxnum_samplex3\n        Added on Feb 19\n    \"\"\"\n    img_list = []\n    for b in range(point_clouds.shape[0]):\n        img = point_cloud_to_image(point_clouds[b,:,:], imgsize, radius, num_sample)\n        img_list.append(np.expand_dims(img, 0))\n    return np.concatenate(img_list, 0)",
        "detail": "3D-VLP.utils.pc_utils",
        "documentation": {}
    },
    {
        "label": "point_cloud_to_image",
        "kind": 2,
        "importPath": "3D-VLP.utils.pc_utils",
        "description": "3D-VLP.utils.pc_utils",
        "peekOfCode": "def point_cloud_to_image(points, imgsize, radius=1.0, num_sample=128):\n    \"\"\" input is Nx3 points\n        output is imgsize*imgsize*num_sample*3\n        assumes points are in range [-radius, radius]\n        samples num_sample points in each pixel, if there are less than\n        num_sample points, replicate the points\n        Added on Feb 19\n    \"\"\"\n    img = np.zeros((imgsize, imgsize, num_sample, 3))\n    pixel = 2*radius/float(imgsize)",
        "detail": "3D-VLP.utils.pc_utils",
        "documentation": {}
    },
    {
        "label": "read_ply",
        "kind": 2,
        "importPath": "3D-VLP.utils.pc_utils",
        "description": "3D-VLP.utils.pc_utils",
        "peekOfCode": "def read_ply(filename):\n    \"\"\" read XYZ point cloud from filename PLY file \"\"\"\n    plydata = PlyData.read(filename)\n    pc = plydata['vertex'].data\n    pc_array = np.array([[x, y, z] for x,y,z in pc])\n    return pc_array\ndef write_ply(points, filename, text=True):\n    \"\"\" input: Nx3, write points to filename as PLY format. \"\"\"\n    points = [(points[i,0], points[i,1], points[i,2]) for i in range(points.shape[0])]\n    vertex = np.array(points, dtype=[('x', 'f4'), ('y', 'f4'),('z', 'f4')])",
        "detail": "3D-VLP.utils.pc_utils",
        "documentation": {}
    },
    {
        "label": "write_ply",
        "kind": 2,
        "importPath": "3D-VLP.utils.pc_utils",
        "description": "3D-VLP.utils.pc_utils",
        "peekOfCode": "def write_ply(points, filename, text=True):\n    \"\"\" input: Nx3, write points to filename as PLY format. \"\"\"\n    points = [(points[i,0], points[i,1], points[i,2]) for i in range(points.shape[0])]\n    vertex = np.array(points, dtype=[('x', 'f4'), ('y', 'f4'),('z', 'f4')])\n    el = PlyElement.describe(vertex, 'vertex', comments=['vertices'])\n    PlyData([el], text=text).write(filename)\ndef write_ply_color(points, labels, filename, num_classes=None, colormap=pyplot.cm.jet):\n    \"\"\" Color (N,3) points with labels (N) within range 0 ~ num_classes-1 as OBJ file \"\"\"\n    labels = labels.astype(int)\n    N = points.shape[0]",
        "detail": "3D-VLP.utils.pc_utils",
        "documentation": {}
    },
    {
        "label": "write_ply_color",
        "kind": 2,
        "importPath": "3D-VLP.utils.pc_utils",
        "description": "3D-VLP.utils.pc_utils",
        "peekOfCode": "def write_ply_color(points, labels, filename, num_classes=None, colormap=pyplot.cm.jet):\n    \"\"\" Color (N,3) points with labels (N) within range 0 ~ num_classes-1 as OBJ file \"\"\"\n    labels = labels.astype(int)\n    N = points.shape[0]\n    if num_classes is None:\n        num_classes = np.max(labels)+1\n    else:\n        assert(num_classes>np.max(labels))\n    vertex = []\n    #colors = [pyplot.cm.jet(i/float(num_classes)) for i in range(num_classes)]    ",
        "detail": "3D-VLP.utils.pc_utils",
        "documentation": {}
    },
    {
        "label": "write_ply_rgb",
        "kind": 2,
        "importPath": "3D-VLP.utils.pc_utils",
        "description": "3D-VLP.utils.pc_utils",
        "peekOfCode": "def write_ply_rgb(points, colors, filename, text=True, num_classes=None):\n    \"\"\" Color (N,3) points with RGB colors (N,3) within range [0,255] as OBJ file \"\"\"\n    colors = colors.astype(int)\n    points = [(points[i,0], points[i,1], points[i,2], colors[i,0], colors[i,1], colors[i,2]) for i in range(points.shape[0])]\n    vertex = np.array(points, dtype=[('x', 'f4'), ('y', 'f4'),('z', 'f4'),('red', 'u1'), ('green', 'u1'),('blue', 'u1')])\n    el = PlyElement.describe(vertex, 'vertex', comments=['vertices'])\n    PlyData([el], text=text).write(filename)\n# ----------------------------------------\n# Simple Point cloud and Volume Renderers\n# ----------------------------------------",
        "detail": "3D-VLP.utils.pc_utils",
        "documentation": {}
    },
    {
        "label": "pyplot_draw_point_cloud",
        "kind": 2,
        "importPath": "3D-VLP.utils.pc_utils",
        "description": "3D-VLP.utils.pc_utils",
        "peekOfCode": "def pyplot_draw_point_cloud(points, output_filename):\n    \"\"\" points is a Nx3 numpy array \"\"\"\n    import matplotlib.pyplot as plt\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n    ax.scatter(points[:,0], points[:,1], points[:,2])\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_zlabel('z')\n    #savefig(output_filename)",
        "detail": "3D-VLP.utils.pc_utils",
        "documentation": {}
    },
    {
        "label": "pyplot_draw_volume",
        "kind": 2,
        "importPath": "3D-VLP.utils.pc_utils",
        "description": "3D-VLP.utils.pc_utils",
        "peekOfCode": "def pyplot_draw_volume(vol, output_filename):\n    \"\"\" vol is of size vsize*vsize*vsize\n        output an image to output_filename\n    \"\"\"\n    points = volume_to_point_cloud(vol)\n    pyplot_draw_point_cloud(points, output_filename)\n# ----------------------------------------\n# Simple Point manipulations\n# ----------------------------------------\ndef rotate_point_cloud(points, rotation_matrix=None):",
        "detail": "3D-VLP.utils.pc_utils",
        "documentation": {}
    },
    {
        "label": "rotate_point_cloud",
        "kind": 2,
        "importPath": "3D-VLP.utils.pc_utils",
        "description": "3D-VLP.utils.pc_utils",
        "peekOfCode": "def rotate_point_cloud(points, rotation_matrix=None):\n    \"\"\" Input: (n,3), Output: (n,3) \"\"\"\n    # Rotate in-place around Z axis.\n    if rotation_matrix is None:\n        rotation_angle = np.random.uniform() * 2 * np.pi\n        sinval, cosval = np.sin(rotation_angle), np.cos(rotation_angle)     \n        rotation_matrix = np.array([[cosval, sinval, 0],\n                                    [-sinval, cosval, 0],\n                                    [0, 0, 1]])\n    ctr = points.mean(axis=0)",
        "detail": "3D-VLP.utils.pc_utils",
        "documentation": {}
    },
    {
        "label": "rotate_pc_along_y",
        "kind": 2,
        "importPath": "3D-VLP.utils.pc_utils",
        "description": "3D-VLP.utils.pc_utils",
        "peekOfCode": "def rotate_pc_along_y(pc, rot_angle):\n    ''' Input ps is NxC points with first 3 channels as XYZ\n        z is facing forward, x is left ward, y is downward\n    '''\n    cosval = np.cos(rot_angle)\n    sinval = np.sin(rot_angle)\n    rotmat = np.array([[cosval, -sinval],[sinval, cosval]])\n    pc[:,[0,2]] = np.dot(pc[:,[0,2]], np.transpose(rotmat))\n    return pc\ndef rotx(t):",
        "detail": "3D-VLP.utils.pc_utils",
        "documentation": {}
    },
    {
        "label": "rotx",
        "kind": 2,
        "importPath": "3D-VLP.utils.pc_utils",
        "description": "3D-VLP.utils.pc_utils",
        "peekOfCode": "def rotx(t):\n    \"\"\"Rotation about the y-axis.\"\"\"\n    c = np.cos(t)\n    s = np.sin(t)\n    return np.array([[1,  0,  0],\n                    [0,  c,  -s],\n                    [0,  s,  c]])\ndef roty(t):\n    \"\"\"Rotation about the y-axis.\"\"\"\n    c = np.cos(t)",
        "detail": "3D-VLP.utils.pc_utils",
        "documentation": {}
    },
    {
        "label": "roty",
        "kind": 2,
        "importPath": "3D-VLP.utils.pc_utils",
        "description": "3D-VLP.utils.pc_utils",
        "peekOfCode": "def roty(t):\n    \"\"\"Rotation about the y-axis.\"\"\"\n    c = np.cos(t)\n    s = np.sin(t)\n    return np.array([[c,  0,  s],\n                    [0,  1,  0],\n                    [-s, 0,  c]])\ndef roty_batch(t):\n    \"\"\"Rotation about the y-axis.\n    t: (x1,x2,...xn)",
        "detail": "3D-VLP.utils.pc_utils",
        "documentation": {}
    },
    {
        "label": "roty_batch",
        "kind": 2,
        "importPath": "3D-VLP.utils.pc_utils",
        "description": "3D-VLP.utils.pc_utils",
        "peekOfCode": "def roty_batch(t):\n    \"\"\"Rotation about the y-axis.\n    t: (x1,x2,...xn)\n    return: (x1,x2,...,xn,3,3)\n    \"\"\"\n    input_shape = t.shape\n    output = np.zeros(tuple(list(input_shape)+[3,3]))\n    c = np.cos(t)\n    s = np.sin(t)\n    output[...,0,0] = c",
        "detail": "3D-VLP.utils.pc_utils",
        "documentation": {}
    },
    {
        "label": "rotz",
        "kind": 2,
        "importPath": "3D-VLP.utils.pc_utils",
        "description": "3D-VLP.utils.pc_utils",
        "peekOfCode": "def rotz(t):\n    \"\"\"Rotation about the z-axis.\"\"\"\n    c = np.cos(t)\n    s = np.sin(t)\n    return np.array([[c, -s,  0],\n                     [s,  c,  0],\n                     [0,  0,  1]])\n# ----------------------------------------\n# BBox\n# ----------------------------------------",
        "detail": "3D-VLP.utils.pc_utils",
        "documentation": {}
    },
    {
        "label": "bbox_corner_dist_measure",
        "kind": 2,
        "importPath": "3D-VLP.utils.pc_utils",
        "description": "3D-VLP.utils.pc_utils",
        "peekOfCode": "def bbox_corner_dist_measure(crnr1, crnr2):\n    \"\"\" compute distance between box corners to replace iou\n    Args:\n        crnr1, crnr2: Nx3 points of box corners in camera axis (y points down)\n        output is a scalar between 0 and 1        \n    \"\"\"\n    dist = sys.maxsize\n    for y in range(4):\n        rows = ([(x+y)%4 for x in range(4)] + [4+(x+y)%4 for x in range(4)])\n        d_ = np.linalg.norm(crnr2[rows, :] - crnr1, axis=1).sum() / 8.0            ",
        "detail": "3D-VLP.utils.pc_utils",
        "documentation": {}
    },
    {
        "label": "point_cloud_to_bbox",
        "kind": 2,
        "importPath": "3D-VLP.utils.pc_utils",
        "description": "3D-VLP.utils.pc_utils",
        "peekOfCode": "def point_cloud_to_bbox(points):\n    \"\"\" Extract the axis aligned box from a pcl or batch of pcls\n    Args:\n        points: Nx3 points or BxNx3\n        output is 6 dim: xyz pos of center and 3 lengths        \n    \"\"\"\n    which_dim = len(points.shape) - 2 # first dim if a single cloud and second if batch\n    mn, mx = points.min(which_dim), points.max(which_dim)\n    lengths = mx - mn\n    cntr = 0.5*(mn + mx)",
        "detail": "3D-VLP.utils.pc_utils",
        "documentation": {}
    },
    {
        "label": "write_bbox",
        "kind": 2,
        "importPath": "3D-VLP.utils.pc_utils",
        "description": "3D-VLP.utils.pc_utils",
        "peekOfCode": "def write_bbox(scene_bbox, out_filename):\n    \"\"\"Export scene bbox to meshes\n    Args:\n        scene_bbox: (N x 6 numpy array): xyz pos of center and 3 lengths\n        out_filename: (string) filename\n    Note:\n        To visualize the boxes in MeshLab.\n        1. Select the objects (the boxes)\n        2. Filters -> Polygon and Quad Mesh -> Turn into Quad-Dominant Mesh\n        3. Select Wireframe view.",
        "detail": "3D-VLP.utils.pc_utils",
        "documentation": {}
    },
    {
        "label": "write_oriented_bbox",
        "kind": 2,
        "importPath": "3D-VLP.utils.pc_utils",
        "description": "3D-VLP.utils.pc_utils",
        "peekOfCode": "def write_oriented_bbox(scene_bbox, out_filename):\n    \"\"\"Export oriented (around Z axis) scene bbox to meshes\n    Args:\n        scene_bbox: (N x 7 numpy array): xyz pos of center and 3 lengths (dx,dy,dz)\n            and heading angle around Z axis.\n            Y forward, X right, Z upward. heading angle of positive X is 0,\n            heading angle of positive Y is 90 degrees.\n        out_filename: (string) filename\n    \"\"\"\n    def heading2rotmat(heading_angle):",
        "detail": "3D-VLP.utils.pc_utils",
        "documentation": {}
    },
    {
        "label": "write_oriented_bbox_camera_coord",
        "kind": 2,
        "importPath": "3D-VLP.utils.pc_utils",
        "description": "3D-VLP.utils.pc_utils",
        "peekOfCode": "def write_oriented_bbox_camera_coord(scene_bbox, out_filename):\n    \"\"\"Export oriented (around Y axis) scene bbox to meshes\n    Args:\n        scene_bbox: (N x 7 numpy array): xyz pos of center and 3 lengths (dx,dy,dz)\n            and heading angle around Y axis.\n            Z forward, X rightward, Y downward. heading angle of positive X is 0,\n            heading angle of negative Z is 90 degrees.\n        out_filename: (string) filename\n    \"\"\"\n    def heading2rotmat(heading_angle):",
        "detail": "3D-VLP.utils.pc_utils",
        "documentation": {}
    },
    {
        "label": "write_lines_as_cylinders",
        "kind": 2,
        "importPath": "3D-VLP.utils.pc_utils",
        "description": "3D-VLP.utils.pc_utils",
        "peekOfCode": "def write_lines_as_cylinders(pcl, filename, rad=0.005, res=64):\n    \"\"\"Create lines represented as cylinders connecting pairs of 3D points\n    Args:\n        pcl: (N x 2 x 3 numpy array): N pairs of xyz pos             \n        filename: (string) filename for the output mesh (ply) file\n        rad: radius for the cylinder\n        res: number of sections used to create the cylinder\n    \"\"\"\n    scene = trimesh.scene.Scene()\n    for src,tgt in pcl:",
        "detail": "3D-VLP.utils.pc_utils",
        "documentation": {}
    },
    {
        "label": "BASE_DIR",
        "kind": 5,
        "importPath": "3D-VLP.utils.pc_utils",
        "description": "3D-VLP.utils.pc_utils",
        "peekOfCode": "BASE_DIR = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(BASE_DIR)\n# Point cloud IO\nimport numpy as np\ntry:\n    from plyfile import PlyData, PlyElement\nexcept:\n    print(\"Please install the module 'plyfile' for PLY i/o, e.g.\")\n    print(\"pip install plyfile\")\n    sys.exit(-1)",
        "detail": "3D-VLP.utils.pc_utils",
        "documentation": {}
    },
    {
        "label": "export_one_scan",
        "kind": 2,
        "importPath": "data.scannet.batch_load_scannet_data",
        "description": "data.scannet.batch_load_scannet_data",
        "peekOfCode": "def export_one_scan(scan_name, output_filename_prefix):    \n    mesh_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '_vh_clean_2.ply')\n    agg_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '.aggregation.json')\n    seg_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '_vh_clean_2.0.010000.segs.json')\n    meta_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '.txt') # includes axisAlignment info for the train set scans.   \n    mesh_vertices, aligned_vertices, semantic_labels, instance_labels, instance_bboxes, aligned_instance_bboxes = export(mesh_file, agg_file, seg_file, meta_file, LABEL_MAP_FILE, None)\n    mask = np.logical_not(np.in1d(semantic_labels, DONOTCARE_CLASS_IDS))\n    mesh_vertices = mesh_vertices[mask,:]\n    aligned_vertices = aligned_vertices[mask,:]\n    semantic_labels = semantic_labels[mask]",
        "detail": "data.scannet.batch_load_scannet_data",
        "documentation": {}
    },
    {
        "label": "batch_export",
        "kind": 2,
        "importPath": "data.scannet.batch_load_scannet_data",
        "description": "data.scannet.batch_load_scannet_data",
        "peekOfCode": "def batch_export():\n    if not os.path.exists(OUTPUT_FOLDER):\n        print('Creating new data folder: {}'.format(OUTPUT_FOLDER))                \n        os.mkdir(OUTPUT_FOLDER)        \n    for scan_name in SCAN_NAMES:\n        output_filename_prefix = os.path.join(OUTPUT_FOLDER, scan_name)\n        # if os.path.exists(output_filename_prefix + '_vert.npy'): continue\n        print('-'*20+'begin')\n        print(datetime.datetime.now())\n        print(scan_name)",
        "detail": "data.scannet.batch_load_scannet_data",
        "documentation": {}
    },
    {
        "label": "SCANNET_DIR",
        "kind": 5,
        "importPath": "data.scannet.batch_load_scannet_data",
        "description": "data.scannet.batch_load_scannet_data",
        "peekOfCode": "SCANNET_DIR = 'data/scannet/scans_test'\nSCAN_NAMES = sorted([line.rstrip() for line in open('meta_data/scannetv2.txt')])\nLABEL_MAP_FILE = 'meta_data/scannetv2-labels.combined.tsv'\nDONOTCARE_CLASS_IDS = np.array([])\nOBJ_CLASS_IDS = np.array([3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]) # exclude wall (1), floor (2), ceiling (22)\nMAX_NUM_POINT = 50000\nOUTPUT_FOLDER = './scannet_data'\ndef export_one_scan(scan_name, output_filename_prefix):    \n    mesh_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '_vh_clean_2.ply')\n    agg_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '.aggregation.json')",
        "detail": "data.scannet.batch_load_scannet_data",
        "documentation": {}
    },
    {
        "label": "SCAN_NAMES",
        "kind": 5,
        "importPath": "data.scannet.batch_load_scannet_data",
        "description": "data.scannet.batch_load_scannet_data",
        "peekOfCode": "SCAN_NAMES = sorted([line.rstrip() for line in open('meta_data/scannetv2.txt')])\nLABEL_MAP_FILE = 'meta_data/scannetv2-labels.combined.tsv'\nDONOTCARE_CLASS_IDS = np.array([])\nOBJ_CLASS_IDS = np.array([3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]) # exclude wall (1), floor (2), ceiling (22)\nMAX_NUM_POINT = 50000\nOUTPUT_FOLDER = './scannet_data'\ndef export_one_scan(scan_name, output_filename_prefix):    \n    mesh_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '_vh_clean_2.ply')\n    agg_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '.aggregation.json')\n    seg_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '_vh_clean_2.0.010000.segs.json')",
        "detail": "data.scannet.batch_load_scannet_data",
        "documentation": {}
    },
    {
        "label": "LABEL_MAP_FILE",
        "kind": 5,
        "importPath": "data.scannet.batch_load_scannet_data",
        "description": "data.scannet.batch_load_scannet_data",
        "peekOfCode": "LABEL_MAP_FILE = 'meta_data/scannetv2-labels.combined.tsv'\nDONOTCARE_CLASS_IDS = np.array([])\nOBJ_CLASS_IDS = np.array([3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]) # exclude wall (1), floor (2), ceiling (22)\nMAX_NUM_POINT = 50000\nOUTPUT_FOLDER = './scannet_data'\ndef export_one_scan(scan_name, output_filename_prefix):    \n    mesh_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '_vh_clean_2.ply')\n    agg_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '.aggregation.json')\n    seg_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '_vh_clean_2.0.010000.segs.json')\n    meta_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '.txt') # includes axisAlignment info for the train set scans.   ",
        "detail": "data.scannet.batch_load_scannet_data",
        "documentation": {}
    },
    {
        "label": "DONOTCARE_CLASS_IDS",
        "kind": 5,
        "importPath": "data.scannet.batch_load_scannet_data",
        "description": "data.scannet.batch_load_scannet_data",
        "peekOfCode": "DONOTCARE_CLASS_IDS = np.array([])\nOBJ_CLASS_IDS = np.array([3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]) # exclude wall (1), floor (2), ceiling (22)\nMAX_NUM_POINT = 50000\nOUTPUT_FOLDER = './scannet_data'\ndef export_one_scan(scan_name, output_filename_prefix):    \n    mesh_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '_vh_clean_2.ply')\n    agg_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '.aggregation.json')\n    seg_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '_vh_clean_2.0.010000.segs.json')\n    meta_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '.txt') # includes axisAlignment info for the train set scans.   \n    mesh_vertices, aligned_vertices, semantic_labels, instance_labels, instance_bboxes, aligned_instance_bboxes = export(mesh_file, agg_file, seg_file, meta_file, LABEL_MAP_FILE, None)",
        "detail": "data.scannet.batch_load_scannet_data",
        "documentation": {}
    },
    {
        "label": "OBJ_CLASS_IDS",
        "kind": 5,
        "importPath": "data.scannet.batch_load_scannet_data",
        "description": "data.scannet.batch_load_scannet_data",
        "peekOfCode": "OBJ_CLASS_IDS = np.array([3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]) # exclude wall (1), floor (2), ceiling (22)\nMAX_NUM_POINT = 50000\nOUTPUT_FOLDER = './scannet_data'\ndef export_one_scan(scan_name, output_filename_prefix):    \n    mesh_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '_vh_clean_2.ply')\n    agg_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '.aggregation.json')\n    seg_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '_vh_clean_2.0.010000.segs.json')\n    meta_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '.txt') # includes axisAlignment info for the train set scans.   \n    mesh_vertices, aligned_vertices, semantic_labels, instance_labels, instance_bboxes, aligned_instance_bboxes = export(mesh_file, agg_file, seg_file, meta_file, LABEL_MAP_FILE, None)\n    mask = np.logical_not(np.in1d(semantic_labels, DONOTCARE_CLASS_IDS))",
        "detail": "data.scannet.batch_load_scannet_data",
        "documentation": {}
    },
    {
        "label": "MAX_NUM_POINT",
        "kind": 5,
        "importPath": "data.scannet.batch_load_scannet_data",
        "description": "data.scannet.batch_load_scannet_data",
        "peekOfCode": "MAX_NUM_POINT = 50000\nOUTPUT_FOLDER = './scannet_data'\ndef export_one_scan(scan_name, output_filename_prefix):    \n    mesh_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '_vh_clean_2.ply')\n    agg_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '.aggregation.json')\n    seg_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '_vh_clean_2.0.010000.segs.json')\n    meta_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '.txt') # includes axisAlignment info for the train set scans.   \n    mesh_vertices, aligned_vertices, semantic_labels, instance_labels, instance_bboxes, aligned_instance_bboxes = export(mesh_file, agg_file, seg_file, meta_file, LABEL_MAP_FILE, None)\n    mask = np.logical_not(np.in1d(semantic_labels, DONOTCARE_CLASS_IDS))\n    mesh_vertices = mesh_vertices[mask,:]",
        "detail": "data.scannet.batch_load_scannet_data",
        "documentation": {}
    },
    {
        "label": "OUTPUT_FOLDER",
        "kind": 5,
        "importPath": "data.scannet.batch_load_scannet_data",
        "description": "data.scannet.batch_load_scannet_data",
        "peekOfCode": "OUTPUT_FOLDER = './scannet_data'\ndef export_one_scan(scan_name, output_filename_prefix):    \n    mesh_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '_vh_clean_2.ply')\n    agg_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '.aggregation.json')\n    seg_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '_vh_clean_2.0.010000.segs.json')\n    meta_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '.txt') # includes axisAlignment info for the train set scans.   \n    mesh_vertices, aligned_vertices, semantic_labels, instance_labels, instance_bboxes, aligned_instance_bboxes = export(mesh_file, agg_file, seg_file, meta_file, LABEL_MAP_FILE, None)\n    mask = np.logical_not(np.in1d(semantic_labels, DONOTCARE_CLASS_IDS))\n    mesh_vertices = mesh_vertices[mask,:]\n    aligned_vertices = aligned_vertices[mask,:]",
        "detail": "data.scannet.batch_load_scannet_data",
        "documentation": {}
    },
    {
        "label": "get_release_scans",
        "kind": 2,
        "importPath": "data.scannet.download-scannet",
        "description": "data.scannet.download-scannet",
        "peekOfCode": "def get_release_scans(release_file):\n    scan_lines = urllib.request.urlopen(release_file)\n    # scan_lines = urllib.urlopen(release_file)\n    scans = []\n    for scan_line in scan_lines:\n        scan_id = scan_line.decode('utf8').rstrip('\\n')\n        scans.append(scan_id)\n    return scans\ndef download_release(release_scans, out_dir, file_types, use_v1_sens):\n    if len(release_scans) == 0:",
        "detail": "data.scannet.download-scannet",
        "documentation": {}
    },
    {
        "label": "download_release",
        "kind": 2,
        "importPath": "data.scannet.download-scannet",
        "description": "data.scannet.download-scannet",
        "peekOfCode": "def download_release(release_scans, out_dir, file_types, use_v1_sens):\n    if len(release_scans) == 0:\n        return\n    print('Downloading ScanNet ' + RELEASE_NAME + ' release to ' + out_dir + '...')\n    for scan_id in release_scans:\n        scan_out_dir = os.path.join(out_dir, scan_id)\n        download_scan(scan_id, scan_out_dir, file_types, use_v1_sens)\n    print('Downloaded ScanNet ' + RELEASE_NAME + ' release.')\ndef download_file(url, out_file):\n    out_dir = os.path.dirname(out_file)",
        "detail": "data.scannet.download-scannet",
        "documentation": {}
    },
    {
        "label": "download_file",
        "kind": 2,
        "importPath": "data.scannet.download-scannet",
        "description": "data.scannet.download-scannet",
        "peekOfCode": "def download_file(url, out_file):\n    out_dir = os.path.dirname(out_file)\n    if not os.path.isdir(out_dir):\n        os.makedirs(out_dir)\n    if not os.path.isfile(out_file):\n        print('\\t' + url + ' > ' + out_file)\n        fh, out_file_tmp = tempfile.mkstemp(dir=out_dir)\n        f = os.fdopen(fh, 'w')\n        f.close()\n        urllib.request.urlretrieve(url, out_file_tmp)",
        "detail": "data.scannet.download-scannet",
        "documentation": {}
    },
    {
        "label": "download_scan",
        "kind": 2,
        "importPath": "data.scannet.download-scannet",
        "description": "data.scannet.download-scannet",
        "peekOfCode": "def download_scan(scan_id, out_dir, file_types, use_v1_sens):\n    print('Downloading ScanNet ' + RELEASE_NAME + ' scan ' + scan_id + ' ...')\n    if not os.path.isdir(out_dir):\n        os.makedirs(out_dir)\n    for ft in file_types:\n        v1_sens = use_v1_sens and ft == '.sens'\n        url = BASE_URL + RELEASE + '/' + scan_id + '/' + scan_id + ft if not v1_sens else BASE_URL + RELEASES[V1_IDX] + '/' + scan_id + '/' + scan_id + ft\n        out_file = out_dir + '/' + scan_id + ft\n        download_file(url, out_file)\n    print('Downloaded scan ' + scan_id)",
        "detail": "data.scannet.download-scannet",
        "documentation": {}
    },
    {
        "label": "download_task_data",
        "kind": 2,
        "importPath": "data.scannet.download-scannet",
        "description": "data.scannet.download-scannet",
        "peekOfCode": "def download_task_data(out_dir):\n    print('Downloading ScanNet v1 task data...')\n    files = [\n        LABEL_MAP_FILES[V1_IDX], 'obj_classification/data.zip',\n        'obj_classification/trained_models.zip', 'voxel_labeling/data.zip',\n        'voxel_labeling/trained_models.zip'\n    ]\n    for file in files:\n        url = BASE_URL + RELEASES_TASKS[V1_IDX] + '/' + file\n        localpath = os.path.join(out_dir, file)",
        "detail": "data.scannet.download-scannet",
        "documentation": {}
    },
    {
        "label": "download_tfrecords",
        "kind": 2,
        "importPath": "data.scannet.download-scannet",
        "description": "data.scannet.download-scannet",
        "peekOfCode": "def download_tfrecords(in_dir, out_dir):\n    print('Downloading tf records (302 GB)...')\n    if not os.path.exists(out_dir):\n        os.makedirs(out_dir)\n    split_to_num_shards = {'train': 100, 'val': 25, 'test': 10}\n    for folder_name in ['hires_tfrecords', 'lores_tfrecords']:\n        folder_dir = '%s/%s' % (in_dir, folder_name)\n        save_dir = '%s/%s' % (out_dir, folder_name)\n        if not os.path.exists(save_dir):\n            os.makedirs(save_dir)",
        "detail": "data.scannet.download-scannet",
        "documentation": {}
    },
    {
        "label": "download_label_map",
        "kind": 2,
        "importPath": "data.scannet.download-scannet",
        "description": "data.scannet.download-scannet",
        "peekOfCode": "def download_label_map(out_dir):\n    print('Downloading ScanNet ' + RELEASE_NAME + ' label mapping file...')\n    files = [ LABEL_MAP_FILE ]\n    for file in files:\n        url = BASE_URL + RELEASE_TASKS + '/' + file\n        localpath = os.path.join(out_dir, file)\n        localdir = os.path.dirname(localpath)\n        if not os.path.isdir(localdir):\n          os.makedirs(localdir)\n        download_file(url, localpath)",
        "detail": "data.scannet.download-scannet",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "data.scannet.download-scannet",
        "description": "data.scannet.download-scannet",
        "peekOfCode": "def main():\n    parser = argparse.ArgumentParser(description='Downloads ScanNet public data release.')\n    parser.add_argument('-o', '--out_dir', required=True, help='directory in which to download')\n    parser.add_argument('--task_data', action='store_true', help='download task data (v1)')\n    parser.add_argument('--label_map', action='store_true', help='download label map file')\n    parser.add_argument('--v1', action='store_true', help='download ScanNet v1 instead of v2')\n    parser.add_argument('--id', help='specific scan id to download')\n    parser.add_argument('--preprocessed_frames', action='store_true', help='download preprocessed subset of ScanNet frames (' + PREPROCESSED_FRAMES_FILE[1] + ')')\n    parser.add_argument('--test_frames_2d', action='store_true', help='download 2D test frames (' + TEST_FRAMES_FILE[1] + '; also included with whole dataset download)')\n    parser.add_argument('--data_efficient', action='store_true', help='download data efficient task files; also included with whole dataset download)')",
        "detail": "data.scannet.download-scannet",
        "documentation": {}
    },
    {
        "label": "BASE_URL",
        "kind": 5,
        "importPath": "data.scannet.download-scannet",
        "description": "data.scannet.download-scannet",
        "peekOfCode": "BASE_URL = 'http://kaldir.vc.in.tum.de/scannet/'\nTOS_URL = BASE_URL + 'ScanNet_TOS.pdf'\nFILETYPES = ['.aggregation.json', '.sens', '.txt', '_vh_clean.ply', '_vh_clean_2.0.010000.segs.json', '_vh_clean_2.ply', '_vh_clean.segs.json', '_vh_clean.aggregation.json', '_vh_clean_2.labels.ply', '_2d-instance.zip', '_2d-instance-filt.zip', '_2d-label.zip', '_2d-label-filt.zip']\nFILETYPES_TEST = ['.sens', '.txt', '_vh_clean.ply', '_vh_clean_2.ply']\nPREPROCESSED_FRAMES_FILE = ['scannet_frames_25k.zip', '5.6GB']\nTEST_FRAMES_FILE = ['scannet_frames_test.zip', '610MB']\nLABEL_MAP_FILES = ['scannetv2-labels.combined.tsv', 'scannet-labels.combined.tsv']\nDATA_EFFICIENT_FILES = ['limited-reconstruction-scenes.zip', 'limited-annotation-points.zip', 'limited-bboxes.zip', '1.7MB']\nGRIT_FILES = ['ScanNet-GRIT.zip']\nRELEASES = ['v2/scans', 'v1/scans']",
        "detail": "data.scannet.download-scannet",
        "documentation": {}
    },
    {
        "label": "TOS_URL",
        "kind": 5,
        "importPath": "data.scannet.download-scannet",
        "description": "data.scannet.download-scannet",
        "peekOfCode": "TOS_URL = BASE_URL + 'ScanNet_TOS.pdf'\nFILETYPES = ['.aggregation.json', '.sens', '.txt', '_vh_clean.ply', '_vh_clean_2.0.010000.segs.json', '_vh_clean_2.ply', '_vh_clean.segs.json', '_vh_clean.aggregation.json', '_vh_clean_2.labels.ply', '_2d-instance.zip', '_2d-instance-filt.zip', '_2d-label.zip', '_2d-label-filt.zip']\nFILETYPES_TEST = ['.sens', '.txt', '_vh_clean.ply', '_vh_clean_2.ply']\nPREPROCESSED_FRAMES_FILE = ['scannet_frames_25k.zip', '5.6GB']\nTEST_FRAMES_FILE = ['scannet_frames_test.zip', '610MB']\nLABEL_MAP_FILES = ['scannetv2-labels.combined.tsv', 'scannet-labels.combined.tsv']\nDATA_EFFICIENT_FILES = ['limited-reconstruction-scenes.zip', 'limited-annotation-points.zip', 'limited-bboxes.zip', '1.7MB']\nGRIT_FILES = ['ScanNet-GRIT.zip']\nRELEASES = ['v2/scans', 'v1/scans']\nRELEASES_TASKS = ['v2/tasks', 'v1/tasks']",
        "detail": "data.scannet.download-scannet",
        "documentation": {}
    },
    {
        "label": "FILETYPES",
        "kind": 5,
        "importPath": "data.scannet.download-scannet",
        "description": "data.scannet.download-scannet",
        "peekOfCode": "FILETYPES = ['.aggregation.json', '.sens', '.txt', '_vh_clean.ply', '_vh_clean_2.0.010000.segs.json', '_vh_clean_2.ply', '_vh_clean.segs.json', '_vh_clean.aggregation.json', '_vh_clean_2.labels.ply', '_2d-instance.zip', '_2d-instance-filt.zip', '_2d-label.zip', '_2d-label-filt.zip']\nFILETYPES_TEST = ['.sens', '.txt', '_vh_clean.ply', '_vh_clean_2.ply']\nPREPROCESSED_FRAMES_FILE = ['scannet_frames_25k.zip', '5.6GB']\nTEST_FRAMES_FILE = ['scannet_frames_test.zip', '610MB']\nLABEL_MAP_FILES = ['scannetv2-labels.combined.tsv', 'scannet-labels.combined.tsv']\nDATA_EFFICIENT_FILES = ['limited-reconstruction-scenes.zip', 'limited-annotation-points.zip', 'limited-bboxes.zip', '1.7MB']\nGRIT_FILES = ['ScanNet-GRIT.zip']\nRELEASES = ['v2/scans', 'v1/scans']\nRELEASES_TASKS = ['v2/tasks', 'v1/tasks']\nRELEASES_NAMES = ['v2', 'v1']",
        "detail": "data.scannet.download-scannet",
        "documentation": {}
    },
    {
        "label": "FILETYPES_TEST",
        "kind": 5,
        "importPath": "data.scannet.download-scannet",
        "description": "data.scannet.download-scannet",
        "peekOfCode": "FILETYPES_TEST = ['.sens', '.txt', '_vh_clean.ply', '_vh_clean_2.ply']\nPREPROCESSED_FRAMES_FILE = ['scannet_frames_25k.zip', '5.6GB']\nTEST_FRAMES_FILE = ['scannet_frames_test.zip', '610MB']\nLABEL_MAP_FILES = ['scannetv2-labels.combined.tsv', 'scannet-labels.combined.tsv']\nDATA_EFFICIENT_FILES = ['limited-reconstruction-scenes.zip', 'limited-annotation-points.zip', 'limited-bboxes.zip', '1.7MB']\nGRIT_FILES = ['ScanNet-GRIT.zip']\nRELEASES = ['v2/scans', 'v1/scans']\nRELEASES_TASKS = ['v2/tasks', 'v1/tasks']\nRELEASES_NAMES = ['v2', 'v1']\nRELEASE = RELEASES[0]",
        "detail": "data.scannet.download-scannet",
        "documentation": {}
    },
    {
        "label": "PREPROCESSED_FRAMES_FILE",
        "kind": 5,
        "importPath": "data.scannet.download-scannet",
        "description": "data.scannet.download-scannet",
        "peekOfCode": "PREPROCESSED_FRAMES_FILE = ['scannet_frames_25k.zip', '5.6GB']\nTEST_FRAMES_FILE = ['scannet_frames_test.zip', '610MB']\nLABEL_MAP_FILES = ['scannetv2-labels.combined.tsv', 'scannet-labels.combined.tsv']\nDATA_EFFICIENT_FILES = ['limited-reconstruction-scenes.zip', 'limited-annotation-points.zip', 'limited-bboxes.zip', '1.7MB']\nGRIT_FILES = ['ScanNet-GRIT.zip']\nRELEASES = ['v2/scans', 'v1/scans']\nRELEASES_TASKS = ['v2/tasks', 'v1/tasks']\nRELEASES_NAMES = ['v2', 'v1']\nRELEASE = RELEASES[0]\nRELEASE_TASKS = RELEASES_TASKS[0]",
        "detail": "data.scannet.download-scannet",
        "documentation": {}
    },
    {
        "label": "TEST_FRAMES_FILE",
        "kind": 5,
        "importPath": "data.scannet.download-scannet",
        "description": "data.scannet.download-scannet",
        "peekOfCode": "TEST_FRAMES_FILE = ['scannet_frames_test.zip', '610MB']\nLABEL_MAP_FILES = ['scannetv2-labels.combined.tsv', 'scannet-labels.combined.tsv']\nDATA_EFFICIENT_FILES = ['limited-reconstruction-scenes.zip', 'limited-annotation-points.zip', 'limited-bboxes.zip', '1.7MB']\nGRIT_FILES = ['ScanNet-GRIT.zip']\nRELEASES = ['v2/scans', 'v1/scans']\nRELEASES_TASKS = ['v2/tasks', 'v1/tasks']\nRELEASES_NAMES = ['v2', 'v1']\nRELEASE = RELEASES[0]\nRELEASE_TASKS = RELEASES_TASKS[0]\nRELEASE_NAME = RELEASES_NAMES[0]",
        "detail": "data.scannet.download-scannet",
        "documentation": {}
    },
    {
        "label": "LABEL_MAP_FILES",
        "kind": 5,
        "importPath": "data.scannet.download-scannet",
        "description": "data.scannet.download-scannet",
        "peekOfCode": "LABEL_MAP_FILES = ['scannetv2-labels.combined.tsv', 'scannet-labels.combined.tsv']\nDATA_EFFICIENT_FILES = ['limited-reconstruction-scenes.zip', 'limited-annotation-points.zip', 'limited-bboxes.zip', '1.7MB']\nGRIT_FILES = ['ScanNet-GRIT.zip']\nRELEASES = ['v2/scans', 'v1/scans']\nRELEASES_TASKS = ['v2/tasks', 'v1/tasks']\nRELEASES_NAMES = ['v2', 'v1']\nRELEASE = RELEASES[0]\nRELEASE_TASKS = RELEASES_TASKS[0]\nRELEASE_NAME = RELEASES_NAMES[0]\nLABEL_MAP_FILE = LABEL_MAP_FILES[0]",
        "detail": "data.scannet.download-scannet",
        "documentation": {}
    },
    {
        "label": "DATA_EFFICIENT_FILES",
        "kind": 5,
        "importPath": "data.scannet.download-scannet",
        "description": "data.scannet.download-scannet",
        "peekOfCode": "DATA_EFFICIENT_FILES = ['limited-reconstruction-scenes.zip', 'limited-annotation-points.zip', 'limited-bboxes.zip', '1.7MB']\nGRIT_FILES = ['ScanNet-GRIT.zip']\nRELEASES = ['v2/scans', 'v1/scans']\nRELEASES_TASKS = ['v2/tasks', 'v1/tasks']\nRELEASES_NAMES = ['v2', 'v1']\nRELEASE = RELEASES[0]\nRELEASE_TASKS = RELEASES_TASKS[0]\nRELEASE_NAME = RELEASES_NAMES[0]\nLABEL_MAP_FILE = LABEL_MAP_FILES[0]\nRELEASE_SIZE = '1.2TB'",
        "detail": "data.scannet.download-scannet",
        "documentation": {}
    },
    {
        "label": "GRIT_FILES",
        "kind": 5,
        "importPath": "data.scannet.download-scannet",
        "description": "data.scannet.download-scannet",
        "peekOfCode": "GRIT_FILES = ['ScanNet-GRIT.zip']\nRELEASES = ['v2/scans', 'v1/scans']\nRELEASES_TASKS = ['v2/tasks', 'v1/tasks']\nRELEASES_NAMES = ['v2', 'v1']\nRELEASE = RELEASES[0]\nRELEASE_TASKS = RELEASES_TASKS[0]\nRELEASE_NAME = RELEASES_NAMES[0]\nLABEL_MAP_FILE = LABEL_MAP_FILES[0]\nRELEASE_SIZE = '1.2TB'\nV1_IDX = 1",
        "detail": "data.scannet.download-scannet",
        "documentation": {}
    },
    {
        "label": "RELEASES",
        "kind": 5,
        "importPath": "data.scannet.download-scannet",
        "description": "data.scannet.download-scannet",
        "peekOfCode": "RELEASES = ['v2/scans', 'v1/scans']\nRELEASES_TASKS = ['v2/tasks', 'v1/tasks']\nRELEASES_NAMES = ['v2', 'v1']\nRELEASE = RELEASES[0]\nRELEASE_TASKS = RELEASES_TASKS[0]\nRELEASE_NAME = RELEASES_NAMES[0]\nLABEL_MAP_FILE = LABEL_MAP_FILES[0]\nRELEASE_SIZE = '1.2TB'\nV1_IDX = 1\ndef get_release_scans(release_file):",
        "detail": "data.scannet.download-scannet",
        "documentation": {}
    },
    {
        "label": "RELEASES_TASKS",
        "kind": 5,
        "importPath": "data.scannet.download-scannet",
        "description": "data.scannet.download-scannet",
        "peekOfCode": "RELEASES_TASKS = ['v2/tasks', 'v1/tasks']\nRELEASES_NAMES = ['v2', 'v1']\nRELEASE = RELEASES[0]\nRELEASE_TASKS = RELEASES_TASKS[0]\nRELEASE_NAME = RELEASES_NAMES[0]\nLABEL_MAP_FILE = LABEL_MAP_FILES[0]\nRELEASE_SIZE = '1.2TB'\nV1_IDX = 1\ndef get_release_scans(release_file):\n    scan_lines = urllib.request.urlopen(release_file)",
        "detail": "data.scannet.download-scannet",
        "documentation": {}
    },
    {
        "label": "RELEASES_NAMES",
        "kind": 5,
        "importPath": "data.scannet.download-scannet",
        "description": "data.scannet.download-scannet",
        "peekOfCode": "RELEASES_NAMES = ['v2', 'v1']\nRELEASE = RELEASES[0]\nRELEASE_TASKS = RELEASES_TASKS[0]\nRELEASE_NAME = RELEASES_NAMES[0]\nLABEL_MAP_FILE = LABEL_MAP_FILES[0]\nRELEASE_SIZE = '1.2TB'\nV1_IDX = 1\ndef get_release_scans(release_file):\n    scan_lines = urllib.request.urlopen(release_file)\n    # scan_lines = urllib.urlopen(release_file)",
        "detail": "data.scannet.download-scannet",
        "documentation": {}
    },
    {
        "label": "RELEASE",
        "kind": 5,
        "importPath": "data.scannet.download-scannet",
        "description": "data.scannet.download-scannet",
        "peekOfCode": "RELEASE = RELEASES[0]\nRELEASE_TASKS = RELEASES_TASKS[0]\nRELEASE_NAME = RELEASES_NAMES[0]\nLABEL_MAP_FILE = LABEL_MAP_FILES[0]\nRELEASE_SIZE = '1.2TB'\nV1_IDX = 1\ndef get_release_scans(release_file):\n    scan_lines = urllib.request.urlopen(release_file)\n    # scan_lines = urllib.urlopen(release_file)\n    scans = []",
        "detail": "data.scannet.download-scannet",
        "documentation": {}
    },
    {
        "label": "RELEASE_TASKS",
        "kind": 5,
        "importPath": "data.scannet.download-scannet",
        "description": "data.scannet.download-scannet",
        "peekOfCode": "RELEASE_TASKS = RELEASES_TASKS[0]\nRELEASE_NAME = RELEASES_NAMES[0]\nLABEL_MAP_FILE = LABEL_MAP_FILES[0]\nRELEASE_SIZE = '1.2TB'\nV1_IDX = 1\ndef get_release_scans(release_file):\n    scan_lines = urllib.request.urlopen(release_file)\n    # scan_lines = urllib.urlopen(release_file)\n    scans = []\n    for scan_line in scan_lines:",
        "detail": "data.scannet.download-scannet",
        "documentation": {}
    },
    {
        "label": "RELEASE_NAME",
        "kind": 5,
        "importPath": "data.scannet.download-scannet",
        "description": "data.scannet.download-scannet",
        "peekOfCode": "RELEASE_NAME = RELEASES_NAMES[0]\nLABEL_MAP_FILE = LABEL_MAP_FILES[0]\nRELEASE_SIZE = '1.2TB'\nV1_IDX = 1\ndef get_release_scans(release_file):\n    scan_lines = urllib.request.urlopen(release_file)\n    # scan_lines = urllib.urlopen(release_file)\n    scans = []\n    for scan_line in scan_lines:\n        scan_id = scan_line.decode('utf8').rstrip('\\n')",
        "detail": "data.scannet.download-scannet",
        "documentation": {}
    },
    {
        "label": "LABEL_MAP_FILE",
        "kind": 5,
        "importPath": "data.scannet.download-scannet",
        "description": "data.scannet.download-scannet",
        "peekOfCode": "LABEL_MAP_FILE = LABEL_MAP_FILES[0]\nRELEASE_SIZE = '1.2TB'\nV1_IDX = 1\ndef get_release_scans(release_file):\n    scan_lines = urllib.request.urlopen(release_file)\n    # scan_lines = urllib.urlopen(release_file)\n    scans = []\n    for scan_line in scan_lines:\n        scan_id = scan_line.decode('utf8').rstrip('\\n')\n        scans.append(scan_id)",
        "detail": "data.scannet.download-scannet",
        "documentation": {}
    },
    {
        "label": "RELEASE_SIZE",
        "kind": 5,
        "importPath": "data.scannet.download-scannet",
        "description": "data.scannet.download-scannet",
        "peekOfCode": "RELEASE_SIZE = '1.2TB'\nV1_IDX = 1\ndef get_release_scans(release_file):\n    scan_lines = urllib.request.urlopen(release_file)\n    # scan_lines = urllib.urlopen(release_file)\n    scans = []\n    for scan_line in scan_lines:\n        scan_id = scan_line.decode('utf8').rstrip('\\n')\n        scans.append(scan_id)\n    return scans",
        "detail": "data.scannet.download-scannet",
        "documentation": {}
    },
    {
        "label": "V1_IDX",
        "kind": 5,
        "importPath": "data.scannet.download-scannet",
        "description": "data.scannet.download-scannet",
        "peekOfCode": "V1_IDX = 1\ndef get_release_scans(release_file):\n    scan_lines = urllib.request.urlopen(release_file)\n    # scan_lines = urllib.urlopen(release_file)\n    scans = []\n    for scan_line in scan_lines:\n        scan_id = scan_line.decode('utf8').rstrip('\\n')\n        scans.append(scan_id)\n    return scans\ndef download_release(release_scans, out_dir, file_types, use_v1_sens):",
        "detail": "data.scannet.download-scannet",
        "documentation": {}
    },
    {
        "label": "get_release_scans",
        "kind": 2,
        "importPath": "data.scannet.download",
        "description": "data.scannet.download",
        "peekOfCode": "def get_release_scans(release_file):\n    #scan_lines = urllib.request.urlopen(release_file)\n    scan_lines = urllib.urlopen(release_file)\n    scans = []\n    for scan_line in scan_lines:\n        scan_id = scan_line.decode('utf8').rstrip('\\n')\n        scans.append(scan_id)\n    return scans\ndef download_release(release_scans, out_dir, file_types, use_v1_sens):\n    if len(release_scans) == 0:",
        "detail": "data.scannet.download",
        "documentation": {}
    },
    {
        "label": "download_release",
        "kind": 2,
        "importPath": "data.scannet.download",
        "description": "data.scannet.download",
        "peekOfCode": "def download_release(release_scans, out_dir, file_types, use_v1_sens):\n    if len(release_scans) == 0:\n        return\n    print('Downloading ScanNet ' + RELEASE_NAME + ' release to ' + out_dir + '...')\n    for scan_id in release_scans:\n        scan_out_dir = os.path.join(out_dir, scan_id)\n        download_scan(scan_id, scan_out_dir, file_types, use_v1_sens)\n    print('Downloaded ScanNet ' + RELEASE_NAME + ' release.')\ndef download_file(url, out_file):\n    out_dir = os.path.dirname(out_file)",
        "detail": "data.scannet.download",
        "documentation": {}
    },
    {
        "label": "download_file",
        "kind": 2,
        "importPath": "data.scannet.download",
        "description": "data.scannet.download",
        "peekOfCode": "def download_file(url, out_file):\n    out_dir = os.path.dirname(out_file)\n    if not os.path.isdir(out_dir):\n        os.makedirs(out_dir)\n    if not os.path.isfile(out_file):\n        print('\\t' + url + ' > ' + out_file)\n        fh, out_file_tmp = tempfile.mkstemp(dir=out_dir)\n        f = os.fdopen(fh, 'w')\n        f.close()\n        #urllib.request.urlretrieve(url, out_file_tmp)",
        "detail": "data.scannet.download",
        "documentation": {}
    },
    {
        "label": "download_scan",
        "kind": 2,
        "importPath": "data.scannet.download",
        "description": "data.scannet.download",
        "peekOfCode": "def download_scan(scan_id, out_dir, file_types, use_v1_sens):\n    print('Downloading ScanNet ' + RELEASE_NAME + ' scan ' + scan_id + ' ...')\n    if not os.path.isdir(out_dir):\n        os.makedirs(out_dir)\n    for ft in file_types:\n        v1_sens = use_v1_sens and ft == '.sens'\n        url = BASE_URL + RELEASE + '/' + scan_id + '/' + scan_id + ft if not v1_sens else BASE_URL + RELEASES[V1_IDX] + '/' + scan_id + '/' + scan_id + ft\n        out_file = out_dir + '/' + scan_id + ft\n        download_file(url, out_file)\n    print('Downloaded scan ' + scan_id)",
        "detail": "data.scannet.download",
        "documentation": {}
    },
    {
        "label": "download_task_data",
        "kind": 2,
        "importPath": "data.scannet.download",
        "description": "data.scannet.download",
        "peekOfCode": "def download_task_data(out_dir):\n    print('Downloading ScanNet v1 task data...')\n    files = [\n        LABEL_MAP_FILES[V1_IDX], 'obj_classification/data.zip',\n        'obj_classification/trained_models.zip', 'voxel_labeling/data.zip',\n        'voxel_labeling/trained_models.zip'\n    ]\n    for file in files:\n        url = BASE_URL + RELEASES_TASKS[V1_IDX] + '/' + file\n        localpath = os.path.join(out_dir, file)",
        "detail": "data.scannet.download",
        "documentation": {}
    },
    {
        "label": "download_label_map",
        "kind": 2,
        "importPath": "data.scannet.download",
        "description": "data.scannet.download",
        "peekOfCode": "def download_label_map(out_dir):\n    print('Downloading ScanNet ' + RELEASE_NAME + ' label mapping file...')\n    files = [ LABEL_MAP_FILE ]\n    for file in files:\n        url = BASE_URL + RELEASE_TASKS + '/' + file\n        localpath = os.path.join(out_dir, file)\n        localdir = os.path.dirname(localpath)\n        if not os.path.isdir(localdir):\n          os.makedirs(localdir)\n        download_file(url, localpath)",
        "detail": "data.scannet.download",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "data.scannet.download",
        "description": "data.scannet.download",
        "peekOfCode": "def main():\n    parser = argparse.ArgumentParser(description='Downloads ScanNet public data release.')\n    parser.add_argument('-o', '--out_dir', required=True, help='directory in which to download')\n    parser.add_argument('--task_data', action='store_true', help='download task data (v1)')\n    parser.add_argument('--label_map', action='store_true', help='download label map file')\n    parser.add_argument('--v1', action='store_true', help='download ScanNet v1 instead of v2')\n    parser.add_argument('--id', help='specific scan id to download')\n    parser.add_argument('--preprocessed_frames', action='store_true', help='download preprocessed subset of ScanNet frames (' + PREPROCESSED_FRAMES_FILE[1] + ')')\n    parser.add_argument('--test_frames_2d', action='store_true', help='download 2D test frames (' + TEST_FRAMES_FILE[1] + '; also included with whole dataset download)')\n    parser.add_argument('--type', help='specific file type to download (.aggregation.json, .sens, .txt, _vh_clean.ply, _vh_clean_2.0.010000.segs.json, _vh_clean_2.ply, _vh_clean.segs.json, _vh_clean.aggregation.json, _vh_clean_2.labels.ply, _2d-instance.zip, _2d-instance-filt.zip, _2d-label.zip, _2d-label-filt.zip)')",
        "detail": "data.scannet.download",
        "documentation": {}
    },
    {
        "label": "BASE_URL",
        "kind": 5,
        "importPath": "data.scannet.download",
        "description": "data.scannet.download",
        "peekOfCode": "BASE_URL = 'http://kaldir.vc.in.tum.de/scannet/'\nTOS_URL = BASE_URL + 'ScanNet_TOS.pdf'\nFILETYPES = ['.aggregation.json', '.sens', '.txt', '_vh_clean.ply', '_vh_clean_2.0.010000.segs.json', '_vh_clean_2.ply', '_vh_clean.segs.json', '_vh_clean.aggregation.json', '_vh_clean_2.labels.ply', '_2d-instance.zip', '_2d-instance-filt.zip', '_2d-label.zip', '_2d-label-filt.zip']\nFILETYPES_TEST = ['.sens', '.txt', '_vh_clean.ply', '_vh_clean_2.ply']\nPREPROCESSED_FRAMES_FILE = ['scannet_frames_25k.zip', '5.6GB']\nTEST_FRAMES_FILE = ['scannet_frames_test.zip', '610MB']\nLABEL_MAP_FILES = ['scannetv2-labels.combined.tsv', 'scannet-labels.combined.tsv']\nRELEASES = ['v2/scans', 'v1/scans']\nRELEASES_TASKS = ['v2/tasks', 'v1/tasks']\nRELEASES_NAMES = ['v2', 'v1']",
        "detail": "data.scannet.download",
        "documentation": {}
    },
    {
        "label": "TOS_URL",
        "kind": 5,
        "importPath": "data.scannet.download",
        "description": "data.scannet.download",
        "peekOfCode": "TOS_URL = BASE_URL + 'ScanNet_TOS.pdf'\nFILETYPES = ['.aggregation.json', '.sens', '.txt', '_vh_clean.ply', '_vh_clean_2.0.010000.segs.json', '_vh_clean_2.ply', '_vh_clean.segs.json', '_vh_clean.aggregation.json', '_vh_clean_2.labels.ply', '_2d-instance.zip', '_2d-instance-filt.zip', '_2d-label.zip', '_2d-label-filt.zip']\nFILETYPES_TEST = ['.sens', '.txt', '_vh_clean.ply', '_vh_clean_2.ply']\nPREPROCESSED_FRAMES_FILE = ['scannet_frames_25k.zip', '5.6GB']\nTEST_FRAMES_FILE = ['scannet_frames_test.zip', '610MB']\nLABEL_MAP_FILES = ['scannetv2-labels.combined.tsv', 'scannet-labels.combined.tsv']\nRELEASES = ['v2/scans', 'v1/scans']\nRELEASES_TASKS = ['v2/tasks', 'v1/tasks']\nRELEASES_NAMES = ['v2', 'v1']\nRELEASE = RELEASES[0]",
        "detail": "data.scannet.download",
        "documentation": {}
    },
    {
        "label": "FILETYPES",
        "kind": 5,
        "importPath": "data.scannet.download",
        "description": "data.scannet.download",
        "peekOfCode": "FILETYPES = ['.aggregation.json', '.sens', '.txt', '_vh_clean.ply', '_vh_clean_2.0.010000.segs.json', '_vh_clean_2.ply', '_vh_clean.segs.json', '_vh_clean.aggregation.json', '_vh_clean_2.labels.ply', '_2d-instance.zip', '_2d-instance-filt.zip', '_2d-label.zip', '_2d-label-filt.zip']\nFILETYPES_TEST = ['.sens', '.txt', '_vh_clean.ply', '_vh_clean_2.ply']\nPREPROCESSED_FRAMES_FILE = ['scannet_frames_25k.zip', '5.6GB']\nTEST_FRAMES_FILE = ['scannet_frames_test.zip', '610MB']\nLABEL_MAP_FILES = ['scannetv2-labels.combined.tsv', 'scannet-labels.combined.tsv']\nRELEASES = ['v2/scans', 'v1/scans']\nRELEASES_TASKS = ['v2/tasks', 'v1/tasks']\nRELEASES_NAMES = ['v2', 'v1']\nRELEASE = RELEASES[0]\nRELEASE_TASKS = RELEASES_TASKS[0]",
        "detail": "data.scannet.download",
        "documentation": {}
    },
    {
        "label": "FILETYPES_TEST",
        "kind": 5,
        "importPath": "data.scannet.download",
        "description": "data.scannet.download",
        "peekOfCode": "FILETYPES_TEST = ['.sens', '.txt', '_vh_clean.ply', '_vh_clean_2.ply']\nPREPROCESSED_FRAMES_FILE = ['scannet_frames_25k.zip', '5.6GB']\nTEST_FRAMES_FILE = ['scannet_frames_test.zip', '610MB']\nLABEL_MAP_FILES = ['scannetv2-labels.combined.tsv', 'scannet-labels.combined.tsv']\nRELEASES = ['v2/scans', 'v1/scans']\nRELEASES_TASKS = ['v2/tasks', 'v1/tasks']\nRELEASES_NAMES = ['v2', 'v1']\nRELEASE = RELEASES[0]\nRELEASE_TASKS = RELEASES_TASKS[0]\nRELEASE_NAME = RELEASES_NAMES[0]",
        "detail": "data.scannet.download",
        "documentation": {}
    },
    {
        "label": "PREPROCESSED_FRAMES_FILE",
        "kind": 5,
        "importPath": "data.scannet.download",
        "description": "data.scannet.download",
        "peekOfCode": "PREPROCESSED_FRAMES_FILE = ['scannet_frames_25k.zip', '5.6GB']\nTEST_FRAMES_FILE = ['scannet_frames_test.zip', '610MB']\nLABEL_MAP_FILES = ['scannetv2-labels.combined.tsv', 'scannet-labels.combined.tsv']\nRELEASES = ['v2/scans', 'v1/scans']\nRELEASES_TASKS = ['v2/tasks', 'v1/tasks']\nRELEASES_NAMES = ['v2', 'v1']\nRELEASE = RELEASES[0]\nRELEASE_TASKS = RELEASES_TASKS[0]\nRELEASE_NAME = RELEASES_NAMES[0]\nLABEL_MAP_FILE = LABEL_MAP_FILES[0]",
        "detail": "data.scannet.download",
        "documentation": {}
    },
    {
        "label": "TEST_FRAMES_FILE",
        "kind": 5,
        "importPath": "data.scannet.download",
        "description": "data.scannet.download",
        "peekOfCode": "TEST_FRAMES_FILE = ['scannet_frames_test.zip', '610MB']\nLABEL_MAP_FILES = ['scannetv2-labels.combined.tsv', 'scannet-labels.combined.tsv']\nRELEASES = ['v2/scans', 'v1/scans']\nRELEASES_TASKS = ['v2/tasks', 'v1/tasks']\nRELEASES_NAMES = ['v2', 'v1']\nRELEASE = RELEASES[0]\nRELEASE_TASKS = RELEASES_TASKS[0]\nRELEASE_NAME = RELEASES_NAMES[0]\nLABEL_MAP_FILE = LABEL_MAP_FILES[0]\nRELEASE_SIZE = '1.2TB'",
        "detail": "data.scannet.download",
        "documentation": {}
    },
    {
        "label": "LABEL_MAP_FILES",
        "kind": 5,
        "importPath": "data.scannet.download",
        "description": "data.scannet.download",
        "peekOfCode": "LABEL_MAP_FILES = ['scannetv2-labels.combined.tsv', 'scannet-labels.combined.tsv']\nRELEASES = ['v2/scans', 'v1/scans']\nRELEASES_TASKS = ['v2/tasks', 'v1/tasks']\nRELEASES_NAMES = ['v2', 'v1']\nRELEASE = RELEASES[0]\nRELEASE_TASKS = RELEASES_TASKS[0]\nRELEASE_NAME = RELEASES_NAMES[0]\nLABEL_MAP_FILE = LABEL_MAP_FILES[0]\nRELEASE_SIZE = '1.2TB'\nV1_IDX = 1",
        "detail": "data.scannet.download",
        "documentation": {}
    },
    {
        "label": "RELEASES",
        "kind": 5,
        "importPath": "data.scannet.download",
        "description": "data.scannet.download",
        "peekOfCode": "RELEASES = ['v2/scans', 'v1/scans']\nRELEASES_TASKS = ['v2/tasks', 'v1/tasks']\nRELEASES_NAMES = ['v2', 'v1']\nRELEASE = RELEASES[0]\nRELEASE_TASKS = RELEASES_TASKS[0]\nRELEASE_NAME = RELEASES_NAMES[0]\nLABEL_MAP_FILE = LABEL_MAP_FILES[0]\nRELEASE_SIZE = '1.2TB'\nV1_IDX = 1\ndef get_release_scans(release_file):",
        "detail": "data.scannet.download",
        "documentation": {}
    },
    {
        "label": "RELEASES_TASKS",
        "kind": 5,
        "importPath": "data.scannet.download",
        "description": "data.scannet.download",
        "peekOfCode": "RELEASES_TASKS = ['v2/tasks', 'v1/tasks']\nRELEASES_NAMES = ['v2', 'v1']\nRELEASE = RELEASES[0]\nRELEASE_TASKS = RELEASES_TASKS[0]\nRELEASE_NAME = RELEASES_NAMES[0]\nLABEL_MAP_FILE = LABEL_MAP_FILES[0]\nRELEASE_SIZE = '1.2TB'\nV1_IDX = 1\ndef get_release_scans(release_file):\n    #scan_lines = urllib.request.urlopen(release_file)",
        "detail": "data.scannet.download",
        "documentation": {}
    },
    {
        "label": "RELEASES_NAMES",
        "kind": 5,
        "importPath": "data.scannet.download",
        "description": "data.scannet.download",
        "peekOfCode": "RELEASES_NAMES = ['v2', 'v1']\nRELEASE = RELEASES[0]\nRELEASE_TASKS = RELEASES_TASKS[0]\nRELEASE_NAME = RELEASES_NAMES[0]\nLABEL_MAP_FILE = LABEL_MAP_FILES[0]\nRELEASE_SIZE = '1.2TB'\nV1_IDX = 1\ndef get_release_scans(release_file):\n    #scan_lines = urllib.request.urlopen(release_file)\n    scan_lines = urllib.urlopen(release_file)",
        "detail": "data.scannet.download",
        "documentation": {}
    },
    {
        "label": "RELEASE",
        "kind": 5,
        "importPath": "data.scannet.download",
        "description": "data.scannet.download",
        "peekOfCode": "RELEASE = RELEASES[0]\nRELEASE_TASKS = RELEASES_TASKS[0]\nRELEASE_NAME = RELEASES_NAMES[0]\nLABEL_MAP_FILE = LABEL_MAP_FILES[0]\nRELEASE_SIZE = '1.2TB'\nV1_IDX = 1\ndef get_release_scans(release_file):\n    #scan_lines = urllib.request.urlopen(release_file)\n    scan_lines = urllib.urlopen(release_file)\n    scans = []",
        "detail": "data.scannet.download",
        "documentation": {}
    },
    {
        "label": "RELEASE_TASKS",
        "kind": 5,
        "importPath": "data.scannet.download",
        "description": "data.scannet.download",
        "peekOfCode": "RELEASE_TASKS = RELEASES_TASKS[0]\nRELEASE_NAME = RELEASES_NAMES[0]\nLABEL_MAP_FILE = LABEL_MAP_FILES[0]\nRELEASE_SIZE = '1.2TB'\nV1_IDX = 1\ndef get_release_scans(release_file):\n    #scan_lines = urllib.request.urlopen(release_file)\n    scan_lines = urllib.urlopen(release_file)\n    scans = []\n    for scan_line in scan_lines:",
        "detail": "data.scannet.download",
        "documentation": {}
    },
    {
        "label": "RELEASE_NAME",
        "kind": 5,
        "importPath": "data.scannet.download",
        "description": "data.scannet.download",
        "peekOfCode": "RELEASE_NAME = RELEASES_NAMES[0]\nLABEL_MAP_FILE = LABEL_MAP_FILES[0]\nRELEASE_SIZE = '1.2TB'\nV1_IDX = 1\ndef get_release_scans(release_file):\n    #scan_lines = urllib.request.urlopen(release_file)\n    scan_lines = urllib.urlopen(release_file)\n    scans = []\n    for scan_line in scan_lines:\n        scan_id = scan_line.decode('utf8').rstrip('\\n')",
        "detail": "data.scannet.download",
        "documentation": {}
    },
    {
        "label": "LABEL_MAP_FILE",
        "kind": 5,
        "importPath": "data.scannet.download",
        "description": "data.scannet.download",
        "peekOfCode": "LABEL_MAP_FILE = LABEL_MAP_FILES[0]\nRELEASE_SIZE = '1.2TB'\nV1_IDX = 1\ndef get_release_scans(release_file):\n    #scan_lines = urllib.request.urlopen(release_file)\n    scan_lines = urllib.urlopen(release_file)\n    scans = []\n    for scan_line in scan_lines:\n        scan_id = scan_line.decode('utf8').rstrip('\\n')\n        scans.append(scan_id)",
        "detail": "data.scannet.download",
        "documentation": {}
    },
    {
        "label": "RELEASE_SIZE",
        "kind": 5,
        "importPath": "data.scannet.download",
        "description": "data.scannet.download",
        "peekOfCode": "RELEASE_SIZE = '1.2TB'\nV1_IDX = 1\ndef get_release_scans(release_file):\n    #scan_lines = urllib.request.urlopen(release_file)\n    scan_lines = urllib.urlopen(release_file)\n    scans = []\n    for scan_line in scan_lines:\n        scan_id = scan_line.decode('utf8').rstrip('\\n')\n        scans.append(scan_id)\n    return scans",
        "detail": "data.scannet.download",
        "documentation": {}
    },
    {
        "label": "V1_IDX",
        "kind": 5,
        "importPath": "data.scannet.download",
        "description": "data.scannet.download",
        "peekOfCode": "V1_IDX = 1\ndef get_release_scans(release_file):\n    #scan_lines = urllib.request.urlopen(release_file)\n    scan_lines = urllib.urlopen(release_file)\n    scans = []\n    for scan_line in scan_lines:\n        scan_id = scan_line.decode('utf8').rstrip('\\n')\n        scans.append(scan_id)\n    return scans\ndef download_release(release_scans, out_dir, file_types, use_v1_sens):",
        "detail": "data.scannet.download",
        "documentation": {}
    },
    {
        "label": "read_aggregation",
        "kind": 2,
        "importPath": "data.scannet.load_scannet_data",
        "description": "data.scannet.load_scannet_data",
        "peekOfCode": "def read_aggregation(filename):\n    object_id_to_segs = {}\n    label_to_segs = {}\n    with open(filename) as f:\n        data = json.load(f)\n        num_objects = len(data['segGroups'])\n        for i in range(num_objects):\n            object_id = data['segGroups'][i]['objectId'] + 1 # instance ids should be 1-indexed\n            label = data['segGroups'][i]['label']\n            segs = data['segGroups'][i]['segments']",
        "detail": "data.scannet.load_scannet_data",
        "documentation": {}
    },
    {
        "label": "read_segmentation",
        "kind": 2,
        "importPath": "data.scannet.load_scannet_data",
        "description": "data.scannet.load_scannet_data",
        "peekOfCode": "def read_segmentation(filename):\n    seg_to_verts = {}\n    with open(filename) as f:\n        data = json.load(f)\n        num_verts = len(data['segIndices'])\n        for i in range(num_verts):\n            seg_id = data['segIndices'][i]\n            if seg_id in seg_to_verts:\n                seg_to_verts[seg_id].append(i)\n            else:",
        "detail": "data.scannet.load_scannet_data",
        "documentation": {}
    },
    {
        "label": "export",
        "kind": 2,
        "importPath": "data.scannet.load_scannet_data",
        "description": "data.scannet.load_scannet_data",
        "peekOfCode": "def export(mesh_file, agg_file, seg_file, meta_file, label_map_file, output_file=None):\n    \"\"\" points are XYZ RGB (RGB in 0-255),\n    semantic label as nyu40 ids,\n    instance label as 1-#instance,\n    box as (cx,cy,cz,dx,dy,dz,semantic_label)\n    \"\"\"\n    label_map = scannet_utils.read_label_mapping(label_map_file, label_from='raw_category', label_to='nyu40id')    \n    # mesh_vertices = scannet_utils.read_mesh_vertices_rgb(mesh_file)\n    mesh_vertices = scannet_utils.read_mesh_vertices_rgb_normal(mesh_file)\n    # Load scene axis alignment matrix",
        "detail": "data.scannet.load_scannet_data",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "data.scannet.load_scannet_data",
        "description": "data.scannet.load_scannet_data",
        "peekOfCode": "def main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--scan_path', required=True, help='path to scannet scene (e.g., data/ScanNet/v2/scene0000_00')\n    parser.add_argument('--output_file', required=True, help='output file')\n    parser.add_argument('--label_map_file', required=True, help='path to scannetv2-labels.combined.tsv')\n    opt = parser.parse_args()\n    scan_name = os.path.split(opt.scan_path)[-1]\n    mesh_file = os.path.join(opt.scan_path, scan_name + '_vh_clean_2.ply')\n    agg_file = os.path.join(opt.scan_path, scan_name + '.aggregation.json')\n    seg_file = os.path.join(opt.scan_path, scan_name + '_vh_clean_2.0.010000.segs.json')",
        "detail": "data.scannet.load_scannet_data",
        "documentation": {}
    },
    {
        "label": "ScannetDatasetConfig",
        "kind": 6,
        "importPath": "data.scannet.model_util_scannet",
        "description": "data.scannet.model_util_scannet",
        "peekOfCode": "class ScannetDatasetConfig(object):\n    def __init__(self):\n        self.type2class = {'cabinet':0, 'bed':1, 'chair':2, 'sofa':3, 'table':4, 'door':5,\n            'window':6,'bookshelf':7,'picture':8, 'counter':9, 'desk':10, 'curtain':11,\n            'refrigerator':12, 'shower curtain':13, 'toilet':14, 'sink':15, 'bathtub':16, 'others':17}  \n        self.class2type = {self.type2class[t]:t for t in self.type2class}\n        self.nyu40ids = np.array([3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]) # exclude wall (1), floor (2), ceiling (22)\n        self.nyu40id2class = self._get_nyu40id2class()\n        self.mean_size_arr = np.load(os.path.join(CONF.PATH.SCANNET, 'meta_data/scannet_reference_means.npz'))['arr_0']\n        self.num_class = len(self.type2class.keys())",
        "detail": "data.scannet.model_util_scannet",
        "documentation": {}
    },
    {
        "label": "in_hull",
        "kind": 2,
        "importPath": "data.scannet.model_util_scannet",
        "description": "data.scannet.model_util_scannet",
        "peekOfCode": "def in_hull(p, hull):\n    from scipy.spatial import Delaunay\n    if not isinstance(hull,Delaunay):\n        hull = Delaunay(hull)\n    return hull.find_simplex(p)>=0\ndef extract_pc_in_box3d(pc, box3d):\n    ''' pc: (N,3), box3d: (8,3) '''\n    box3d_roi_inds = in_hull(pc[:,0:3], box3d)\n    return pc[box3d_roi_inds,:], box3d_roi_inds\ndef rotate_aligned_boxes(input_boxes, rot_mat):    ",
        "detail": "data.scannet.model_util_scannet",
        "documentation": {}
    },
    {
        "label": "extract_pc_in_box3d",
        "kind": 2,
        "importPath": "data.scannet.model_util_scannet",
        "description": "data.scannet.model_util_scannet",
        "peekOfCode": "def extract_pc_in_box3d(pc, box3d):\n    ''' pc: (N,3), box3d: (8,3) '''\n    box3d_roi_inds = in_hull(pc[:,0:3], box3d)\n    return pc[box3d_roi_inds,:], box3d_roi_inds\ndef rotate_aligned_boxes(input_boxes, rot_mat):    \n    centers, lengths = input_boxes[:,0:3], input_boxes[:,3:6]    \n    new_centers = np.dot(centers, np.transpose(rot_mat))\n    dx, dy = lengths[:,0]/2.0, lengths[:,1]/2.0\n    new_x = np.zeros((dx.shape[0], 4))\n    new_y = np.zeros((dx.shape[0], 4))",
        "detail": "data.scannet.model_util_scannet",
        "documentation": {}
    },
    {
        "label": "rotate_aligned_boxes",
        "kind": 2,
        "importPath": "data.scannet.model_util_scannet",
        "description": "data.scannet.model_util_scannet",
        "peekOfCode": "def rotate_aligned_boxes(input_boxes, rot_mat):    \n    centers, lengths = input_boxes[:,0:3], input_boxes[:,3:6]    \n    new_centers = np.dot(centers, np.transpose(rot_mat))\n    dx, dy = lengths[:,0]/2.0, lengths[:,1]/2.0\n    new_x = np.zeros((dx.shape[0], 4))\n    new_y = np.zeros((dx.shape[0], 4))\n    for i, crnr in enumerate([(-1,-1), (1, -1), (1, 1), (-1, 1)]):        \n        crnrs = np.zeros((dx.shape[0], 3))\n        crnrs[:,0] = crnr[0]*dx\n        crnrs[:,1] = crnr[1]*dy",
        "detail": "data.scannet.model_util_scannet",
        "documentation": {}
    },
    {
        "label": "rotate_aligned_boxes_along_axis",
        "kind": 2,
        "importPath": "data.scannet.model_util_scannet",
        "description": "data.scannet.model_util_scannet",
        "peekOfCode": "def rotate_aligned_boxes_along_axis(input_boxes, rot_mat, axis):    \n    centers, lengths = input_boxes[:,0:3], input_boxes[:,3:6]    \n    new_centers = np.dot(centers, np.transpose(rot_mat))\n    if axis == \"x\":     \n        d1, d2 = lengths[:,1]/2.0, lengths[:,2]/2.0\n    elif axis == \"y\":\n        d1, d2 = lengths[:,0]/2.0, lengths[:,2]/2.0\n    else:\n        d1, d2 = lengths[:,0]/2.0, lengths[:,1]/2.0\n    new_1 = np.zeros((d1.shape[0], 4))",
        "detail": "data.scannet.model_util_scannet",
        "documentation": {}
    },
    {
        "label": "normalize_v3",
        "kind": 2,
        "importPath": "data.scannet.scannet_utils",
        "description": "data.scannet.scannet_utils",
        "peekOfCode": "def normalize_v3(arr):\n    ''' Normalize a numpy array of 3 component vectors shape=(n,3) '''\n    lens = np.sqrt( arr[:,0]**2 + arr[:,1]**2 + arr[:,2]**2 )\n    arr[:,0] /= (lens + 1e-8)\n    arr[:,1] /= (lens + 1e-8)\n    arr[:,2] /= (lens + 1e-8)                \n    return arr\ndef compute_normal(vertices, faces):\n    #Create a zeroed array with the same type and shape as our vertices i.e., per vertex normal\n    normals = np.zeros( vertices.shape, dtype=vertices.dtype )",
        "detail": "data.scannet.scannet_utils",
        "documentation": {}
    },
    {
        "label": "compute_normal",
        "kind": 2,
        "importPath": "data.scannet.scannet_utils",
        "description": "data.scannet.scannet_utils",
        "peekOfCode": "def compute_normal(vertices, faces):\n    #Create a zeroed array with the same type and shape as our vertices i.e., per vertex normal\n    normals = np.zeros( vertices.shape, dtype=vertices.dtype )\n    #Create an indexed view into the vertex array using the array of three indices for triangles\n    tris = vertices[faces]\n    #Calculate the normal for all the triangles, by taking the cross product of the vectors v1-v0, and v2-v0 in each triangle             \n    n = np.cross( tris[::,1 ] - tris[::,0]  , tris[::,2 ] - tris[::,0] )\n    # n is now an array of normals per triangle. The length of each normal is dependent the vertices, \n    # we need to normalize these, so that our next step weights each normal equally.\n    normalize_v3(n)",
        "detail": "data.scannet.scannet_utils",
        "documentation": {}
    },
    {
        "label": "represents_int",
        "kind": 2,
        "importPath": "data.scannet.scannet_utils",
        "description": "data.scannet.scannet_utils",
        "peekOfCode": "def represents_int(s):\n    ''' if string s represents an int. '''\n    try: \n        int(s)\n        return True\n    except ValueError:\n        return False\ndef read_label_mapping(filename, label_from='raw_category', label_to='nyu40id'):\n    assert os.path.isfile(filename)\n    mapping = dict()",
        "detail": "data.scannet.scannet_utils",
        "documentation": {}
    },
    {
        "label": "read_label_mapping",
        "kind": 2,
        "importPath": "data.scannet.scannet_utils",
        "description": "data.scannet.scannet_utils",
        "peekOfCode": "def read_label_mapping(filename, label_from='raw_category', label_to='nyu40id'):\n    assert os.path.isfile(filename)\n    mapping = dict()\n    with open(filename) as csvfile:\n        reader = csv.DictReader(csvfile, delimiter='\\t')\n        for row in reader:\n            mapping[row[label_from]] = int(row[label_to])\n    if represents_int(list(mapping.keys())[0]):\n        mapping = {int(k):v for k,v in mapping.items()}\n    return mapping",
        "detail": "data.scannet.scannet_utils",
        "documentation": {}
    },
    {
        "label": "read_mesh_vertices",
        "kind": 2,
        "importPath": "data.scannet.scannet_utils",
        "description": "data.scannet.scannet_utils",
        "peekOfCode": "def read_mesh_vertices(filename):\n    \"\"\" read XYZ for each vertex.\n    \"\"\"\n    assert os.path.isfile(filename)\n    with open(filename, 'rb') as f:\n        plydata = PlyData.read(f)\n        num_verts = plydata['vertex'].count\n        vertices = np.zeros(shape=[num_verts, 3], dtype=np.float32)\n        vertices[:,0] = plydata['vertex'].data['x']\n        vertices[:,1] = plydata['vertex'].data['y']",
        "detail": "data.scannet.scannet_utils",
        "documentation": {}
    },
    {
        "label": "read_mesh_vertices_rgb",
        "kind": 2,
        "importPath": "data.scannet.scannet_utils",
        "description": "data.scannet.scannet_utils",
        "peekOfCode": "def read_mesh_vertices_rgb(filename):\n    \"\"\" read XYZ RGB for each vertex.\n    Note: RGB values are in 0-255\n    \"\"\"\n    assert os.path.isfile(filename)\n    with open(filename, 'rb') as f:\n        plydata = PlyData.read(f)\n        num_verts = plydata['vertex'].count\n        vertices = np.zeros(shape=[num_verts, 6], dtype=np.float32)\n        vertices[:,0] = plydata['vertex'].data['x']",
        "detail": "data.scannet.scannet_utils",
        "documentation": {}
    },
    {
        "label": "read_mesh_vertices_rgb_normal",
        "kind": 2,
        "importPath": "data.scannet.scannet_utils",
        "description": "data.scannet.scannet_utils",
        "peekOfCode": "def read_mesh_vertices_rgb_normal(filename):\n    \"\"\" read XYZ RGB normals point cloud from filename PLY file \"\"\"\n    assert(os.path.isfile(filename))\n    with open(filename, 'rb') as f:\n        plydata = PlyData.read(f)\n        num_verts = plydata['vertex'].count\n        vertices = np.zeros(shape=[num_verts, 9], dtype=np.float32)\n        vertices[:,0] = plydata['vertex'].data['x']\n        vertices[:,1] = plydata['vertex'].data['y']\n        vertices[:,2] = plydata['vertex'].data['z']",
        "detail": "data.scannet.scannet_utils",
        "documentation": {}
    },
    {
        "label": "SCAN2CAD",
        "kind": 5,
        "importPath": "data.ScanCAD2ScanNet",
        "description": "data.ScanCAD2ScanNet",
        "peekOfCode": "SCAN2CAD = json.load(open(os.path.join('.', \"full_annotations.json\")))\nALIGNED_CAD2INST = json.load(open(os.path.join('.', \"aligned_cad2inst_id.json\")))\nprint(\"parsing...\")\nparsed = {}\nfor scan2cad_data in tqdm(SCAN2CAD):\n    scene_id = scan2cad_data[\"id_scan\"]\n    for cad_id, cad_data in enumerate(scan2cad_data[\"aligned_models\"]):\n        try:\n            rotation_quaternion = np.quaternion(*cad_data[\"trs\"][\"rotation\"])\n            rotation_matrix = quaternion.as_rotation_matrix(rotation_quaternion).astype(np.float64)",
        "detail": "data.ScanCAD2ScanNet",
        "documentation": {}
    },
    {
        "label": "ALIGNED_CAD2INST",
        "kind": 5,
        "importPath": "data.ScanCAD2ScanNet",
        "description": "data.ScanCAD2ScanNet",
        "peekOfCode": "ALIGNED_CAD2INST = json.load(open(os.path.join('.', \"aligned_cad2inst_id.json\")))\nprint(\"parsing...\")\nparsed = {}\nfor scan2cad_data in tqdm(SCAN2CAD):\n    scene_id = scan2cad_data[\"id_scan\"]\n    for cad_id, cad_data in enumerate(scan2cad_data[\"aligned_models\"]):\n        try:\n            rotation_quaternion = np.quaternion(*cad_data[\"trs\"][\"rotation\"])\n            rotation_matrix = quaternion.as_rotation_matrix(rotation_quaternion).astype(np.float64)\n            instance_id = ALIGNED_CAD2INST[scene_id][str(cad_id)]",
        "detail": "data.ScanCAD2ScanNet",
        "documentation": {}
    },
    {
        "label": "parsed",
        "kind": 5,
        "importPath": "data.ScanCAD2ScanNet",
        "description": "data.ScanCAD2ScanNet",
        "peekOfCode": "parsed = {}\nfor scan2cad_data in tqdm(SCAN2CAD):\n    scene_id = scan2cad_data[\"id_scan\"]\n    for cad_id, cad_data in enumerate(scan2cad_data[\"aligned_models\"]):\n        try:\n            rotation_quaternion = np.quaternion(*cad_data[\"trs\"][\"rotation\"])\n            rotation_matrix = quaternion.as_rotation_matrix(rotation_quaternion).astype(np.float64)\n            instance_id = ALIGNED_CAD2INST[scene_id][str(cad_id)]\n            if scene_id not in parsed: parsed[scene_id] = {}\n            parsed[scene_id][instance_id] = rotation_matrix.tolist()",
        "detail": "data.ScanCAD2ScanNet",
        "documentation": {}
    },
    {
        "label": "random_point_cloud",
        "kind": 2,
        "importPath": "dataset.data_converter",
        "description": "dataset.data_converter",
        "peekOfCode": "def random_point_cloud(pc, pc_mask, mask_ratio):\n    output_mask = []\n    for i in range(len(pc)):\n        if pc_mask[i] == 0:\n            output_mask.append(0)\n        else:\n            prob = random.random()\n            if prob < mask_ratio:\n                output_mask.append(0)\n            else:",
        "detail": "dataset.data_converter",
        "documentation": {}
    },
    {
        "label": "random_caption_word",
        "kind": 2,
        "importPath": "dataset.data_converter",
        "description": "dataset.data_converter",
        "peekOfCode": "def random_caption_word(tokens, tokens_mask, tokenizer, vocab, mask_ratio):\n    output_label = []\n    output_tokens = tokens.clone()\n    for i, token in enumerate(tokens): # 101 cls 102 sep use them as SOS and EOS token\n        if tokens_mask[i] == 0 or token == 101:\n            output_label.append(-1)\n        elif token == 102:\n            output_tokens[i] = tokenizer.mask_token_id\n            output_label.append(vocab.token_to_id('[EOS]'))\n        else:",
        "detail": "dataset.data_converter",
        "documentation": {}
    },
    {
        "label": "ScanFamilyDatasetWrapper",
        "kind": 6,
        "importPath": "dataset.data_wrapper",
        "description": "dataset.data_wrapper",
        "peekOfCode": "class ScanFamilyDatasetWrapper(torch.utils.data.Dataset):\n    def __init__(self, dataset, tokenizer, max_seq_length=80, max_obj_len=80):\n        self.dataset = dataset\n        self.tokenizer = tokenizer\n        self.max_seq_length = max_seq_length\n        self.max_obj_len = max_obj_len\n    def __len__(self):\n        return len(self.dataset)\n    def pad_tensors(self, tensors, lens=None, pad=0):\n        try:",
        "detail": "dataset.data_wrapper",
        "documentation": {}
    },
    {
        "label": "CaptionDatasetWrapper",
        "kind": 6,
        "importPath": "dataset.data_wrapper",
        "description": "dataset.data_wrapper",
        "peekOfCode": "class CaptionDatasetWrapper(torch.utils.data.Dataset):\n    def __init__(self, dataset, tokenizer, vocab, corpus, max_seq_length=80, max_obj_len=80, txt_mask_ratio=0.15, split='train'):\n        self.dataset = dataset\n        self.tokenizer = tokenizer\n        self.vocab = vocab\n        self.corpus = torch.load(corpus)\n        self.max_seq_length = max_seq_length\n        self.max_obj_len = max_obj_len\n        self.txt_mask_ratio = txt_mask_ratio\n        self.split = split",
        "detail": "dataset.data_wrapper",
        "documentation": {}
    },
    {
        "label": "get_scanrefer_dataset",
        "kind": 2,
        "importPath": "dataset.dataset_factory",
        "description": "dataset.dataset_factory",
        "peekOfCode": "def get_scanrefer_dataset(split='train', **args):\n    dataset = ScanReferDataset(split=split, **args)    \n    return dataset\n@registry.register_dataset(\"scanrefer_task\")\ndef get_scanrefer_task_dataset(split='train', tokenizer=None, txt_seq_length=50, pc_seq_length=80, **args):\n    tokenizer = registry.get_language_model(tokenizer)()\n    dataset = ScanReferDataset(split=split, max_obj_len=pc_seq_length, **args)\n    return ScanFamilyDatasetWrapper(dataset=dataset, tokenizer=tokenizer, max_seq_length=txt_seq_length, max_obj_len=pc_seq_length)\n@registry.register_dataset(\"referit3d\")\ndef get_referit3d_dataset(split='train', **args):",
        "detail": "dataset.dataset_factory",
        "documentation": {}
    },
    {
        "label": "get_scanrefer_task_dataset",
        "kind": 2,
        "importPath": "dataset.dataset_factory",
        "description": "dataset.dataset_factory",
        "peekOfCode": "def get_scanrefer_task_dataset(split='train', tokenizer=None, txt_seq_length=50, pc_seq_length=80, **args):\n    tokenizer = registry.get_language_model(tokenizer)()\n    dataset = ScanReferDataset(split=split, max_obj_len=pc_seq_length, **args)\n    return ScanFamilyDatasetWrapper(dataset=dataset, tokenizer=tokenizer, max_seq_length=txt_seq_length, max_obj_len=pc_seq_length)\n@registry.register_dataset(\"referit3d\")\ndef get_referit3d_dataset(split='train', **args):\n    dataset = Referit3DDataset(split=split, **args)    \n    return dataset\n@registry.register_dataset(\"referit3d_task\")\ndef get_referit3d_task_dataset(split='train', tokenizer=None, txt_seq_length=50, pc_seq_length=80, **args):",
        "detail": "dataset.dataset_factory",
        "documentation": {}
    },
    {
        "label": "get_referit3d_dataset",
        "kind": 2,
        "importPath": "dataset.dataset_factory",
        "description": "dataset.dataset_factory",
        "peekOfCode": "def get_referit3d_dataset(split='train', **args):\n    dataset = Referit3DDataset(split=split, **args)    \n    return dataset\n@registry.register_dataset(\"referit3d_task\")\ndef get_referit3d_task_dataset(split='train', tokenizer=None, txt_seq_length=50, pc_seq_length=80, **args):\n    tokenizer = registry.get_language_model(tokenizer)()\n    dataset = Referit3DDataset(split=split, max_obj_len=pc_seq_length, **args)\n    return ScanFamilyDatasetWrapper(dataset=dataset, tokenizer=tokenizer, max_seq_length=txt_seq_length, max_obj_len=pc_seq_length)\n@registry.register_dataset(\"scanqa\")\ndef get_scanqa_dataset(split='train', **args):",
        "detail": "dataset.dataset_factory",
        "documentation": {}
    },
    {
        "label": "get_referit3d_task_dataset",
        "kind": 2,
        "importPath": "dataset.dataset_factory",
        "description": "dataset.dataset_factory",
        "peekOfCode": "def get_referit3d_task_dataset(split='train', tokenizer=None, txt_seq_length=50, pc_seq_length=80, **args):\n    tokenizer = registry.get_language_model(tokenizer)()\n    dataset = Referit3DDataset(split=split, max_obj_len=pc_seq_length, **args)\n    return ScanFamilyDatasetWrapper(dataset=dataset, tokenizer=tokenizer, max_seq_length=txt_seq_length, max_obj_len=pc_seq_length)\n@registry.register_dataset(\"scanqa\")\ndef get_scanqa_dataset(split='train', **args):\n    dataset = ScanQADataset(split=split, **args)    \n    return dataset\n@registry.register_dataset(\"scanqa_task\")\ndef get_scanqa_task_dataset(split='train', tokenizer=None, txt_seq_length=50, pc_seq_length=80, **args):",
        "detail": "dataset.dataset_factory",
        "documentation": {}
    },
    {
        "label": "get_scanqa_dataset",
        "kind": 2,
        "importPath": "dataset.dataset_factory",
        "description": "dataset.dataset_factory",
        "peekOfCode": "def get_scanqa_dataset(split='train', **args):\n    dataset = ScanQADataset(split=split, **args)    \n    return dataset\n@registry.register_dataset(\"scanqa_task\")\ndef get_scanqa_task_dataset(split='train', tokenizer=None, txt_seq_length=50, pc_seq_length=80, **args):\n    tokenizer = registry.get_language_model(tokenizer)()\n    dataset = ScanQADataset(split=split, max_obj_len=pc_seq_length, **args)\n    return ScanFamilyDatasetWrapper(dataset=dataset, tokenizer=tokenizer, max_seq_length=txt_seq_length, max_obj_len=pc_seq_length)\n@registry.register_dataset(\"sqa\")\ndef get_scanqa_dataset(split='train', **args):",
        "detail": "dataset.dataset_factory",
        "documentation": {}
    },
    {
        "label": "get_scanqa_task_dataset",
        "kind": 2,
        "importPath": "dataset.dataset_factory",
        "description": "dataset.dataset_factory",
        "peekOfCode": "def get_scanqa_task_dataset(split='train', tokenizer=None, txt_seq_length=50, pc_seq_length=80, **args):\n    tokenizer = registry.get_language_model(tokenizer)()\n    dataset = ScanQADataset(split=split, max_obj_len=pc_seq_length, **args)\n    return ScanFamilyDatasetWrapper(dataset=dataset, tokenizer=tokenizer, max_seq_length=txt_seq_length, max_obj_len=pc_seq_length)\n@registry.register_dataset(\"sqa\")\ndef get_scanqa_dataset(split='train', **args):\n    dataset = SQADataset(split=split, **args)    \n    return dataset\n@registry.register_dataset(\"sqa_task\")\ndef get_scanqa_task_dataset(split='train', tokenizer=None, txt_seq_length=50, pc_seq_length=80, **args):",
        "detail": "dataset.dataset_factory",
        "documentation": {}
    },
    {
        "label": "get_scanqa_dataset",
        "kind": 2,
        "importPath": "dataset.dataset_factory",
        "description": "dataset.dataset_factory",
        "peekOfCode": "def get_scanqa_dataset(split='train', **args):\n    dataset = SQADataset(split=split, **args)    \n    return dataset\n@registry.register_dataset(\"sqa_task\")\ndef get_scanqa_task_dataset(split='train', tokenizer=None, txt_seq_length=50, pc_seq_length=80, **args):\n    tokenizer = registry.get_language_model(tokenizer)()\n    dataset = SQADataset(split=split, max_obj_len=pc_seq_length, **args)\n    return ScanFamilyDatasetWrapper(dataset=dataset, tokenizer=tokenizer, max_seq_length=txt_seq_length, max_obj_len=pc_seq_length)\n@registry.register_dataset(\"caption_task\")\ndef get_caption_task_dataset(split='train', tokenizer=None, vocab=None, corpus=None, txt_seq_length=60, pc_seq_length=80, txt_mask_ratio=0.15, **args):",
        "detail": "dataset.dataset_factory",
        "documentation": {}
    },
    {
        "label": "get_scanqa_task_dataset",
        "kind": 2,
        "importPath": "dataset.dataset_factory",
        "description": "dataset.dataset_factory",
        "peekOfCode": "def get_scanqa_task_dataset(split='train', tokenizer=None, txt_seq_length=50, pc_seq_length=80, **args):\n    tokenizer = registry.get_language_model(tokenizer)()\n    dataset = SQADataset(split=split, max_obj_len=pc_seq_length, **args)\n    return ScanFamilyDatasetWrapper(dataset=dataset, tokenizer=tokenizer, max_seq_length=txt_seq_length, max_obj_len=pc_seq_length)\n@registry.register_dataset(\"caption_task\")\ndef get_caption_task_dataset(split='train', tokenizer=None, vocab=None, corpus=None, txt_seq_length=60, pc_seq_length=80, txt_mask_ratio=0.15, **args):\n    if split == 'test':\n        dataset = Scan2CapTestDataset(split=split, max_obj_len=pc_seq_length, **args)\n    else:\n        dataset = Scan2CapDataset(split=split, max_obj_len=pc_seq_length, **args)",
        "detail": "dataset.dataset_factory",
        "documentation": {}
    },
    {
        "label": "get_caption_task_dataset",
        "kind": 2,
        "importPath": "dataset.dataset_factory",
        "description": "dataset.dataset_factory",
        "peekOfCode": "def get_caption_task_dataset(split='train', tokenizer=None, vocab=None, corpus=None, txt_seq_length=60, pc_seq_length=80, txt_mask_ratio=0.15, **args):\n    if split == 'test':\n        dataset = Scan2CapTestDataset(split=split, max_obj_len=pc_seq_length, **args)\n    else:\n        dataset = Scan2CapDataset(split=split, max_obj_len=pc_seq_length, **args)\n    tokenizer = registry.get_language_model(tokenizer)()\n    vocab = registry.get_language_model(\"vocabulary\")(vocab)\n    return CaptionDatasetWrapper(dataset, tokenizer, vocab, corpus, txt_seq_length, pc_seq_length, txt_mask_ratio, split)\n@registry.register_dataset(\"pretrain\")\ndef get_pretrain_dataset(split='train', tokenizer=None, vocab=None, corpus=None,  txt_seq_length=100, pc_seq_length=80, txt_mask_ratio=0.15, **args):",
        "detail": "dataset.dataset_factory",
        "documentation": {}
    },
    {
        "label": "get_pretrain_dataset",
        "kind": 2,
        "importPath": "dataset.dataset_factory",
        "description": "dataset.dataset_factory",
        "peekOfCode": "def get_pretrain_dataset(split='train', tokenizer=None, vocab=None, corpus=None,  txt_seq_length=100, pc_seq_length=80, txt_mask_ratio=0.15, **args):\n    tokenizer = registry.get_language_model(tokenizer)()\n    dataset = SynDataset(split=split, max_obj_len=pc_seq_length, **args)\n    vocab = registry.get_language_model(\"vocabulary\")(vocab)\n    return CaptionDatasetWrapper(dataset, tokenizer, vocab, corpus, txt_seq_length, pc_seq_length, txt_mask_ratio, split)\nif __name__ == '__main__':\n    #dataset = get_scanqa_dataset()\n    pass",
        "detail": "dataset.dataset_factory",
        "documentation": {}
    },
    {
        "label": "LoadScannetMixin",
        "kind": 6,
        "importPath": "dataset.dataset_mixin",
        "description": "dataset.dataset_mixin",
        "peekOfCode": "class LoadScannetMixin(object):\n    def __init__(self):\n        pass\n    def load_scannet(self, scan_ids, pc_type, load_inst_info):\n        scans = {}\n        # attribute\n        # inst_labels, inst_locs, inst_colors, pcds, / pcds_pred, inst_labels_pred\n        for scan_id in scan_ids:\n            # load inst\n            if load_inst_info:",
        "detail": "dataset.dataset_mixin",
        "documentation": {}
    },
    {
        "label": "DataAugmentationMixin",
        "kind": 6,
        "importPath": "dataset.dataset_mixin",
        "description": "dataset.dataset_mixin",
        "peekOfCode": "class DataAugmentationMixin(object):\n    def __init__(self):\n        pass\n    def build_rotate_mat(self):\n        theta_idx = np.random.randint(len(ROTATE_ANGLES))\n        theta = ROTATE_ANGLES[theta_idx]\n        if (theta is not None) and (theta != 0) and (self.split == 'train'):\n            rot_matrix = np.array([\n                [np.cos(theta), -np.sin(theta), 0],\n                [np.sin(theta), np.cos(theta), 0],",
        "detail": "dataset.dataset_mixin",
        "documentation": {}
    },
    {
        "label": "ROTATE_ANGLES",
        "kind": 5,
        "importPath": "dataset.dataset_mixin",
        "description": "dataset.dataset_mixin",
        "peekOfCode": "ROTATE_ANGLES = [0, np.pi/2, np.pi, np.pi*3/2]\nclass LoadScannetMixin(object):\n    def __init__(self):\n        pass\n    def load_scannet(self, scan_ids, pc_type, load_inst_info):\n        scans = {}\n        # attribute\n        # inst_labels, inst_locs, inst_colors, pcds, / pcds_pred, inst_labels_pred\n        for scan_id in scan_ids:\n            # load inst",
        "detail": "dataset.dataset_mixin",
        "documentation": {}
    },
    {
        "label": "SCAN_FAMILY_BASE",
        "kind": 5,
        "importPath": "dataset.path_config",
        "description": "dataset.path_config",
        "peekOfCode": "SCAN_FAMILY_BASE = \"./data/scanfamily\"\n# path generated\nMASK_BASE = os.path.join(SCAN_FAMILY_BASE, \"save_mask\")",
        "detail": "dataset.path_config",
        "documentation": {}
    },
    {
        "label": "MASK_BASE",
        "kind": 5,
        "importPath": "dataset.path_config",
        "description": "dataset.path_config",
        "peekOfCode": "MASK_BASE = os.path.join(SCAN_FAMILY_BASE, \"save_mask\")",
        "detail": "dataset.path_config",
        "documentation": {}
    },
    {
        "label": "Referit3DDataset",
        "kind": 6,
        "importPath": "dataset.referit3d",
        "description": "dataset.referit3d",
        "peekOfCode": "class Referit3DDataset(Dataset, LoadScannetMixin, DataAugmentationMixin):\n    def __init__(self, split='train', anno_type='nr3d', max_obj_len=60, num_points=1024, pc_type='gt', sem_type='607', filter_lang=False, sr3d_plus_aug=False):\n        # make sure all input params is valid\n        # use ground truth for training\n        # test can be both ground truth and non-ground truth\n        assert pc_type in ['gt', 'pred']\n        assert sem_type in ['607']\n        assert split in ['train', 'val', 'test']\n        assert anno_type in ['nr3d', 'sr3d']\n        if split == 'train':",
        "detail": "dataset.referit3d",
        "documentation": {}
    },
    {
        "label": "Scan2CapDataset",
        "kind": 6,
        "importPath": "dataset.scan2cap",
        "description": "dataset.scan2cap",
        "peekOfCode": "class Scan2CapDataset(Dataset, LoadScannetMixin, DataAugmentationMixin):\n    def __init__(self, split='train', max_obj_len=60, num_points=1024, pc_type='gt', sem_type='607', filter_lang=False, iou_threshold=0.5):\n        # make sure all input params is valid\n        # use ground truth for training\n        # test can be both ground truth and non-ground truth\n        assert pc_type in ['gt', 'pred']\n        assert sem_type in ['607']\n        assert split in ['train', 'val', 'test']\n        if split == 'train':\n            pc_type = 'gt'",
        "detail": "dataset.scan2cap",
        "documentation": {}
    },
    {
        "label": "Scan2CapTestDataset",
        "kind": 6,
        "importPath": "dataset.scan2cap",
        "description": "dataset.scan2cap",
        "peekOfCode": "class Scan2CapTestDataset(Dataset, LoadScannetMixin, DataAugmentationMixin):\n    def __init__(self, split='test', max_obj_len=60, num_points=1024, pc_type='pred', sem_type='607', filter_lang=False):\n        # make sure all input params is valid\n        # use ground truth for training\n        # test can be both ground truth and non-ground truth\n        assert pc_type in ['pred']\n        assert sem_type in ['607']\n        assert split in ['test']\n        # load file\n        anno_file = os.path.join(SCAN_FAMILY_BASE, 'annotations/refer/scanrefer.jsonl')",
        "detail": "dataset.scan2cap",
        "documentation": {}
    },
    {
        "label": "prepare_corpus",
        "kind": 2,
        "importPath": "dataset.scan2cap",
        "description": "dataset.scan2cap",
        "peekOfCode": "def prepare_corpus(save_file):\n    anno_file = os.path.join(SCAN_FAMILY_BASE, 'annotations/refer/scanrefer.jsonl')\n    split_file = os.path.join(SCAN_FAMILY_BASE, 'annotations/splits/scannetv2_'+ 'val' + \".txt\")\n    split_scan_ids = set([x.strip() for x in open(split_file, 'r')])\n    data = {}\n    with jsonlines.open(anno_file, 'r') as f:\n        for item in f:\n            if item['scan_id'] in split_scan_ids:\n                scene_id = item['scan_id']\n                object_id = int(item['target_id'])",
        "detail": "dataset.scan2cap",
        "documentation": {}
    },
    {
        "label": "Answer",
        "kind": 6,
        "importPath": "dataset.scanqa",
        "description": "dataset.scanqa",
        "peekOfCode": "class Answer(object):\n    def __init__(self, answers=None, unk_token='<unk>', ignore_idx=-100):\n        if answers is None:\n            answers = []\n        self.unk_token = unk_token\n        self.ignore_idx = ignore_idx\n        self.vocab = {x: i for i, x in enumerate(answers)}\n        self.rev_vocab = dict((v, k) for k, v in self.vocab.items())\n    def itos(self, i):\n        if i == self.ignore_idx:",
        "detail": "dataset.scanqa",
        "documentation": {}
    },
    {
        "label": "ScanQADataset",
        "kind": 6,
        "importPath": "dataset.scanqa",
        "description": "dataset.scanqa",
        "peekOfCode": "class ScanQADataset(Dataset, LoadScannetMixin, DataAugmentationMixin):\n    def __init__(self, split='train', max_obj_len=60, num_points=1024, pc_type='gt', sem_type='607', filter_lang=False, use_unanswer=True, drop_sample=0.0):\n        # make sure all input params is valid\n        assert pc_type in ['gt', 'pred']\n        assert sem_type in ['607']\n        assert split in ['train', 'val', 'test_w_obj', 'test_wo_obj']\n        if split == 'train':\n            pc_type = 'gt'\n        # build answer\n        train_data = json.load(open(os.path.join(SCAN_FAMILY_BASE, 'annotations/qa/ScanQA_v1.0_' + 'train' + \".json\")))",
        "detail": "dataset.scanqa",
        "documentation": {}
    },
    {
        "label": "ScanReferDataset",
        "kind": 6,
        "importPath": "dataset.scanrefer",
        "description": "dataset.scanrefer",
        "peekOfCode": "class ScanReferDataset(Dataset, LoadScannetMixin, DataAugmentationMixin):\n    def __init__(self, split='train', max_obj_len=60, num_points=1024, pc_type='gt', sem_type='607', filter_lang=False, drop_sample=0.0):\n        # make sure all input params is valid\n        # use ground truth for training\n        # test can be both ground truth and non-ground truth\n        assert pc_type in ['gt', 'pred']\n        assert sem_type in ['607']\n        assert split in ['train', 'val', 'test']\n        if split == 'train':\n            pc_type = 'gt'",
        "detail": "dataset.scanrefer",
        "documentation": {}
    },
    {
        "label": "Answer",
        "kind": 6,
        "importPath": "dataset.sqa",
        "description": "dataset.sqa",
        "peekOfCode": "class Answer(object):\n    def __init__(self, answers=None, unk_token='u'):\n        if answers is None:\n            answers = []\n        self.vocab = {x: i for i, x in enumerate(answers)}\n        self.rev_vocab = dict((v, k) for k, v in self.vocab.items())\n        self.unk_token = unk_token\n        self.ignore_idx = self.vocab['u']\n    def itos(self, i):\n        if i == self.ignore_idx:",
        "detail": "dataset.sqa",
        "documentation": {}
    },
    {
        "label": "SQADataset",
        "kind": 6,
        "importPath": "dataset.sqa",
        "description": "dataset.sqa",
        "peekOfCode": "class SQADataset(Dataset, LoadScannetMixin, DataAugmentationMixin):\n    def __init__(self, split='train', max_obj_len=60, num_points=1024, pc_type='gt', sem_type='607', filter_lang=False, use_unanswer=True):\n        # make sure all input params is valid\n        assert pc_type in ['gt', 'pred']\n        assert sem_type in ['607']\n        assert split in ['train', 'val', 'test']\n        if split == 'train':\n            pc_type = 'gt'\n        elif split == 'val':\n            split = 'test'",
        "detail": "dataset.sqa",
        "documentation": {}
    },
    {
        "label": "get_question_type",
        "kind": 2,
        "importPath": "dataset.sqa",
        "description": "dataset.sqa",
        "peekOfCode": "def get_question_type(question):\n    question = question.lstrip()\n    if question[:4].lower() == 'what':\n        return 0\n    elif question[:2].lower() == 'is':\n        return 1\n    elif question[:3].lower() == 'how':\n        return 2\n    elif question[:3].lower() == 'can':\n        return 3",
        "detail": "dataset.sqa",
        "documentation": {}
    },
    {
        "label": "SynDataset",
        "kind": 6,
        "importPath": "dataset.syn",
        "description": "dataset.syn",
        "peekOfCode": "class SynDataset(Dataset, LoadScannetMixin, DataAugmentationMixin):\n    def __init__(self, split='train', max_obj_len=60, num_points=1024, pc_type='gt', sem_type='607', filter_lang=False, iou_threshold=0.5):\n        # make sure all input params is valid\n        # use ground truth for training\n        # test can be both ground truth and non-ground truth\n        assert pc_type in ['gt', 'pred']\n        assert sem_type in ['607']\n        assert split in ['train', 'val', 'test']\n        if split == 'train':\n            pc_type = 'gt'",
        "detail": "dataset.syn",
        "documentation": {}
    },
    {
        "label": "Scan2CapTestDataset",
        "kind": 6,
        "importPath": "dataset.syn",
        "description": "dataset.syn",
        "peekOfCode": "class Scan2CapTestDataset(Dataset, LoadScannetMixin, DataAugmentationMixin):\n    def __init__(self, split='test', max_obj_len=60, num_points=1024, pc_type='pred', sem_type='607', filter_lang=False):\n        # make sure all input params is valid\n        # use ground truth for training\n        # test can be both ground truth and non-ground truth\n        assert pc_type in ['pred']\n        assert sem_type in ['607']\n        assert split in ['test']\n        # load file\n        anno_file = os.path.join(SCAN_FAMILY_BASE, 'annotations/refer/scanrefer.jsonl')",
        "detail": "dataset.syn",
        "documentation": {}
    },
    {
        "label": "prepare_corpus",
        "kind": 2,
        "importPath": "dataset.syn",
        "description": "dataset.syn",
        "peekOfCode": "def prepare_corpus(save_file):\n    anno_file = os.path.join(SCAN_FAMILY_BASE, 'annotations/refer/scanrefer.jsonl')\n    split_file = os.path.join(SCAN_FAMILY_BASE, 'annotations/splits/scannetv2_'+ 'val' + \".txt\")\n    split_scan_ids = set([x.strip() for x in open(split_file, 'r')])\n    data = {}\n    with jsonlines.open(anno_file, 'r') as f:\n        for item in f:\n            if item['scan_id'] in split_scan_ids:\n                scene_id = item['scan_id']\n                object_id = int(item['target_id'])",
        "detail": "dataset.syn",
        "documentation": {}
    },
    {
        "label": "get_bert_tokenizer",
        "kind": 2,
        "importPath": "model.language.lang_encoder",
        "description": "model.language.lang_encoder",
        "peekOfCode": "def get_bert_tokenizer():\n    tokenizer = BertTokenizer.from_pretrained(\n        \"bert-base-uncased\",\n        do_lower_case=True)\n    return tokenizer\n@registry.register_language_model(\"bert_lang_encoder\")\ndef get_bert_lang_encoder(num_hidden_layer=3):\n    txt_bert_config = BertConfig(\n        hidden_size=768,\n        num_hidden_layers=num_hidden_layer,",
        "detail": "model.language.lang_encoder",
        "documentation": {}
    },
    {
        "label": "get_bert_lang_encoder",
        "kind": 2,
        "importPath": "model.language.lang_encoder",
        "description": "model.language.lang_encoder",
        "peekOfCode": "def get_bert_lang_encoder(num_hidden_layer=3):\n    txt_bert_config = BertConfig(\n        hidden_size=768,\n        num_hidden_layers=num_hidden_layer,\n        num_attention_heads=12, type_vocab_size=2\n    )\n    txt_encoder = BertModel.from_pretrained(\n        'bert-base-uncased', config=txt_bert_config\n    )\n    return txt_encoder",
        "detail": "model.language.lang_encoder",
        "documentation": {}
    },
    {
        "label": "__version__",
        "kind": 5,
        "importPath": "model.vision.pointnet2._version",
        "description": "model.vision.pointnet2._version",
        "peekOfCode": "__version__ = \"3.0.0\"",
        "detail": "model.vision.pointnet2._version",
        "documentation": {}
    },
    {
        "label": "_PointnetSAModuleBase",
        "kind": 6,
        "importPath": "model.vision.pointnet2.pointnet2_modules",
        "description": "model.vision.pointnet2.pointnet2_modules",
        "peekOfCode": "class _PointnetSAModuleBase(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.npoint = None\n        self.groupers = None\n        self.mlps = None\n    def forward(self, xyz: torch.Tensor,\n                features: torch.Tensor = None) -> (torch.Tensor, torch.Tensor):\n        r\"\"\"\n        Parameters",
        "detail": "model.vision.pointnet2.pointnet2_modules",
        "documentation": {}
    },
    {
        "label": "PointnetSAModuleMSG",
        "kind": 6,
        "importPath": "model.vision.pointnet2.pointnet2_modules",
        "description": "model.vision.pointnet2.pointnet2_modules",
        "peekOfCode": "class PointnetSAModuleMSG(_PointnetSAModuleBase):\n    r\"\"\"Pointnet set abstrction layer with multiscale grouping\n    Parameters\n    ----------\n    npoint : int\n        Number of features\n    radii : list of float32\n        list of radii to group with\n    nsamples : list of int32\n        Number of samples in each ball query",
        "detail": "model.vision.pointnet2.pointnet2_modules",
        "documentation": {}
    },
    {
        "label": "PointnetSAModule",
        "kind": 6,
        "importPath": "model.vision.pointnet2.pointnet2_modules",
        "description": "model.vision.pointnet2.pointnet2_modules",
        "peekOfCode": "class PointnetSAModule(PointnetSAModuleMSG):\n    r\"\"\"Pointnet set abstrction layer\n    Parameters\n    ----------\n    npoint : int\n        Number of features\n    radius : float\n        Radius of ball\n    nsample : int\n        Number of samples in the ball query",
        "detail": "model.vision.pointnet2.pointnet2_modules",
        "documentation": {}
    },
    {
        "label": "PointnetSAModuleVotes",
        "kind": 6,
        "importPath": "model.vision.pointnet2.pointnet2_modules",
        "description": "model.vision.pointnet2.pointnet2_modules",
        "peekOfCode": "class PointnetSAModuleVotes(nn.Module):\n    ''' Modified based on _PointnetSAModuleBase and PointnetSAModuleMSG\n    with extra support for returning point indices for getting their GT votes '''\n    def __init__(\n            self,\n            *,\n            mlp: List[int],\n            npoint: int = None,\n            radius: float = None,\n            nsample: int = None,",
        "detail": "model.vision.pointnet2.pointnet2_modules",
        "documentation": {}
    },
    {
        "label": "PointnetSAModuleMSGVotes",
        "kind": 6,
        "importPath": "model.vision.pointnet2.pointnet2_modules",
        "description": "model.vision.pointnet2.pointnet2_modules",
        "peekOfCode": "class PointnetSAModuleMSGVotes(nn.Module):\n    ''' Modified based on _PointnetSAModuleBase and PointnetSAModuleMSG\n    with extra support for returning point indices for getting their GT votes '''\n    def __init__(\n            self,\n            *,\n            mlps: List[List[int]],\n            npoint: int,\n            radii: List[float],\n            nsamples: List[int],",
        "detail": "model.vision.pointnet2.pointnet2_modules",
        "documentation": {}
    },
    {
        "label": "PointnetFPModule",
        "kind": 6,
        "importPath": "model.vision.pointnet2.pointnet2_modules",
        "description": "model.vision.pointnet2.pointnet2_modules",
        "peekOfCode": "class PointnetFPModule(nn.Module):\n    r\"\"\"Propigates the features of one set to another\n    Parameters\n    ----------\n    mlp : list\n        Pointnet module parameters\n    bn : bool\n        Use batchnorm\n    \"\"\"\n    def __init__(self, *, mlp: List[int], bn: bool = True):",
        "detail": "model.vision.pointnet2.pointnet2_modules",
        "documentation": {}
    },
    {
        "label": "PointnetLFPModuleMSG",
        "kind": 6,
        "importPath": "model.vision.pointnet2.pointnet2_modules",
        "description": "model.vision.pointnet2.pointnet2_modules",
        "peekOfCode": "class PointnetLFPModuleMSG(nn.Module):\n    ''' Modified based on _PointnetSAModuleBase and PointnetSAModuleMSG\n    learnable feature propagation layer.'''\n    def __init__(\n            self,\n            *,\n            mlps: List[List[int]],\n            radii: List[float],\n            nsamples: List[int],\n            post_mlp: List[int],",
        "detail": "model.vision.pointnet2.pointnet2_modules",
        "documentation": {}
    },
    {
        "label": "BASE_DIR",
        "kind": 5,
        "importPath": "model.vision.pointnet2.pointnet2_modules",
        "description": "model.vision.pointnet2.pointnet2_modules",
        "peekOfCode": "BASE_DIR = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(BASE_DIR)\nimport pointnet2_utils\nimport pytorch_utils as pt_utils\nfrom typing import List\nclass _PointnetSAModuleBase(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.npoint = None\n        self.groupers = None",
        "detail": "model.vision.pointnet2.pointnet2_modules",
        "documentation": {}
    },
    {
        "label": "test_interpolation_grad",
        "kind": 2,
        "importPath": "model.vision.pointnet2.pointnet2_test",
        "description": "model.vision.pointnet2.pointnet2_test",
        "peekOfCode": "def test_interpolation_grad():\n    batch_size = 1\n    feat_dim = 2\n    m = 4\n    feats = torch.randn(batch_size, feat_dim, m, requires_grad=True).float().cuda()\n    def interpolate_func(inputs):\n        idx = torch.from_numpy(np.array([[[0,1,2],[1,2,3]]])).int().cuda()\n        weight = torch.from_numpy(np.array([[[1,1,1],[2,2,2]]])).float().cuda()\n        interpolated_feats = pointnet2_utils.three_interpolate(inputs, idx, weight)\n        return interpolated_feats",
        "detail": "model.vision.pointnet2.pointnet2_test",
        "documentation": {}
    },
    {
        "label": "BASE_DIR",
        "kind": 5,
        "importPath": "model.vision.pointnet2.pointnet2_test",
        "description": "model.vision.pointnet2.pointnet2_test",
        "peekOfCode": "BASE_DIR = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(BASE_DIR)\nimport pointnet2_utils\ndef test_interpolation_grad():\n    batch_size = 1\n    feat_dim = 2\n    m = 4\n    feats = torch.randn(batch_size, feat_dim, m, requires_grad=True).float().cuda()\n    def interpolate_func(inputs):\n        idx = torch.from_numpy(np.array([[[0,1,2],[1,2,3]]])).int().cuda()",
        "detail": "model.vision.pointnet2.pointnet2_test",
        "documentation": {}
    },
    {
        "label": "RandomDropout",
        "kind": 6,
        "importPath": "model.vision.pointnet2.pointnet2_utils",
        "description": "model.vision.pointnet2.pointnet2_utils",
        "peekOfCode": "class RandomDropout(nn.Module):\n    def __init__(self, p=0.5, inplace=False):\n        super(RandomDropout, self).__init__()\n        self.p = p\n        self.inplace = inplace\n    def forward(self, X):\n        theta = torch.Tensor(1).uniform_(0, self.p)[0]\n        return pt_utils.feature_dropout_no_scaling(X, theta, self.train, self.inplace)\nclass FurthestPointSampling(Function):\n    @staticmethod",
        "detail": "model.vision.pointnet2.pointnet2_utils",
        "documentation": {}
    },
    {
        "label": "FurthestPointSampling",
        "kind": 6,
        "importPath": "model.vision.pointnet2.pointnet2_utils",
        "description": "model.vision.pointnet2.pointnet2_utils",
        "peekOfCode": "class FurthestPointSampling(Function):\n    @staticmethod\n    def forward(ctx, xyz, npoint):\n        # type: (Any, torch.Tensor, int) -> torch.Tensor\n        r\"\"\"\n        Uses iterative furthest point sampling to select a set of npoint features that have the largest\n        minimum distance\n        Parameters\n        ----------\n        xyz : torch.Tensor",
        "detail": "model.vision.pointnet2.pointnet2_utils",
        "documentation": {}
    },
    {
        "label": "GatherOperation",
        "kind": 6,
        "importPath": "model.vision.pointnet2.pointnet2_utils",
        "description": "model.vision.pointnet2.pointnet2_utils",
        "peekOfCode": "class GatherOperation(Function):\n    @staticmethod\n    def forward(ctx, features, idx):\n        # type: (Any, torch.Tensor, torch.Tensor) -> torch.Tensor\n        r\"\"\"\n        Parameters\n        ----------\n        features : torch.Tensor\n            (B, C, N) tensor\n        idx : torch.Tensor",
        "detail": "model.vision.pointnet2.pointnet2_utils",
        "documentation": {}
    },
    {
        "label": "ThreeNN",
        "kind": 6,
        "importPath": "model.vision.pointnet2.pointnet2_utils",
        "description": "model.vision.pointnet2.pointnet2_utils",
        "peekOfCode": "class ThreeNN(Function):\n    @staticmethod\n    def forward(ctx, unknown, known):\n        # type: (Any, torch.Tensor, torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]\n        r\"\"\"\n            Find the three nearest neighbors of unknown in known\n        Parameters\n        ----------\n        unknown : torch.Tensor\n            (B, n, 3) tensor of known features",
        "detail": "model.vision.pointnet2.pointnet2_utils",
        "documentation": {}
    },
    {
        "label": "ThreeInterpolate",
        "kind": 6,
        "importPath": "model.vision.pointnet2.pointnet2_utils",
        "description": "model.vision.pointnet2.pointnet2_utils",
        "peekOfCode": "class ThreeInterpolate(Function):\n    @staticmethod\n    def forward(ctx, features, idx, weight):\n        # type(Any, torch.Tensor, torch.Tensor, torch.Tensor) -> Torch.Tensor\n        r\"\"\"\n            Performs weight linear interpolation on 3 features\n        Parameters\n        ----------\n        features : torch.Tensor\n            (B, c, m) Features descriptors to be interpolated from",
        "detail": "model.vision.pointnet2.pointnet2_utils",
        "documentation": {}
    },
    {
        "label": "GroupingOperation",
        "kind": 6,
        "importPath": "model.vision.pointnet2.pointnet2_utils",
        "description": "model.vision.pointnet2.pointnet2_utils",
        "peekOfCode": "class GroupingOperation(Function):\n    @staticmethod\n    def forward(ctx, features, idx):\n        # type: (Any, torch.Tensor, torch.Tensor) -> torch.Tensor\n        r\"\"\"\n        Parameters\n        ----------\n        features : torch.Tensor\n            (B, C, N) tensor of features to group\n        idx : torch.Tensor",
        "detail": "model.vision.pointnet2.pointnet2_utils",
        "documentation": {}
    },
    {
        "label": "BallQuery",
        "kind": 6,
        "importPath": "model.vision.pointnet2.pointnet2_utils",
        "description": "model.vision.pointnet2.pointnet2_utils",
        "peekOfCode": "class BallQuery(Function):\n    @staticmethod\n    def forward(ctx, radius, nsample, xyz, new_xyz):\n        # type: (Any, float, int, torch.Tensor, torch.Tensor) -> torch.Tensor\n        r\"\"\"\n        Parameters\n        ----------\n        radius : float\n            radius of the balls\n        nsample : int",
        "detail": "model.vision.pointnet2.pointnet2_utils",
        "documentation": {}
    },
    {
        "label": "QueryAndGroup",
        "kind": 6,
        "importPath": "model.vision.pointnet2.pointnet2_utils",
        "description": "model.vision.pointnet2.pointnet2_utils",
        "peekOfCode": "class QueryAndGroup(nn.Module):\n    r\"\"\"\n    Groups with a ball query of radius\n    Parameters\n    ---------\n    radius : float32\n        Radius of ball\n    nsample : int32\n        Maximum number of features to gather in the ball\n    \"\"\"",
        "detail": "model.vision.pointnet2.pointnet2_utils",
        "documentation": {}
    },
    {
        "label": "GroupAll",
        "kind": 6,
        "importPath": "model.vision.pointnet2.pointnet2_utils",
        "description": "model.vision.pointnet2.pointnet2_utils",
        "peekOfCode": "class GroupAll(nn.Module):\n    r\"\"\"\n    Groups all features\n    Parameters\n    ---------\n    \"\"\"\n    def __init__(self, use_xyz=True, ret_grouped_xyz=False):\n        # type: (GroupAll, bool) -> None\n        super(GroupAll, self).__init__()\n        self.use_xyz = use_xyz",
        "detail": "model.vision.pointnet2.pointnet2_utils",
        "documentation": {}
    },
    {
        "label": "furthest_point_sample",
        "kind": 5,
        "importPath": "model.vision.pointnet2.pointnet2_utils",
        "description": "model.vision.pointnet2.pointnet2_utils",
        "peekOfCode": "furthest_point_sample = FurthestPointSampling.apply\nclass GatherOperation(Function):\n    @staticmethod\n    def forward(ctx, features, idx):\n        # type: (Any, torch.Tensor, torch.Tensor) -> torch.Tensor\n        r\"\"\"\n        Parameters\n        ----------\n        features : torch.Tensor\n            (B, C, N) tensor",
        "detail": "model.vision.pointnet2.pointnet2_utils",
        "documentation": {}
    },
    {
        "label": "gather_operation",
        "kind": 5,
        "importPath": "model.vision.pointnet2.pointnet2_utils",
        "description": "model.vision.pointnet2.pointnet2_utils",
        "peekOfCode": "gather_operation = GatherOperation.apply\nclass ThreeNN(Function):\n    @staticmethod\n    def forward(ctx, unknown, known):\n        # type: (Any, torch.Tensor, torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]\n        r\"\"\"\n            Find the three nearest neighbors of unknown in known\n        Parameters\n        ----------\n        unknown : torch.Tensor",
        "detail": "model.vision.pointnet2.pointnet2_utils",
        "documentation": {}
    },
    {
        "label": "three_nn",
        "kind": 5,
        "importPath": "model.vision.pointnet2.pointnet2_utils",
        "description": "model.vision.pointnet2.pointnet2_utils",
        "peekOfCode": "three_nn = ThreeNN.apply\nclass ThreeInterpolate(Function):\n    @staticmethod\n    def forward(ctx, features, idx, weight):\n        # type(Any, torch.Tensor, torch.Tensor, torch.Tensor) -> Torch.Tensor\n        r\"\"\"\n            Performs weight linear interpolation on 3 features\n        Parameters\n        ----------\n        features : torch.Tensor",
        "detail": "model.vision.pointnet2.pointnet2_utils",
        "documentation": {}
    },
    {
        "label": "three_interpolate",
        "kind": 5,
        "importPath": "model.vision.pointnet2.pointnet2_utils",
        "description": "model.vision.pointnet2.pointnet2_utils",
        "peekOfCode": "three_interpolate = ThreeInterpolate.apply\nclass GroupingOperation(Function):\n    @staticmethod\n    def forward(ctx, features, idx):\n        # type: (Any, torch.Tensor, torch.Tensor) -> torch.Tensor\n        r\"\"\"\n        Parameters\n        ----------\n        features : torch.Tensor\n            (B, C, N) tensor of features to group",
        "detail": "model.vision.pointnet2.pointnet2_utils",
        "documentation": {}
    },
    {
        "label": "grouping_operation",
        "kind": 5,
        "importPath": "model.vision.pointnet2.pointnet2_utils",
        "description": "model.vision.pointnet2.pointnet2_utils",
        "peekOfCode": "grouping_operation = GroupingOperation.apply\nclass BallQuery(Function):\n    @staticmethod\n    def forward(ctx, radius, nsample, xyz, new_xyz):\n        # type: (Any, float, int, torch.Tensor, torch.Tensor) -> torch.Tensor\n        r\"\"\"\n        Parameters\n        ----------\n        radius : float\n            radius of the balls",
        "detail": "model.vision.pointnet2.pointnet2_utils",
        "documentation": {}
    },
    {
        "label": "ball_query",
        "kind": 5,
        "importPath": "model.vision.pointnet2.pointnet2_utils",
        "description": "model.vision.pointnet2.pointnet2_utils",
        "peekOfCode": "ball_query = BallQuery.apply\nclass QueryAndGroup(nn.Module):\n    r\"\"\"\n    Groups with a ball query of radius\n    Parameters\n    ---------\n    radius : float32\n        Radius of ball\n    nsample : int32\n        Maximum number of features to gather in the ball",
        "detail": "model.vision.pointnet2.pointnet2_utils",
        "documentation": {}
    },
    {
        "label": "SharedMLP",
        "kind": 6,
        "importPath": "model.vision.pointnet2.pytorch_utils",
        "description": "model.vision.pointnet2.pytorch_utils",
        "peekOfCode": "class SharedMLP(nn.Sequential):\n    def __init__(\n            self,\n            args: List[int],\n            *,\n            bn: bool = False,\n            activation=nn.ReLU(inplace=True),\n            preact: bool = False,\n            first: bool = False,\n            name: str = \"\"",
        "detail": "model.vision.pointnet2.pytorch_utils",
        "documentation": {}
    },
    {
        "label": "_BNBase",
        "kind": 6,
        "importPath": "model.vision.pointnet2.pytorch_utils",
        "description": "model.vision.pointnet2.pytorch_utils",
        "peekOfCode": "class _BNBase(nn.Sequential):\n    def __init__(self, in_size, batch_norm=None, name=\"\"):\n        super().__init__()\n        self.add_module(name + \"bn\", batch_norm(in_size))\n        nn.init.constant_(self[0].weight, 1.0)\n        nn.init.constant_(self[0].bias, 0)\nclass BatchNorm1d(_BNBase):\n    def __init__(self, in_size: int, *, name: str = \"\"):\n        super().__init__(in_size, batch_norm=nn.BatchNorm1d, name=name)\nclass BatchNorm2d(_BNBase):",
        "detail": "model.vision.pointnet2.pytorch_utils",
        "documentation": {}
    },
    {
        "label": "BatchNorm1d",
        "kind": 6,
        "importPath": "model.vision.pointnet2.pytorch_utils",
        "description": "model.vision.pointnet2.pytorch_utils",
        "peekOfCode": "class BatchNorm1d(_BNBase):\n    def __init__(self, in_size: int, *, name: str = \"\"):\n        super().__init__(in_size, batch_norm=nn.BatchNorm1d, name=name)\nclass BatchNorm2d(_BNBase):\n    def __init__(self, in_size: int, name: str = \"\"):\n        super().__init__(in_size, batch_norm=nn.BatchNorm2d, name=name)\nclass BatchNorm3d(_BNBase):\n    def __init__(self, in_size: int, name: str = \"\"):\n        super().__init__(in_size, batch_norm=nn.BatchNorm3d, name=name)\nclass _ConvBase(nn.Sequential):",
        "detail": "model.vision.pointnet2.pytorch_utils",
        "documentation": {}
    },
    {
        "label": "BatchNorm2d",
        "kind": 6,
        "importPath": "model.vision.pointnet2.pytorch_utils",
        "description": "model.vision.pointnet2.pytorch_utils",
        "peekOfCode": "class BatchNorm2d(_BNBase):\n    def __init__(self, in_size: int, name: str = \"\"):\n        super().__init__(in_size, batch_norm=nn.BatchNorm2d, name=name)\nclass BatchNorm3d(_BNBase):\n    def __init__(self, in_size: int, name: str = \"\"):\n        super().__init__(in_size, batch_norm=nn.BatchNorm3d, name=name)\nclass _ConvBase(nn.Sequential):\n    def __init__(\n            self,\n            in_size,",
        "detail": "model.vision.pointnet2.pytorch_utils",
        "documentation": {}
    },
    {
        "label": "BatchNorm3d",
        "kind": 6,
        "importPath": "model.vision.pointnet2.pytorch_utils",
        "description": "model.vision.pointnet2.pytorch_utils",
        "peekOfCode": "class BatchNorm3d(_BNBase):\n    def __init__(self, in_size: int, name: str = \"\"):\n        super().__init__(in_size, batch_norm=nn.BatchNorm3d, name=name)\nclass _ConvBase(nn.Sequential):\n    def __init__(\n            self,\n            in_size,\n            out_size,\n            kernel_size,\n            stride,",
        "detail": "model.vision.pointnet2.pytorch_utils",
        "documentation": {}
    },
    {
        "label": "_ConvBase",
        "kind": 6,
        "importPath": "model.vision.pointnet2.pytorch_utils",
        "description": "model.vision.pointnet2.pytorch_utils",
        "peekOfCode": "class _ConvBase(nn.Sequential):\n    def __init__(\n            self,\n            in_size,\n            out_size,\n            kernel_size,\n            stride,\n            padding,\n            activation,\n            bn,",
        "detail": "model.vision.pointnet2.pytorch_utils",
        "documentation": {}
    },
    {
        "label": "Conv1d",
        "kind": 6,
        "importPath": "model.vision.pointnet2.pytorch_utils",
        "description": "model.vision.pointnet2.pytorch_utils",
        "peekOfCode": "class Conv1d(_ConvBase):\n    def __init__(\n            self,\n            in_size: int,\n            out_size: int,\n            *,\n            kernel_size: int = 1,\n            stride: int = 1,\n            padding: int = 0,\n            activation=nn.ReLU(inplace=True),",
        "detail": "model.vision.pointnet2.pytorch_utils",
        "documentation": {}
    },
    {
        "label": "Conv2d",
        "kind": 6,
        "importPath": "model.vision.pointnet2.pytorch_utils",
        "description": "model.vision.pointnet2.pytorch_utils",
        "peekOfCode": "class Conv2d(_ConvBase):\n    def __init__(\n            self,\n            in_size: int,\n            out_size: int,\n            *,\n            kernel_size: Tuple[int, int] = (1, 1),\n            stride: Tuple[int, int] = (1, 1),\n            padding: Tuple[int, int] = (0, 0),\n            activation=nn.ReLU(inplace=True),",
        "detail": "model.vision.pointnet2.pytorch_utils",
        "documentation": {}
    },
    {
        "label": "Conv3d",
        "kind": 6,
        "importPath": "model.vision.pointnet2.pytorch_utils",
        "description": "model.vision.pointnet2.pytorch_utils",
        "peekOfCode": "class Conv3d(_ConvBase):\n    def __init__(\n            self,\n            in_size: int,\n            out_size: int,\n            *,\n            kernel_size: Tuple[int, int, int] = (1, 1, 1),\n            stride: Tuple[int, int, int] = (1, 1, 1),\n            padding: Tuple[int, int, int] = (0, 0, 0),\n            activation=nn.ReLU(inplace=True),",
        "detail": "model.vision.pointnet2.pytorch_utils",
        "documentation": {}
    },
    {
        "label": "FC",
        "kind": 6,
        "importPath": "model.vision.pointnet2.pytorch_utils",
        "description": "model.vision.pointnet2.pytorch_utils",
        "peekOfCode": "class FC(nn.Sequential):\n    def __init__(\n            self,\n            in_size: int,\n            out_size: int,\n            *,\n            activation=nn.ReLU(inplace=True),\n            bn: bool = False,\n            init=None,\n            preact: bool = False,",
        "detail": "model.vision.pointnet2.pytorch_utils",
        "documentation": {}
    },
    {
        "label": "BNMomentumScheduler",
        "kind": 6,
        "importPath": "model.vision.pointnet2.pytorch_utils",
        "description": "model.vision.pointnet2.pytorch_utils",
        "peekOfCode": "class BNMomentumScheduler(object):\n    def __init__(\n            self, model, bn_lambda, last_epoch=-1,\n            setter=set_bn_momentum_default\n    ):\n        if not isinstance(model, nn.Module):\n            raise RuntimeError(\n                \"Class '{}' is not a PyTorch nn Module\".format(\n                    type(model).__name__\n                )",
        "detail": "model.vision.pointnet2.pytorch_utils",
        "documentation": {}
    },
    {
        "label": "set_bn_momentum_default",
        "kind": 2,
        "importPath": "model.vision.pointnet2.pytorch_utils",
        "description": "model.vision.pointnet2.pytorch_utils",
        "peekOfCode": "def set_bn_momentum_default(bn_momentum):\n    def fn(m):\n        if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):\n            m.momentum = bn_momentum\n    return fn\nclass BNMomentumScheduler(object):\n    def __init__(\n            self, model, bn_lambda, last_epoch=-1,\n            setter=set_bn_momentum_default\n    ):",
        "detail": "model.vision.pointnet2.pytorch_utils",
        "documentation": {}
    },
    {
        "label": "_this_dir",
        "kind": 5,
        "importPath": "model.vision.pointnet2.setup",
        "description": "model.vision.pointnet2.setup",
        "peekOfCode": "_this_dir = osp.dirname(osp.abspath(__file__))\n_ext_src_root = \"_ext_src\"\n_ext_sources = glob.glob(\"{}/src/*.cpp\".format(_ext_src_root)) + glob.glob(\n    \"{}/src/*.cu\".format(_ext_src_root)\n)\n_ext_headers = glob.glob(\"{}/include/*\".format(_ext_src_root))\nrequirements = [\"torch>=1.4\"]\nos.environ[\"TORCH_CUDA_ARCH_LIST\"] = \"3.7+PTX;5.0;6.0;6.1;6.2;7.0;7.5\"\nexec(open(\"_version.py\").read())\nsetup(",
        "detail": "model.vision.pointnet2.setup",
        "documentation": {}
    },
    {
        "label": "_ext_src_root",
        "kind": 5,
        "importPath": "model.vision.pointnet2.setup",
        "description": "model.vision.pointnet2.setup",
        "peekOfCode": "_ext_src_root = \"_ext_src\"\n_ext_sources = glob.glob(\"{}/src/*.cpp\".format(_ext_src_root)) + glob.glob(\n    \"{}/src/*.cu\".format(_ext_src_root)\n)\n_ext_headers = glob.glob(\"{}/include/*\".format(_ext_src_root))\nrequirements = [\"torch>=1.4\"]\nos.environ[\"TORCH_CUDA_ARCH_LIST\"] = \"3.7+PTX;5.0;6.0;6.1;6.2;7.0;7.5\"\nexec(open(\"_version.py\").read())\nsetup(\n    name='pointnet2',",
        "detail": "model.vision.pointnet2.setup",
        "documentation": {}
    },
    {
        "label": "_ext_sources",
        "kind": 5,
        "importPath": "model.vision.pointnet2.setup",
        "description": "model.vision.pointnet2.setup",
        "peekOfCode": "_ext_sources = glob.glob(\"{}/src/*.cpp\".format(_ext_src_root)) + glob.glob(\n    \"{}/src/*.cu\".format(_ext_src_root)\n)\n_ext_headers = glob.glob(\"{}/include/*\".format(_ext_src_root))\nrequirements = [\"torch>=1.4\"]\nos.environ[\"TORCH_CUDA_ARCH_LIST\"] = \"3.7+PTX;5.0;6.0;6.1;6.2;7.0;7.5\"\nexec(open(\"_version.py\").read())\nsetup(\n    name='pointnet2',\n    version=__version__,",
        "detail": "model.vision.pointnet2.setup",
        "documentation": {}
    },
    {
        "label": "_ext_headers",
        "kind": 5,
        "importPath": "model.vision.pointnet2.setup",
        "description": "model.vision.pointnet2.setup",
        "peekOfCode": "_ext_headers = glob.glob(\"{}/include/*\".format(_ext_src_root))\nrequirements = [\"torch>=1.4\"]\nos.environ[\"TORCH_CUDA_ARCH_LIST\"] = \"3.7+PTX;5.0;6.0;6.1;6.2;7.0;7.5\"\nexec(open(\"_version.py\").read())\nsetup(\n    name='pointnet2',\n    version=__version__,\n    packages=find_packages(),\n    install_requires=requirements,\n    ext_modules=[",
        "detail": "model.vision.pointnet2.setup",
        "documentation": {}
    },
    {
        "label": "requirements",
        "kind": 5,
        "importPath": "model.vision.pointnet2.setup",
        "description": "model.vision.pointnet2.setup",
        "peekOfCode": "requirements = [\"torch>=1.4\"]\nos.environ[\"TORCH_CUDA_ARCH_LIST\"] = \"3.7+PTX;5.0;6.0;6.1;6.2;7.0;7.5\"\nexec(open(\"_version.py\").read())\nsetup(\n    name='pointnet2',\n    version=__version__,\n    packages=find_packages(),\n    install_requires=requirements,\n    ext_modules=[\n        CUDAExtension(",
        "detail": "model.vision.pointnet2.setup",
        "documentation": {}
    },
    {
        "label": "os.environ[\"TORCH_CUDA_ARCH_LIST\"]",
        "kind": 5,
        "importPath": "model.vision.pointnet2.setup",
        "description": "model.vision.pointnet2.setup",
        "peekOfCode": "os.environ[\"TORCH_CUDA_ARCH_LIST\"] = \"3.7+PTX;5.0;6.0;6.1;6.2;7.0;7.5\"\nexec(open(\"_version.py\").read())\nsetup(\n    name='pointnet2',\n    version=__version__,\n    packages=find_packages(),\n    install_requires=requirements,\n    ext_modules=[\n        CUDAExtension(\n            name='pointnet2._ext',",
        "detail": "model.vision.pointnet2.setup",
        "documentation": {}
    },
    {
        "label": "AllMixup",
        "kind": 6,
        "importPath": "model.vision.basic_modules",
        "description": "model.vision.basic_modules",
        "peekOfCode": "class AllMixup:\n    def __init__(self) -> None:\n        pass\n    def __call__(self, obj_sem_cls_pred, obj_labels, cur_step, total_steps):\n        mixup_sem_cls_pred = torch.zeros_like(obj_sem_cls_pred)\n        for i in range(mixup_sem_cls_pred.shape[0]):\n            for j in range(mixup_sem_cls_pred.shape[1]):\n                if obj_labels[i, j] >= 0:\n                    mixup_sem_cls_pred[i, j, obj_labels[i, j]] = 1.0\n        return mixup_sem_cls_pred",
        "detail": "model.vision.basic_modules",
        "documentation": {}
    },
    {
        "label": "LinearDecayMixup",
        "kind": 6,
        "importPath": "model.vision.basic_modules",
        "description": "model.vision.basic_modules",
        "peekOfCode": "class LinearDecayMixup:\n    def __init__(self, mixup_stage1, mixup_stage2) -> None:\n        self.stage1_rate = mixup_stage1\n        self.stage2_rate = mixup_stage2\n        assert self.stage2_rate > self.stage1_rate\n    def __call__(self, obj_sem_cls_pred, obj_labels, cur_step, total_steps):\n        if cur_step < total_steps * self.stage1_rate:\n            mixup_ratio = 1.0\n        elif cur_step < total_steps * self.stage2_rate:\n            mixup_ratio = (total_steps * self.stage2_rate - cur_step) / ((self.stage2_rate - self.stage1_rate) * total_steps)",
        "detail": "model.vision.basic_modules",
        "documentation": {}
    },
    {
        "label": "get_mlp_head",
        "kind": 2,
        "importPath": "model.vision.basic_modules",
        "description": "model.vision.basic_modules",
        "peekOfCode": "def get_mlp_head(input_size, hidden_size, output_size, dropout=0):\n    return nn.Sequential(\n        nn.Linear(input_size, hidden_size//2),\n        nn.ReLU(),\n        nn.LayerNorm(hidden_size//2, eps=1e-12),\n        nn.Dropout(dropout),\n        nn.Linear(hidden_size//2, output_size)\n        )\ndef _get_activation_fn(activation):\n    \"\"\"Return an activation function given a string\"\"\"",
        "detail": "model.vision.basic_modules",
        "documentation": {}
    },
    {
        "label": "init_weights",
        "kind": 2,
        "importPath": "model.vision.basic_modules",
        "description": "model.vision.basic_modules",
        "peekOfCode": "def init_weights(module):\n    \"\"\"Initialize the weights\"\"\"\n    if isinstance(module, nn.Linear):\n        # Slightly different from the TF version which uses truncated_normal for initialization\n        # cf https://github.com/pytorch/pytorch/pull/5617\n        module.weight.data.normal_(mean=0.0, std=0.02)\n        if module.bias is not None:\n            module.bias.data.zero_()\n    elif isinstance(module, nn.Embedding):\n        module.weight.data.normal_(mean=0.0, std=0.02)",
        "detail": "model.vision.basic_modules",
        "documentation": {}
    },
    {
        "label": "calc_pairwise_locs",
        "kind": 2,
        "importPath": "model.vision.basic_modules",
        "description": "model.vision.basic_modules",
        "peekOfCode": "def calc_pairwise_locs(obj_centers, obj_whls, eps=1e-10, pairwise_rel_type='center', spatial_dist_norm=True, spatial_dim=5):\n    if pairwise_rel_type == 'mlp':\n        obj_locs = torch.cat([obj_centers, obj_whls], 2)\n        pairwise_locs = torch.cat(\n            [einops.repeat(obj_locs, 'b l d -> b l x d', x=obj_locs.size(1)),\n            einops.repeat(obj_locs, 'b l d -> b x l d', x=obj_locs.size(1))],\n            dim=3\n        )\n        return pairwise_locs\n    pairwise_locs = einops.repeat(obj_centers, 'b l d -> b l 1 d') \\",
        "detail": "model.vision.basic_modules",
        "documentation": {}
    },
    {
        "label": "get_mixup_function",
        "kind": 2,
        "importPath": "model.vision.basic_modules",
        "description": "model.vision.basic_modules",
        "peekOfCode": "def get_mixup_function(mixup_strategy, mixup_stage1, mixup_stage2):\n    if mixup_strategy is None:\n        return None\n    assert mixup_strategy in ['linear_decay', 'all_mixup']\n    if mixup_strategy == 'linear_decay':\n        return LinearDecayMixup(mixup_stage1, mixup_stage2)\n    elif mixup_strategy == 'all_mixup':\n        return AllMixup()\nclass AllMixup:\n    def __init__(self) -> None:",
        "detail": "model.vision.basic_modules",
        "documentation": {}
    },
    {
        "label": "generate_causal_mask",
        "kind": 2,
        "importPath": "model.vision.basic_modules",
        "description": "model.vision.basic_modules",
        "peekOfCode": "def generate_causal_mask(length):\n    return (torch.triu(torch.ones(length, length))==1).transpose(0,1)\ndef generate_mm_casual_mask(txt_length, pc_length):\n    txt_mask = torch.cat((generate_causal_mask(txt_length), torch.ones(txt_length, pc_length)==1), dim=1)\n    pc_mask = torch.cat((torch.zeros(pc_length, txt_length), generate_causal_mask(pc_length)), dim=1)\n    return torch.cat((txt_mask, pc_mask), dim=0)",
        "detail": "model.vision.basic_modules",
        "documentation": {}
    },
    {
        "label": "generate_mm_casual_mask",
        "kind": 2,
        "importPath": "model.vision.basic_modules",
        "description": "model.vision.basic_modules",
        "peekOfCode": "def generate_mm_casual_mask(txt_length, pc_length):\n    txt_mask = torch.cat((generate_causal_mask(txt_length), torch.ones(txt_length, pc_length)==1), dim=1)\n    pc_mask = torch.cat((torch.zeros(pc_length, txt_length), generate_causal_mask(pc_length)), dim=1)\n    return torch.cat((txt_mask, pc_mask), dim=0)",
        "detail": "model.vision.basic_modules",
        "documentation": {}
    },
    {
        "label": "BertPredictionHeadTransform",
        "kind": 6,
        "importPath": "model.vision.caption_head",
        "description": "model.vision.caption_head",
        "peekOfCode": "class BertPredictionHeadTransform(nn.Module):\n    def __init__(self, hidden_size, hidden_act='gelu'):\n        super().__init__()\n        self.dense = nn.Linear(hidden_size, hidden_size)\n        self.transform_act_fn = F.gelu\n        self.LayerNorm = nn.LayerNorm(hidden_size)\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.transform_act_fn(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states)",
        "detail": "model.vision.caption_head",
        "documentation": {}
    },
    {
        "label": "BertLMPredictionHead",
        "kind": 6,
        "importPath": "model.vision.caption_head",
        "description": "model.vision.caption_head",
        "peekOfCode": "class BertLMPredictionHead(nn.Module):\n    def __init__(self, hidden_size, vocab_size):\n        super().__init__()\n        self.transform = BertPredictionHeadTransform(hidden_size=hidden_size, hidden_act='gelu')\n        self.decoder = nn.Linear(hidden_size, vocab_size, bias=False)\n        self.bias = nn.Parameter(torch.zeros(vocab_size))\n    def forward(self, hidden_states):\n        hidden_states = self.transform(hidden_states)\n        hidden_states = self.decoder(hidden_states) + self.bias\n        return hidden_states",
        "detail": "model.vision.caption_head",
        "documentation": {}
    },
    {
        "label": "CaptionHeadV1",
        "kind": 6,
        "importPath": "model.vision.caption_head",
        "description": "model.vision.caption_head",
        "peekOfCode": "class CaptionHeadV1(nn.Module):\n    def __init__(self, hidden_size=768, vocab_size=4233):\n        super().__init__()\n        self.lm_pred_head = BertLMPredictionHead(hidden_size, vocab_size)\n    def forward(self, txt_embeds):\n        txt_caption_cls_logits = self.lm_pred_head(txt_embeds)\n        return txt_caption_cls_logits",
        "detail": "model.vision.caption_head",
        "documentation": {}
    },
    {
        "label": "GroundHeadV1",
        "kind": 6,
        "importPath": "model.vision.grounding_head",
        "description": "model.vision.grounding_head",
        "peekOfCode": "class GroundHeadV1(nn.Module):\n    def __init__(self, input_size=768, hidden_size=768, sem_cls_size=607, dropout=0.3):\n        super().__init__()\n        self.og3d_head = get_mlp_head(\n            input_size, hidden_size, \n            1, dropout=dropout\n        )\n        self.txt_clf_head = get_mlp_head(\n            input_size, hidden_size,\n            sem_cls_size, dropout=dropout",
        "detail": "model.vision.grounding_head",
        "documentation": {}
    },
    {
        "label": "PointNetPP",
        "kind": 6,
        "importPath": "model.vision.point_encoder",
        "description": "model.vision.point_encoder",
        "peekOfCode": "class PointNetPP(nn.Module):\n    \"\"\"\n    Pointnet++ encoder.\n    For the hyper parameters please advise the paper (https://arxiv.org/abs/1706.02413)\n    \"\"\"\n    def __init__(self, sa_n_points: list,\n                 sa_n_samples: list,\n                 sa_radii: list,\n                 sa_mlps: list,\n                 bn=True,",
        "detail": "model.vision.point_encoder",
        "documentation": {}
    },
    {
        "label": "PcdObjEncoder",
        "kind": 6,
        "importPath": "model.vision.point_encoder",
        "description": "model.vision.point_encoder",
        "peekOfCode": "class PcdObjEncoder(nn.Module):\n    def __init__(self, path=None, freeze=False):\n        super().__init__()\n        self.pcd_net = PointNetPP(\n            sa_n_points=[32, 16, None],\n            sa_n_samples=[32, 32, None],\n            sa_radii=[0.2, 0.4, None],\n            sa_mlps=[[3, 64, 64, 128], [128, 128, 128, 256], [256, 256, 512, 768]],\n        )\n        self.obj3d_clf_pre_head = get_mlp_head(768, 768, 607, dropout=0.3)",
        "detail": "model.vision.point_encoder",
        "documentation": {}
    },
    {
        "label": "PointTokenizeEncoder",
        "kind": 6,
        "importPath": "model.vision.point_encoder",
        "description": "model.vision.point_encoder",
        "peekOfCode": "class PointTokenizeEncoder(nn.Module):\n    def __init__(self, backbone='pointnet++', hidden_size=768, path=None, freeze_feature=False,\n                num_attention_heads=12, spatial_dim=5, num_layers=4, dim_loc=6, pairwise_rel_type='center',\n                mixup_strategy=None, mixup_stage1=None, mixup_stage2=None):\n        super().__init__()\n        assert backbone in ['pointnet++', 'pointnext']\n        # build backbone\n        if backbone == 'pointnet++':\n            self.point_feature_extractor = PointNetPP(\n                sa_n_points=[32, 16, None],",
        "detail": "model.vision.point_encoder",
        "documentation": {}
    },
    {
        "label": "break_up_pc",
        "kind": 2,
        "importPath": "model.vision.point_encoder",
        "description": "model.vision.point_encoder",
        "peekOfCode": "def break_up_pc(pc: Tensor):\n    \"\"\"\n    Split the pointcloud into xyz positions and features tensors.\n    This method is taken from VoteNet codebase (https://github.com/facebookresearch/votenet)\n    @param pc: pointcloud [N, 3 + C]\n    :return: the xyz tensor and the feature tensor\n    \"\"\"\n    xyz = pc[..., 0:3].contiguous()\n    features = (\n        pc[..., 3:].transpose(1, 2).contiguous()",
        "detail": "model.vision.point_encoder",
        "documentation": {}
    },
    {
        "label": "BertPredictionHeadTransform",
        "kind": 6,
        "importPath": "model.vision.pretrain_head",
        "description": "model.vision.pretrain_head",
        "peekOfCode": "class BertPredictionHeadTransform(nn.Module):\n    def __init__(self, hidden_size, hidden_act='gelu'):\n        super().__init__()\n        self.dense = nn.Linear(hidden_size, hidden_size)\n        self.transform_act_fn = F.gelu\n        self.LayerNorm = nn.LayerNorm(hidden_size)\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.transform_act_fn(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states)",
        "detail": "model.vision.pretrain_head",
        "documentation": {}
    },
    {
        "label": "BertLMPredictionHead",
        "kind": 6,
        "importPath": "model.vision.pretrain_head",
        "description": "model.vision.pretrain_head",
        "peekOfCode": "class BertLMPredictionHead(nn.Module):\n    def __init__(self, hidden_size, vocab_size):\n        super().__init__()\n        self.transform = BertPredictionHeadTransform(hidden_size=hidden_size, hidden_act='gelu')\n        self.decoder = nn.Linear(hidden_size, vocab_size, bias=False)\n        self.bias = nn.Parameter(torch.zeros(vocab_size))\n    def forward(self, hidden_states):\n        hidden_states = self.transform(hidden_states)\n        hidden_states = self.decoder(hidden_states) + self.bias\n        return hidden_states",
        "detail": "model.vision.pretrain_head",
        "documentation": {}
    },
    {
        "label": "PretrainHeadV1",
        "kind": 6,
        "importPath": "model.vision.pretrain_head",
        "description": "model.vision.pretrain_head",
        "peekOfCode": "class PretrainHeadV1(nn.Module):\n    def __init__(self, hidden_size=768, vocab_size=30522):\n        super().__init__()\n        self.lm_pred_head = BertLMPredictionHead(hidden_size, vocab_size)\n        self.contrastive_head = get_mlp_head(hidden_size, hidden_size, 2)\n    def forward(self, txt_embeds):\n        txt_lm_cls_logits = self.lm_pred_head(txt_embeds)\n        scene_txt_match_logit = self.contrastive_head(txt_embeds[:, 0, :])\n        return txt_lm_cls_logits, scene_txt_match_logit",
        "detail": "model.vision.pretrain_head",
        "documentation": {}
    },
    {
        "label": "FC",
        "kind": 6,
        "importPath": "model.vision.qahead",
        "description": "model.vision.qahead",
        "peekOfCode": "class FC(nn.Module):\n    def __init__(self, in_size, out_size, pdrop=0., use_gelu=True):\n        super(FC, self).__init__()\n        self.pdrop = pdrop\n        self.use_gelu = use_gelu\n        self.linear = nn.Linear(in_size, out_size)\n        if use_gelu:\n            #self.relu = nn.Relu(inplace=True)\n            self.gelu = nn.GELU()\n        if pdrop > 0:",
        "detail": "model.vision.qahead",
        "documentation": {}
    },
    {
        "label": "MLP",
        "kind": 6,
        "importPath": "model.vision.qahead",
        "description": "model.vision.qahead",
        "peekOfCode": "class MLP(nn.Module):\n    def __init__(self, in_size, mid_size, out_size, pdrop=0., use_gelu=True):\n        super(MLP, self).__init__()\n        self.fc = FC(in_size, mid_size, pdrop=pdrop, use_gelu=use_gelu)\n        self.linear = nn.Linear(mid_size, out_size)\n    def forward(self, x):\n        return self.linear(self.fc(x))\nclass AttFlat(nn.Module):\n    def __init__(self, hidden_size, flat_mlp_size=512, flat_glimpses=1, flat_out_size=1024, pdrop=0.1):\n        super(AttFlat, self).__init__()",
        "detail": "model.vision.qahead",
        "documentation": {}
    },
    {
        "label": "AttFlat",
        "kind": 6,
        "importPath": "model.vision.qahead",
        "description": "model.vision.qahead",
        "peekOfCode": "class AttFlat(nn.Module):\n    def __init__(self, hidden_size, flat_mlp_size=512, flat_glimpses=1, flat_out_size=1024, pdrop=0.1):\n        super(AttFlat, self).__init__()\n        self.mlp = MLP(\n            in_size=hidden_size,\n            mid_size=flat_mlp_size,\n            out_size=flat_glimpses,\n            pdrop=pdrop,\n            use_gelu=True\n        )",
        "detail": "model.vision.qahead",
        "documentation": {}
    },
    {
        "label": "QAHeadV1",
        "kind": 6,
        "importPath": "model.vision.qahead",
        "description": "model.vision.qahead",
        "peekOfCode": "class QAHeadV1(nn.Module):\n    def __init__(self, hidden_size=768, mlp_size=256, glimpse=1, flat_out_size=512, num_answers=8864):\n        super().__init__()\n        self.attflat_visual = AttFlat(hidden_size, mlp_size, glimpse, flat_out_size, 0.1)\n        self.attflat_lang = AttFlat(hidden_size, mlp_size, glimpse, flat_out_size, 0.1)\n        self.answer_cls = nn.Sequential(\n                nn.Linear(flat_out_size, hidden_size),\n                nn.GELU(),\n                nn.Dropout(0.3),\n                nn.Linear(hidden_size, num_answers)",
        "detail": "model.vision.qahead",
        "documentation": {}
    },
    {
        "label": "TransformerDecoderLayer",
        "kind": 6,
        "importPath": "model.vision.transformers",
        "description": "model.vision.transformers",
        "peekOfCode": "class TransformerDecoderLayer(nn.Module):\n    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n        super().__init__()\n        self.self_attn = nn.MultiheadAttention(\n            d_model, nhead, dropout=dropout, batch_first=True\n        )\n        self.multihead_attn = nn.MultiheadAttention(\n            d_model, nhead, dropout=dropout, batch_first=True\n        )\n        # Implementation of Feedforward model",
        "detail": "model.vision.transformers",
        "documentation": {}
    },
    {
        "label": "TransformerEncoderLayer",
        "kind": 6,
        "importPath": "model.vision.transformers",
        "description": "model.vision.transformers",
        "peekOfCode": "class TransformerEncoderLayer(nn.Module):\n    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n        super().__init__()\n        self.self_attn = nn.MultiheadAttention(\n            d_model, nhead, dropout=dropout, batch_first=True\n        )\n        # Implementation of Feedforward model\n        self.linear1 = nn.Linear(d_model, dim_feedforward)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, d_model)",
        "detail": "model.vision.transformers",
        "documentation": {}
    },
    {
        "label": "MultiHeadAttentionSpatial",
        "kind": 6,
        "importPath": "model.vision.transformers",
        "description": "model.vision.transformers",
        "peekOfCode": "class MultiHeadAttentionSpatial(nn.Module):\n    def __init__(\n        self, d_model, n_head, dropout=0.1, spatial_multihead=True, spatial_dim=5,\n        spatial_attn_fusion='mul',\n    ):\n        super().__init__()\n        assert d_model % n_head == 0, 'd_model: %d, n_head: %d' %(d_model, n_head)\n        self.n_head = n_head\n        self.d_model = d_model\n        self.d_per_head = d_model // n_head",
        "detail": "model.vision.transformers",
        "documentation": {}
    },
    {
        "label": "TransformerSpatialDecoderLayer",
        "kind": 6,
        "importPath": "model.vision.transformers",
        "description": "model.vision.transformers",
        "peekOfCode": "class TransformerSpatialDecoderLayer(TransformerDecoderLayer):\n    def __init__(\n        self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\",\n        spatial_multihead=True, spatial_dim=5, spatial_attn_fusion='mul'\n    ):\n        super().__init__(\n            d_model, nhead, dim_feedforward=dim_feedforward, dropout=dropout, activation=activation\n        )\n        del self.self_attn\n        self.self_attn = MultiHeadAttentionSpatial(",
        "detail": "model.vision.transformers",
        "documentation": {}
    },
    {
        "label": "TransformerSpatialEncoderLayer",
        "kind": 6,
        "importPath": "model.vision.transformers",
        "description": "model.vision.transformers",
        "peekOfCode": "class TransformerSpatialEncoderLayer(TransformerEncoderLayer):\n    def __init__(\n        self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\",\n        spatial_multihead=True, spatial_dim=5, spatial_attn_fusion='mul'\n    ):\n        super().__init__(\n            d_model, nhead, dim_feedforward=dim_feedforward, dropout=dropout, activation=activation\n        )\n        del self.self_attn\n        self.self_attn = MultiHeadAttentionSpatial(",
        "detail": "model.vision.transformers",
        "documentation": {}
    },
    {
        "label": "UnifiedSpatialCrossEncoderV2",
        "kind": 6,
        "importPath": "model.vision.unified_encoder",
        "description": "model.vision.unified_encoder",
        "peekOfCode": "class UnifiedSpatialCrossEncoderV2(nn.Module):\n    \"\"\"\n       spatial_dim: spatial feature dim, used to modify attention\n       dim_loc: \n    \"\"\"\n    def __init__(self, hidden_size=768, num_attention_heads=12, num_layers=4, dim_loc=6):\n        super().__init__()\n        # unfied encoder\n        unified_encoder_layer =  TransformerEncoderLayer(hidden_size, num_attention_heads)\n        self.unified_encoder = _get_clones(unified_encoder_layer, num_layers)",
        "detail": "model.vision.unified_encoder",
        "documentation": {}
    },
    {
        "label": "Pointnet2Backbone",
        "kind": 6,
        "importPath": "models.base_module.backbone_module",
        "description": "models.base_module.backbone_module",
        "peekOfCode": "class Pointnet2Backbone(nn.Module):\n    r\"\"\"\n       Backbone network for point cloud feature learning.\n       Based on Pointnet++ single-scale grouping network. \n       Parameters\n       ----------\n       input_feature_dim: int\n            Number of input channels in the feature descriptor for each point.\n            e.g. 3 for RGB.\n    \"\"\"",
        "detail": "models.base_module.backbone_module",
        "documentation": {}
    },
    {
        "label": "copy_params",
        "kind": 2,
        "importPath": "models.base_module.ema_utils",
        "description": "models.base_module.ema_utils",
        "peekOfCode": "def copy_params(model_pairs):\n    for model_pair in model_pairs:\n        for param, param_m in zip(model_pair[0].parameters(), model_pair[1].parameters()):\n            param_m.data.copy_(param.data)  # initialize\n            param_m.requires_grad = False  # not update by gradient\ndef _momentum_update(model_pairs, momentum=0.995):\n    for model_pair in model_pairs:\n        for param, param_m in zip(model_pair[0].parameters(), model_pair[1].parameters()):\n            param_m.data = param_m.data * momentum + param.data * (1. - momentum)",
        "detail": "models.base_module.ema_utils",
        "documentation": {}
    },
    {
        "label": "LangModule",
        "kind": 6,
        "importPath": "models.base_module.lang_module",
        "description": "models.base_module.lang_module",
        "peekOfCode": "class LangModule(nn.Module):\n    def __init__(self, num_text_classes, use_lang_classifier=True, use_bidir=False,\n                 emb_size=300, hidden_size=256, mask_ratio_l=0.2, qa=False):\n        super().__init__()\n        self.num_text_classes = num_text_classes\n        self.use_lang_classifier = use_lang_classifier\n        self.use_bidir = use_bidir\n        self.gru = nn.GRU(\n            input_size=emb_size,\n            hidden_size=hidden_size,",
        "detail": "models.base_module.lang_module",
        "documentation": {}
    },
    {
        "label": "VotingModule",
        "kind": 6,
        "importPath": "models.base_module.voting_module",
        "description": "models.base_module.voting_module",
        "peekOfCode": "class VotingModule(nn.Module):\n    def __init__(self, vote_factor, seed_feature_dim):\n        \"\"\" Votes generation from seed point features.\n        Args:\n            vote_facotr: int\n                number of votes generated from each seed point\n            seed_feature_dim: int\n                number of channels of seed point features\n            vote_feature_dim: int\n                number of channels of vote features",
        "detail": "models.base_module.voting_module",
        "documentation": {}
    },
    {
        "label": "CapNet",
        "kind": 6,
        "importPath": "models.capnet.capnet",
        "description": "models.capnet.capnet",
        "peekOfCode": "class CapNet(nn.Module):\n    def __init__(self, num_class, vocabulary, embeddings, num_heading_bin, num_size_cluster, mean_size_arr, \n    input_feature_dim=0, num_proposal=256, num_locals=-1, vote_factor=1, sampling=\"vote_fps\",\n    no_caption=False, use_topdown=False, query_mode=\"corner\", \n    graph_mode=\"graph_conv\", num_graph_steps=0, use_relation=False, graph_aggr=\"add\",\n    use_orientation=False, num_bins=6, use_distance=False, use_new=False, \n    emb_size=300, hidden_size=512, dataset_config=None):\n        super().__init__()\n        self.num_class = num_class\n        self.num_heading_bin = num_heading_bin",
        "detail": "models.capnet.capnet",
        "documentation": {}
    },
    {
        "label": "SceneCaptionModule",
        "kind": 6,
        "importPath": "models.capnet.caption_module",
        "description": "models.capnet.caption_module",
        "peekOfCode": "class SceneCaptionModule(nn.Module):\n    def __init__(self, vocabulary, embeddings, emb_size=300, feat_size=128, hidden_size=512, num_proposals=256):\n        super().__init__() \n        raise NotImplementedError()\nclass TopDownSceneCaptionModule(nn.Module):\n    def __init__(self, vocabulary, embeddings, emb_size=300, feat_size=128, hidden_size=512, num_proposals=256,\n        num_locals=-1, query_mode=\"corner\", use_relation=False, use_oracle=False, head=4, depth=2):\n        super().__init__()\n        self.use_box_embedding = True\n        self.use_dist_weight_matrix = True",
        "detail": "models.capnet.caption_module",
        "documentation": {}
    },
    {
        "label": "TopDownSceneCaptionModule",
        "kind": 6,
        "importPath": "models.capnet.caption_module",
        "description": "models.capnet.caption_module",
        "peekOfCode": "class TopDownSceneCaptionModule(nn.Module):\n    def __init__(self, vocabulary, embeddings, emb_size=300, feat_size=128, hidden_size=512, num_proposals=256,\n        num_locals=-1, query_mode=\"corner\", use_relation=False, use_oracle=False, head=4, depth=2):\n        super().__init__()\n        self.use_box_embedding = True\n        self.use_dist_weight_matrix = True\n        self.vocabulary = vocabulary\n        self.embeddings = embeddings\n        self.num_vocabs = len(vocabulary[\"word2idx\"])\n        self.emb_size = emb_size",
        "detail": "models.capnet.caption_module",
        "documentation": {}
    },
    {
        "label": "select_target",
        "kind": 2,
        "importPath": "models.capnet.caption_module",
        "description": "models.capnet.caption_module",
        "peekOfCode": "def select_target(data_dict):\n    # predicted bbox\n    pred_bbox = data_dict[\"pred_bbox_corner\"] # batch_size, num_proposals, 8, 3\n    batch_size, num_proposals, _, _ = pred_bbox.shape\n    # ground truth bbox\n    gt_bbox = data_dict[\"ref_box_corner_label\"].float() # batch_size, 8, 3\n    target_ids = []\n    target_ious = []\n    for i in range(batch_size):\n        # convert the bbox parameters to bbox corners",
        "detail": "models.capnet.caption_module",
        "documentation": {}
    },
    {
        "label": "select_multi_target",
        "kind": 2,
        "importPath": "models.capnet.caption_module",
        "description": "models.capnet.caption_module",
        "peekOfCode": "def select_multi_target(data_dict):\n    # predicted bbox\n    pred_bbox = data_dict[\"pred_bbox_corner\"] # batch_size, num_proposals, 8, 3\n    batch_size, num_proposals, box_size, _ = pred_bbox.shape\n    # ground truth bbox\n    gt_bbox = data_dict[\"ref_box_corner_label_list\"].float() # batch_size, lang_num_max, 8, 3\n    batch_size, len_nun_max, box_size = gt_bbox.shape[:3]\n    # print(\"word_embs\", word_embs[0][0][0][:5], word_embs[0][1][0][:5])\n    gt_bbox = gt_bbox.reshape(batch_size * len_nun_max, box_size, -1) # batch_size * lang_num_max, 8, 3\n    pred_bbox = pred_bbox[:, None, :, :, :].repeat(1, len_nun_max, 1, 1, 1)\\",
        "detail": "models.capnet.caption_module",
        "documentation": {}
    },
    {
        "label": "DC",
        "kind": 5,
        "importPath": "models.capnet.caption_module",
        "description": "models.capnet.caption_module",
        "peekOfCode": "DC = ScannetDatasetConfig()\ndef select_target(data_dict):\n    # predicted bbox\n    pred_bbox = data_dict[\"pred_bbox_corner\"] # batch_size, num_proposals, 8, 3\n    batch_size, num_proposals, _, _ = pred_bbox.shape\n    # ground truth bbox\n    gt_bbox = data_dict[\"ref_box_corner_label\"].float() # batch_size, 8, 3\n    target_ids = []\n    target_ious = []\n    for i in range(batch_size):",
        "detail": "models.capnet.caption_module",
        "documentation": {}
    },
    {
        "label": "MatchModule",
        "kind": 6,
        "importPath": "models.ft_module.fusion_ft",
        "description": "models.ft_module.fusion_ft",
        "peekOfCode": "class MatchModule(nn.Module):\n    def __init__(self, num_proposals=256, lang_size=256, hidden_size=128, lang_num_size=300, det_channel=128, head=4, depth=1):\n        super().__init__()\n        self.num_proposals = num_proposals\n        self.lang_size = lang_size\n        self.hidden_size = hidden_size\n        self.depth = depth\n        self.match = nn.Sequential(\n            nn.Conv1d(hidden_size, hidden_size, 1),\n            nn.BatchNorm1d(hidden_size),",
        "detail": "models.ft_module.fusion_ft",
        "documentation": {}
    },
    {
        "label": "RelationModule",
        "kind": 6,
        "importPath": "models.ft_module.relation_ft",
        "description": "models.ft_module.relation_ft",
        "peekOfCode": "class RelationModule(nn.Module):\n    def __init__(self, num_proposals=256, hidden_size=128, lang_num_size=300, det_channel=128, head=4, depth=2, use_obj_embedding=True):\n        super().__init__()\n        self.use_box_embedding = True\n        self.use_dist_weight_matrix = True\n        # self.use_obj_embedding = False  # 3D ONLY\n        self.use_obj_embedding = use_obj_embedding\n        self.num_proposals = num_proposals\n        self.hidden_size = hidden_size\n        self.depth = depth",
        "detail": "models.ft_module.relation_ft",
        "documentation": {}
    },
    {
        "label": "JointNet",
        "kind": 6,
        "importPath": "models.network.jointnet",
        "description": "models.network.jointnet",
        "peekOfCode": "class JointNet(nn.Module):\n    def __init__(self, num_class, num_heading_bin, num_size_cluster, mean_size_arr, vocabulary, embeddings,\n                 input_feature_dim=0, num_proposal=128, num_locals=-1, vote_factor=1, sampling=\"vote_fps\",\n                 no_caption=False, use_topdown=False, query_mode=\"corner\", num_graph_steps=0, use_relation=False,\n                 use_lang_classifier=True, use_bidir=False, no_reference=False,\n                 emb_size=300, ground_hidden_size=256, caption_hidden_size=512, dataset_config=None):\n        super().__init__()\n        self.num_class = num_class\n        self.num_heading_bin = num_heading_bin\n        self.num_size_cluster = num_size_cluster",
        "detail": "models.network.jointnet",
        "documentation": {}
    },
    {
        "label": "JointNet",
        "kind": 6,
        "importPath": "models.network.network_ft",
        "description": "models.network.network_ft",
        "peekOfCode": "class JointNet(nn.Module):\n    def __init__(self, num_class, num_heading_bin, num_size_cluster, mean_size_arr, vocabulary, embeddings,\n                 input_feature_dim=0, num_proposal=128, num_locals=-1, vote_factor=1, sampling=\"vote_fps\",\n                 no_caption=False, use_topdown=False, query_mode=\"corner\", num_graph_steps=0, use_relation=False,\n                 use_lang_classifier=True, use_bidir=False, no_reference=False,\n                 emb_size=300, ground_hidden_size=256, caption_hidden_size=512, dataset_config=None, use_obj_embedding=True):\n        super().__init__()\n        self.num_class = num_class\n        self.num_heading_bin = num_heading_bin\n        self.num_size_cluster = num_size_cluster",
        "detail": "models.network.network_ft",
        "documentation": {}
    },
    {
        "label": "JointNet",
        "kind": 6,
        "importPath": "models.network.network_ft_qa",
        "description": "models.network.network_ft_qa",
        "peekOfCode": "class JointNet(nn.Module):\n    def __init__(self, num_answers,\n                 # proposal\n                 num_object_class, input_feature_dim,\n                 num_heading_bin, num_size_cluster, mean_size_arr,\n                 num_proposal=256, vote_factor=1, sampling=\"vote_fps\", seed_feat_dim=256, proposal_size=128,\n                 # qa\n                 # answer_cls_loss=\"ce\",\n                 answer_pdrop=0.3,\n                 mcan_num_layers=2,",
        "detail": "models.network.network_ft_qa",
        "documentation": {}
    },
    {
        "label": "JointNet",
        "kind": 6,
        "importPath": "models.network.network_pretrain",
        "description": "models.network.network_pretrain",
        "peekOfCode": "class JointNet(nn.Module):\n    def __init__(self, num_class, num_heading_bin, num_size_cluster, mean_size_arr,\n                 input_feature_dim=0, num_proposal=128, vote_factor=1, sampling=\"vote_fps\",\n                 use_lang_classifier=True, use_bidir=False, emb_size=300, ground_hidden_size=256,\n                 dataset_config=None, mask_ratio=0.75, mask_ratio_l=0.2, recon_points=False):\n        super().__init__()\n        self.num_class = num_class\n        self.num_heading_bin = num_heading_bin\n        self.num_size_cluster = num_size_cluster\n        self.mean_size_arr = mean_size_arr",
        "detail": "models.network.network_pretrain",
        "documentation": {}
    },
    {
        "label": "MatchModule",
        "kind": 6,
        "importPath": "models.pretrain_module.fusion_module",
        "description": "models.pretrain_module.fusion_module",
        "peekOfCode": "class MatchModule(nn.Module):\n    def __init__(self, num_proposals=256, lang_size=256, hidden_size=128, head=4, depth=2, mask_class=18,\n                 vocab_size=3433, recon_points=False):\n        super().__init__()\n        self.num_proposals = num_proposals\n        self.lang_size = lang_size\n        self.hidden_size = hidden_size\n        self.depth = depth\n        self.recon_points = recon_points\n        # proposal-language matching decoder",
        "detail": "models.pretrain_module.fusion_module",
        "documentation": {}
    },
    {
        "label": "build_layers",
        "kind": 2,
        "importPath": "models.pretrain_module.fusion_module",
        "description": "models.pretrain_module.fusion_module",
        "peekOfCode": "def build_layers(hidden_size, out_size):\n    return nn.Sequential(\n        nn.Conv1d(hidden_size, hidden_size, 1),\n        nn.BatchNorm1d(hidden_size),\n        nn.PReLU(),\n        nn.Conv1d(hidden_size, hidden_size, 1),\n        nn.BatchNorm1d(hidden_size),\n        nn.PReLU(),\n        nn.Conv1d(hidden_size, out_size, 1)\n    )",
        "detail": "models.pretrain_module.fusion_module",
        "documentation": {}
    },
    {
        "label": "mhatt",
        "kind": 2,
        "importPath": "models.pretrain_module.fusion_module",
        "description": "models.pretrain_module.fusion_module",
        "peekOfCode": "def mhatt(hidden_size, head, depth):\n    return nn.ModuleList(\n        MultiHeadAttention(d_model=hidden_size, d_k=hidden_size // head, d_v=hidden_size // head, h=head) for i in\n        range(depth)\n    )",
        "detail": "models.pretrain_module.fusion_module",
        "documentation": {}
    },
    {
        "label": "EmbeddingModule",
        "kind": 6,
        "importPath": "models.pretrain_module.mask_utils",
        "description": "models.pretrain_module.mask_utils",
        "peekOfCode": "class EmbeddingModule(nn.Module):\n    def __init__(self, npoint, feat_dim):\n        super().__init__()\n        self.npoint = npoint\n        self.vote_aggregation = PointnetSAModuleVotes(\n            npoint=npoint,\n            radius=0.3,\n            nsample=16,\n            mlp=[feat_dim, 128, 128, 128],\n            use_xyz=True,",
        "detail": "models.pretrain_module.mask_utils",
        "documentation": {}
    },
    {
        "label": "generate_mask",
        "kind": 2,
        "importPath": "models.pretrain_module.mask_utils",
        "description": "models.pretrain_module.mask_utils",
        "peekOfCode": "def generate_mask(device, mask_ratio=0.5, n_sample=256, B=10):\n    overall_mask = np.zeros([B, n_sample])\n    num_mask = int(n_sample * mask_ratio)\n    for i in range(B):\n        mask = np.hstack([\n            np.zeros(n_sample - num_mask),\n            np.ones(num_mask)\n        ])\n        np.random.shuffle(mask)\n        overall_mask[i, :] = mask",
        "detail": "models.pretrain_module.mask_utils",
        "documentation": {}
    },
    {
        "label": "proposal_mask",
        "kind": 2,
        "importPath": "models.pretrain_module.mask_utils",
        "description": "models.pretrain_module.mask_utils",
        "peekOfCode": "def proposal_mask(data_dict):\n    proposal_xyz = data_dict['aggregated_vote_xyz']\n    proposal_features = data_dict['aggregated_vote_features']  # (B, num_proposal, 128)\n    vote_inds = data_dict['aggregated_vote_inds']\n    # data_dict['pred_bbox_corner']\n    mask = data_dict['mask_label']  # (B, num_proposal)\n    B, n_proposal = proposal_xyz.shape[:2]\n    vis_xyz = proposal_xyz[~mask].reshape(B, -1, 3)\n    mask_xyz = proposal_xyz[mask].reshape(B, -1, 3)\n    vis_features = proposal_features[~mask].reshape(B, -1, -128)  #############################",
        "detail": "models.pretrain_module.mask_utils",
        "documentation": {}
    },
    {
        "label": "RelationModule",
        "kind": 6,
        "importPath": "models.pretrain_module.relation_module",
        "description": "models.pretrain_module.relation_module",
        "peekOfCode": "class RelationModule(nn.Module):\n    def __init__(self, num_proposals=256, hidden_size=128, lang_num_size=300, det_channel=128, head=4, depth=2):\n        super().__init__()\n        self.use_box_embedding = True\n        self.use_dist_weight_matrix = True\n        self.use_obj_embedding = True\n        # self.use_obj_embedding = False  # 3D only\n        self.num_proposals = num_proposals\n        self.hidden_size = hidden_size\n        self.depth = depth",
        "detail": "models.pretrain_module.relation_module",
        "documentation": {}
    },
    {
        "label": "ROIGridPooler",
        "kind": 6,
        "importPath": "models.proposal_module.ROI_heads.pooler_interp",
        "description": "models.proposal_module.ROI_heads.pooler_interp",
        "peekOfCode": "class ROIGridPooler(nn.Module):\n    def __init__(\n            self,\n            grid_size,\n            seed_feat_dim,\n            rep_type,\n            ray_density,\n            revisit_method: str,\n            interp_num: int,\n            one_step_type: str,",
        "detail": "models.proposal_module.ROI_heads.pooler_interp",
        "documentation": {}
    },
    {
        "label": "nn_distance",
        "kind": 2,
        "importPath": "models.proposal_module.ROI_heads.pooler_interp",
        "description": "models.proposal_module.ROI_heads.pooler_interp",
        "peekOfCode": "def nn_distance(pc1, pc2):\n    \"\"\"\n    Compute distance\n    Args:\n        pc1: (B, N, C)\n        pc2: (B, M, C)\n    Returns:\n        dist: (B, N, M)\n    \"\"\"\n    N = pc1.shape[1]",
        "detail": "models.proposal_module.ROI_heads.pooler_interp",
        "documentation": {}
    },
    {
        "label": "ray_based_points",
        "kind": 2,
        "importPath": "models.proposal_module.ROI_heads.pooler_interp",
        "description": "models.proposal_module.ROI_heads.pooler_interp",
        "peekOfCode": "def ray_based_points(center, rois, angle, density):\n    batch_size, num_proposal, _ = rois.shape  # (B, N, 6)\n    R = rotz_batch_pytorch(angle).reshape(-1, 3, 3)  # Rotation matrix ~ (B*N, 3, 3)\n    # Convert param pairs (rois, angle) to point locations\n    num_key_points = density * 6\n    back_proj_points = torch.zeros((batch_size, num_proposal, 6, 3)).cuda()  # (B, N, 6, 3)\n    back_proj_points[:, :, 0, 0] = - rois[:, :, 0]  # back\n    back_proj_points[:, :, 1, 1] = - rois[:, :, 1]  # left\n    back_proj_points[:, :, 2, 2] = - rois[:, :, 2]  # down\n    back_proj_points[:, :, 3, 0] = rois[:, :, 3]  # front",
        "detail": "models.proposal_module.ROI_heads.pooler_interp",
        "documentation": {}
    },
    {
        "label": "grid_based_points",
        "kind": 2,
        "importPath": "models.proposal_module.ROI_heads.pooler_interp",
        "description": "models.proposal_module.ROI_heads.pooler_interp",
        "peekOfCode": "def grid_based_points(center, rois, heading_angle, grid_size):\n    B = heading_angle.shape[0]\n    N = heading_angle.shape[1]\n    # Rotation matrix ~ (B*N, 3, 3)\n    R = rotz_batch_pytorch(heading_angle.float()).view(-1, 3, 3)\n    # (B*N, gs**3, 3)\n    local_grid_points = get_dense_grid_points(rois, B*N, grid_size)\n    # (B*N, gs**3, 3) ~ add Rotation\n    local_grid_points = torch.matmul(local_grid_points, R)\n    # (B*N, gs**3, 3)",
        "detail": "models.proposal_module.ROI_heads.pooler_interp",
        "documentation": {}
    },
    {
        "label": "rotz_batch_pytorch",
        "kind": 2,
        "importPath": "models.proposal_module.ROI_heads.pooler_interp",
        "description": "models.proposal_module.ROI_heads.pooler_interp",
        "peekOfCode": "def rotz_batch_pytorch(t):\n    \"\"\"\n    Rotation about z-axis\n    :param t:\n    :return:\n    \"\"\"\n    input_shape = t.shape\n    output = torch.zeros(tuple(list(input_shape) + [3, 3])).cuda()\n    c = torch.cos(t)\n    s = torch.sin(t)",
        "detail": "models.proposal_module.ROI_heads.pooler_interp",
        "documentation": {}
    },
    {
        "label": "get_dense_grid_points",
        "kind": 2,
        "importPath": "models.proposal_module.ROI_heads.pooler_interp",
        "description": "models.proposal_module.ROI_heads.pooler_interp",
        "peekOfCode": "def get_dense_grid_points(rois, batch_size_rcnn, grid_size):\n    \"\"\"\n    :param rois: (B, num_proposal, 6) ~ back/left/down/front/right/up\n    :param batch_size_rcnn: B*num_proposal\n    :param grid_size:\n    :return:\n    \"\"\"\n    faked_features = rois.new_ones((grid_size, grid_size, grid_size))  # alis gs for grid_size\n    dense_idx = faked_features.nonzero()  # (gs**3, 3) [x_idx, y_idx, z_idx]\n    dense_idx = dense_idx.repeat(batch_size_rcnn, 1, 1).float()  # (batch_size_rcnn, gs**3, 3)",
        "detail": "models.proposal_module.ROI_heads.pooler_interp",
        "documentation": {}
    },
    {
        "label": "StandardROIHeads",
        "kind": 6,
        "importPath": "models.proposal_module.ROI_heads.roi_heads",
        "description": "models.proposal_module.ROI_heads.roi_heads",
        "peekOfCode": "class StandardROIHeads(nn.Module):\n    def __init__(\n            self,\n            num_heading_bin,\n            num_class,\n            use_exp=True,\n            use_centerness=False,\n            obj_loss=True,\n            back_trace_type=None,\n            seed_feat_dim=256,",
        "detail": "models.proposal_module.ROI_heads.roi_heads",
        "documentation": {}
    },
    {
        "label": "StandardROIHeads_qa",
        "kind": 6,
        "importPath": "models.proposal_module.ROI_heads.roi_heads",
        "description": "models.proposal_module.ROI_heads.roi_heads",
        "peekOfCode": "class StandardROIHeads_qa(nn.Module):\n    def __init__(\n            self,\n            num_heading_bin,\n            num_class,\n            num_size_cluster,\n            use_exp=True,\n            use_centerness=False,\n            obj_loss=True,\n            back_trace_type=None,",
        "detail": "models.proposal_module.ROI_heads.roi_heads",
        "documentation": {}
    },
    {
        "label": "DETR3D",
        "kind": 6,
        "importPath": "models.proposal_module.detr.detr3d",
        "description": "models.proposal_module.detr.detr3d",
        "peekOfCode": "class DETR3D(nn.Module):  # just as a backbone; encoding afterward\n    \"\"\" This is the DETR module that performs object detection \"\"\"\n    def __init__(self, config_transformer, input_channels, class_output_shape, bbox_output_shape, aux_loss=False):  # new: from config_transformer\n        \"\"\" Initializes the model.\n        Parameters:\n            transformer: torch module of the transformer architecture. See transformer.py\n            input_channels: input channel of point cloud features\n            num_classes: number of object classes\n            num_queries: number of object queries, ie detection slot. This is the maximal number of objects\n                         DETR can detect in a single image. For COCO, we recommend 100 queries.",
        "detail": "models.proposal_module.detr.detr3d",
        "documentation": {}
    },
    {
        "label": "PositionEmbeddingSine3D",
        "kind": 6,
        "importPath": "models.proposal_module.detr.position_encoding",
        "description": "models.proposal_module.detr.position_encoding",
        "peekOfCode": "class PositionEmbeddingSine3D(nn.Module):\n    \"\"\"\n    This is a more standard version of the position embedding, very similar to the one\n    used by the Attention is all you need paper, generalized to work on images.\n    \"\"\"\n    def __init__(self, num_pos_feats=64, temperature=10000, scale=None):\n        super().__init__()\n        self.num_pos_feats = num_pos_feats # TODO concat with input dim\n        self.temperature = temperature\n        if scale is None:",
        "detail": "models.proposal_module.detr.position_encoding",
        "documentation": {}
    },
    {
        "label": "build_position_encoding",
        "kind": 2,
        "importPath": "models.proposal_module.detr.position_encoding",
        "description": "models.proposal_module.detr.position_encoding",
        "peekOfCode": "def build_position_encoding(position_embedding, hidden_dim, input_dim, scale=None):\n    N_steps = hidden_dim // input_dim\n    assert hidden_dim % input_dim == 0, 'position encoding not divisable by input_dim'\n    assert N_steps > 0, 'you should have position encoding'\n    if position_embedding in ('sine'):\n        # TODO find a better way of exposing other arguments\n        position_embedding = PositionEmbeddingSine3D(num_pos_feats=N_steps, scale=scale)  # normalize ??\n    elif position_embedding in ('learned'):\n        # position_embedding = PositionEmbeddingLearned(N_steps)\n        pass",
        "detail": "models.proposal_module.detr.position_encoding",
        "documentation": {}
    },
    {
        "label": "Transformer3D",
        "kind": 6,
        "importPath": "models.proposal_module.detr.transformer3D",
        "description": "models.proposal_module.detr.transformer3D",
        "peekOfCode": "class Transformer3D(nn.Module):\n    def __init__(self, d_model=512, nhead=8, num_encoder_layers=6,\n                 num_decoder_layers=6, dim_feedforward=2048, dropout=0,\n                 activation=\"gelu\", normalize_before=False,\n                 return_intermediate_dec=False, have_encoder=True, have_decoder=True, attention_type='default', deformable_type=None, offset_size=3):\n        super().__init__()\n        self.have_encoder = have_encoder\n        assert not have_encoder\n        self.have_decoder = have_decoder\n        if have_decoder:",
        "detail": "models.proposal_module.detr.transformer3D",
        "documentation": {}
    },
    {
        "label": "TransformerEncoder",
        "kind": 6,
        "importPath": "models.proposal_module.detr.transformer3D",
        "description": "models.proposal_module.detr.transformer3D",
        "peekOfCode": "class TransformerEncoder(nn.Module):\n    def __init__(self, encoder_layer, num_layers, norm=None):\n        super().__init__()\n        self.layers = _get_clones(encoder_layer, num_layers)\n        self.num_layers = num_layers\n        self.norm = norm\n    def forward(self, src,\n                mask: Optional[Tensor] = None,\n                src_key_padding_mask: Optional[Tensor] = None,\n                pos: Optional[Tensor] = None):",
        "detail": "models.proposal_module.detr.transformer3D",
        "documentation": {}
    },
    {
        "label": "TransformerDecoder",
        "kind": 6,
        "importPath": "models.proposal_module.detr.transformer3D",
        "description": "models.proposal_module.detr.transformer3D",
        "peekOfCode": "class TransformerDecoder(nn.Module):\n    def __init__(self, decoder_layer, num_layers, norm=None, return_intermediate=False):\n        super().__init__()\n        self.layers = _get_clones(decoder_layer, num_layers)\n        self.num_layers = num_layers\n        self.norm = norm\n        self.return_intermediate = return_intermediate\n    def forward(self, tgt, memory,\n                tgt_mask: Optional[Tensor] = None,\n                memory_mask: Optional[Tensor] = None,",
        "detail": "models.proposal_module.detr.transformer3D",
        "documentation": {}
    },
    {
        "label": "MultiheadPositionalAttention",
        "kind": 6,
        "importPath": "models.proposal_module.detr.transformer3D",
        "description": "models.proposal_module.detr.transformer3D",
        "peekOfCode": "class MultiheadPositionalAttention(nn.Module):  # nearby points\n    def __init__(self, d_model, nhead, dropout, attn_type='nearby'):  # nearby; interpolation\n        super().__init__()\n        assert attn_type in ['nearby', 'nearby_20','interpolation', 'interpolation_10', 'interpolation_20', 'dist', 'dist_10',\n                             'input', 'multiply', 'multiply_20', 'multiply_all', 'myAdd', 'myAdd_20', 'myAdd_all', 'myAdd_5_faster']\n        self.attn_type = attn_type\n        self.nhead = nhead\n        if 'multiply' in self.attn_type or 'myAdd' in self.attn_type:\n            self.attention = MyMultiHeadAttention(d_model, d_k=d_model//nhead, d_v=d_model//nhead, h=nhead, dropout=dropout)\n        else:",
        "detail": "models.proposal_module.detr.transformer3D",
        "documentation": {}
    },
    {
        "label": "TransformerDecoderLayer",
        "kind": 6,
        "importPath": "models.proposal_module.detr.transformer3D",
        "description": "models.proposal_module.detr.transformer3D",
        "peekOfCode": "class TransformerDecoderLayer(nn.Module):\n    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n                 activation=\"relu\", normalize_before=False, attention_type='default', deformable_type=None, offset_size=3):\n        super().__init__()\n        attn_split = attention_type.split(';')\n        if len(attn_split) == 1:\n            attention_input = 'input'\n        else:\n            attention_input = attn_split[0]\n            print('Attention input type', attention_input)",
        "detail": "models.proposal_module.detr.transformer3D",
        "documentation": {}
    },
    {
        "label": "MLP",
        "kind": 6,
        "importPath": "models.proposal_module.detr.transformer3D",
        "description": "models.proposal_module.detr.transformer3D",
        "peekOfCode": "class MLP(nn.Module):\n    \"\"\" Very simple multi-layer perceptron (also called FFN)\"\"\"\n    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, norm=None):\n        super().__init__()\n        self.num_layers = num_layers\n        h = [hidden_dim] * (num_layers - 1)\n        self.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))\n        if norm is not None:\n            print('Using Norm << MLP', flush=True)\n            self.norm = nn.ModuleList(norm(hidden_dim) for i in range(num_layers-1))",
        "detail": "models.proposal_module.detr.transformer3D",
        "documentation": {}
    },
    {
        "label": "decode_scores_boxes",
        "kind": 2,
        "importPath": "models.proposal_module.detr.transformer3D",
        "description": "models.proposal_module.detr.transformer3D",
        "peekOfCode": "def decode_scores_boxes(output_dict, end_points, num_heading_bin, num_size_cluster, mean_size_arr, center_with_bias=False, quality_channel=False):\n    pred_boxes = output_dict['pred_boxes']\n    batch_size = pred_boxes.shape[0]\n    num_proposal = pred_boxes.shape[1]\n    bbox_args_shape = 3+num_heading_bin*2+num_size_cluster*4\n    if quality_channel:\n        bbox_args_shape += 1\n    assert pred_boxes.shape[-1] == bbox_args_shape, 'pred_boxes.shape wrong'\n    if center_with_bias:\n        # print('CENTER ADDING VOTE-XYZ', flush=True)",
        "detail": "models.proposal_module.detr.transformer3D",
        "documentation": {}
    },
    {
        "label": "attn_with_batch_mask",
        "kind": 2,
        "importPath": "models.proposal_module.detr.transformer3D",
        "description": "models.proposal_module.detr.transformer3D",
        "peekOfCode": "def attn_with_batch_mask(layer_attn, q, k, src, src_mask, src_key_padding_mask):\n    bs, src_arr, attn_arr = q.shape[1], [], []\n    for i in range(bs):\n        key_mask, attn_mask = None, None\n        if src_key_padding_mask is not None:\n            key_mask = src_key_padding_mask[i:i+1]\n        if src_mask is not None:\n            attn_mask = src_mask[i]\n        batch_attn = layer_attn(q[:, i:i+1, :], k[:, i:i+1, :], value=src[:, i:i+1, :], attn_mask=attn_mask,\n                                key_padding_mask=key_mask)",
        "detail": "models.proposal_module.detr.transformer3D",
        "documentation": {}
    },
    {
        "label": "build_transformer",
        "kind": 2,
        "importPath": "models.proposal_module.detr.transformer3D",
        "description": "models.proposal_module.detr.transformer3D",
        "peekOfCode": "def build_transformer(args):\n    transformer_type = args.get('transformer_type', 'enc_dec')\n    print('[build transformer] Using transformer type', transformer_type)\n    print(args, '<< transformer config')\n    if transformer_type == 'enc_dec':\n        return Transformer3D(\n            d_model=args.hidden_dim,\n            dropout=args.dropout,\n            nhead=args.nheads,\n            dim_feedforward=args.dim_feedforward,",
        "detail": "models.proposal_module.detr.transformer3D",
        "documentation": {}
    },
    {
        "label": "gelu",
        "kind": 2,
        "importPath": "models.proposal_module.detr.transformer3D",
        "description": "models.proposal_module.detr.transformer3D",
        "peekOfCode": "def gelu(x):\n    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\ndef _get_activation_fn(activation):\n    \"\"\"Return an activation function given a string\"\"\"\n    print(activation, '<< transformer activation', flush=True)  # TODO REMOVE IT\n    if activation == \"relu\":\n        return F.relu\n    if activation == \"gelu\":\n        return F.gelu\n        return gelu",
        "detail": "models.proposal_module.detr.transformer3D",
        "documentation": {}
    },
    {
        "label": "mark",
        "kind": 5,
        "importPath": "models.proposal_module.detr.transformer3D",
        "description": "models.proposal_module.detr.transformer3D",
        "peekOfCode": "mark = False\nclass TransformerDecoderLayer(nn.Module):\n    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n                 activation=\"relu\", normalize_before=False, attention_type='default', deformable_type=None, offset_size=3):\n        super().__init__()\n        attn_split = attention_type.split(';')\n        if len(attn_split) == 1:\n            attention_input = 'input'\n        else:\n            attention_input = attn_split[0]",
        "detail": "models.proposal_module.detr.transformer3D",
        "documentation": {}
    },
    {
        "label": "ProposalModule",
        "kind": 6,
        "importPath": "models.proposal_module.proposal_module_detr",
        "description": "models.proposal_module.proposal_module_detr",
        "peekOfCode": "class ProposalModule(nn.Module):\n    def __init__(self, num_class, num_heading_bin, num_size_cluster, mean_size_arr, num_proposal, sampling,\n                 seed_feat_dim=256, config_transformer=None, quality_channel=False, dataset_config=None):\n        super().__init__()\n        if config_transformer is None:\n            raise NotImplementedError('You should input a config')\n            config_transformer = {\n                'mask': 'near_5',\n                'weighted_input': True,\n                'transformer_type': 'deformable',",
        "detail": "models.proposal_module.proposal_module_detr",
        "documentation": {}
    },
    {
        "label": "decode_scores_classes",
        "kind": 2,
        "importPath": "models.proposal_module.proposal_module_detr",
        "description": "models.proposal_module.proposal_module_detr",
        "peekOfCode": "def decode_scores_classes(output_dict, end_points, num_class):\n    pred_logits = output_dict['pred_logits']\n    assert pred_logits.shape[-1] == 2+num_class, 'pred_logits.shape wrong'\n    objectness_scores = pred_logits[:,:,0:2]  # TODO CHANGE IT; JUST SOFTMAXd\n    end_points['objectness_scores'] = objectness_scores\n    sem_cls_scores = pred_logits[:,:,2:2+num_class] # Bxnum_proposalx10\n    end_points['sem_cls_scores'] = sem_cls_scores\n    return end_points\ndef decode_dataset_config(data_dict, dataset_config):\n    if dataset_config is not None:",
        "detail": "models.proposal_module.proposal_module_detr",
        "documentation": {}
    },
    {
        "label": "decode_dataset_config",
        "kind": 2,
        "importPath": "models.proposal_module.proposal_module_detr",
        "description": "models.proposal_module.proposal_module_detr",
        "peekOfCode": "def decode_dataset_config(data_dict, dataset_config):\n    if dataset_config is not None:\n        # print('decode_dataset_config', flush=True)\n        pred_center = data_dict['center'].detach().cpu().numpy()  # (B,K,3)\n        pred_heading_class = torch.argmax(data_dict['heading_scores'], -1)  # B,num_proposal\n        pred_heading_residual = torch.gather(data_dict['heading_residuals'], 2,\n                                             pred_heading_class.unsqueeze(-1))  # B,num_proposal,1\n        pred_heading_class = pred_heading_class.detach().cpu().numpy()  # B,num_proposal\n        pred_heading_residual = pred_heading_residual.squeeze(2).detach().cpu().numpy()  # B,num_proposal\n        pred_size_class = torch.argmax(data_dict['size_scores'], -1)  # B,num_proposal",
        "detail": "models.proposal_module.proposal_module_detr",
        "documentation": {}
    },
    {
        "label": "decode_scores",
        "kind": 2,
        "importPath": "models.proposal_module.proposal_module_detr",
        "description": "models.proposal_module.proposal_module_detr",
        "peekOfCode": "def decode_scores(output_dict, end_points,  num_class, num_heading_bin, num_size_cluster, mean_size_arr, center_with_bias=False, quality_channel=False, dataset_config=None):\n    end_points = decode_scores_classes(output_dict, end_points, num_class)\n    end_points = decode_scores_boxes(output_dict, end_points, num_heading_bin, num_size_cluster, mean_size_arr, center_with_bias, quality_channel)\n    end_points = decode_dataset_config(end_points, dataset_config)\n    return end_points\nclass ProposalModule(nn.Module):\n    def __init__(self, num_class, num_heading_bin, num_size_cluster, mean_size_arr, num_proposal, sampling,\n                 seed_feat_dim=256, config_transformer=None, quality_channel=False, dataset_config=None):\n        super().__init__()\n        if config_transformer is None:",
        "detail": "models.proposal_module.proposal_module_detr",
        "documentation": {}
    },
    {
        "label": "ProposalModule",
        "kind": 6,
        "importPath": "models.proposal_module.proposal_module_fcos",
        "description": "models.proposal_module.proposal_module_fcos",
        "peekOfCode": "class ProposalModule(nn.Module):\n    def __init__(self, num_class, num_heading_bin, num_size_cluster, mean_size_arr, num_proposal, sampling,\n                 seed_feat_dim=256, qa=False):\n        super().__init__()\n        self.num_class = num_class\n        self.num_heading_bin = num_heading_bin\n        self.num_size_cluster = num_size_cluster\n        self.mean_size_arr = mean_size_arr\n        self.num_proposal = num_proposal\n        self.sampling = sampling",
        "detail": "models.proposal_module.proposal_module_fcos",
        "documentation": {}
    },
    {
        "label": "RelationModule",
        "kind": 6,
        "importPath": "models.proposal_module.relation_module",
        "description": "models.proposal_module.relation_module",
        "peekOfCode": "class RelationModule(nn.Module):\n    def __init__(self, num_proposals=256, hidden_size=128, lang_num_size=300, det_channel=128, head=4, depth=2):\n        super().__init__()\n        self.use_box_embedding = True\n        self.use_dist_weight_matrix = True\n        # self.use_obj_embedding = True\n        self.use_obj_embedding = False  # 3D only\n        self.num_proposals = num_proposals\n        self.hidden_size = hidden_size\n        self.depth = depth",
        "detail": "models.proposal_module.relation_module",
        "documentation": {}
    },
    {
        "label": "FC",
        "kind": 6,
        "importPath": "models.qa_module.mcan_module",
        "description": "models.qa_module.mcan_module",
        "peekOfCode": "class FC(nn.Module):\n    def __init__(self, in_size, out_size, pdrop=0., use_gelu=True):\n        super(FC, self).__init__()\n        self.pdrop = pdrop\n        self.use_gelu = use_gelu\n        self.linear = nn.Linear(in_size, out_size)\n        if use_gelu:\n            #self.relu = nn.Relu(inplace=True)\n            self.gelu = nn.GELU()\n        if pdrop > 0:",
        "detail": "models.qa_module.mcan_module",
        "documentation": {}
    },
    {
        "label": "MLP",
        "kind": 6,
        "importPath": "models.qa_module.mcan_module",
        "description": "models.qa_module.mcan_module",
        "peekOfCode": "class MLP(nn.Module):\n    def __init__(self, in_size, mid_size, out_size, pdrop=0., use_gelu=True):\n        super(MLP, self).__init__()\n        self.fc = FC(in_size, mid_size, pdrop=pdrop, use_gelu=use_gelu)\n        self.linear = nn.Linear(mid_size, out_size)\n    def forward(self, x):\n        return self.linear(self.fc(x))\nclass LayerNorm(nn.Module):\n    def __init__(self, size, eps=1e-6):\n        super(LayerNorm, self).__init__()",
        "detail": "models.qa_module.mcan_module",
        "documentation": {}
    },
    {
        "label": "LayerNorm",
        "kind": 6,
        "importPath": "models.qa_module.mcan_module",
        "description": "models.qa_module.mcan_module",
        "peekOfCode": "class LayerNorm(nn.Module):\n    def __init__(self, size, eps=1e-6):\n        super(LayerNorm, self).__init__()\n        self.eps = eps\n        self.a_2 = nn.Parameter(torch.ones(size))\n        self.b_2 = nn.Parameter(torch.zeros(size))\n    def forward(self, x):\n        mean = x.mean(-1, keepdim=True)\n        std = x.std(-1, keepdim=True)\n        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2",
        "detail": "models.qa_module.mcan_module",
        "documentation": {}
    },
    {
        "label": "AttFlat",
        "kind": 6,
        "importPath": "models.qa_module.mcan_module",
        "description": "models.qa_module.mcan_module",
        "peekOfCode": "class AttFlat(nn.Module):\n    def __init__(self, hidden_size, flat_mlp_size=512, flat_glimpses=1, flat_out_size=1024, pdrop=0.1):\n        super(AttFlat, self).__init__()\n        self.mlp = MLP(\n            in_size=hidden_size,\n            mid_size=flat_mlp_size,\n            out_size=flat_glimpses,\n            pdrop=pdrop,\n            use_gelu=True\n        )",
        "detail": "models.qa_module.mcan_module",
        "documentation": {}
    },
    {
        "label": "MHAtt",
        "kind": 6,
        "importPath": "models.qa_module.mcan_module",
        "description": "models.qa_module.mcan_module",
        "peekOfCode": "class MHAtt(nn.Module):\n    def __init__(self, hidden_size, num_heads=8, pdrop=0.1):\n        super(MHAtt, self).__init__()\n        self.linear_v = nn.Linear(hidden_size, hidden_size)\n        self.linear_k = nn.Linear(hidden_size, hidden_size)\n        self.linear_q = nn.Linear(hidden_size, hidden_size)\n        self.linear_merge = nn.Linear(hidden_size, hidden_size)\n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n        self.head_hidden_size = int(hidden_size / num_heads)",
        "detail": "models.qa_module.mcan_module",
        "documentation": {}
    },
    {
        "label": "FFN",
        "kind": 6,
        "importPath": "models.qa_module.mcan_module",
        "description": "models.qa_module.mcan_module",
        "peekOfCode": "class FFN(nn.Module):\n    def __init__(self, hidden_size, pdrop=0.1):\n        super(FFN, self).__init__()\n        ff_size = int(hidden_size * 4)\n        self.mlp = MLP(\n            in_size=hidden_size,\n            mid_size=ff_size,\n            out_size=hidden_size,\n            pdrop=pdrop,\n            use_gelu=True",
        "detail": "models.qa_module.mcan_module",
        "documentation": {}
    },
    {
        "label": "SA",
        "kind": 6,
        "importPath": "models.qa_module.mcan_module",
        "description": "models.qa_module.mcan_module",
        "peekOfCode": "class SA(nn.Module):\n    def __init__(self, hidden_size, num_heads=8, pdrop=0.1):\n        super(SA, self).__init__()\n        self.mhatt = MHAtt(hidden_size, num_heads, pdrop)\n        self.ffn = FFN(hidden_size, pdrop)\n        self.dropout1 = nn.Dropout(pdrop)\n        self.norm1 = LayerNorm(hidden_size)\n        self.dropout2 = nn.Dropout(pdrop)\n        self.norm2 = LayerNorm(hidden_size)\n    def forward(self, x, x_mask):",
        "detail": "models.qa_module.mcan_module",
        "documentation": {}
    },
    {
        "label": "SGA",
        "kind": 6,
        "importPath": "models.qa_module.mcan_module",
        "description": "models.qa_module.mcan_module",
        "peekOfCode": "class SGA(nn.Module):\n    def __init__(self, hidden_size, num_heads=8, pdrop=0.1):\n        super(SGA, self).__init__()\n        self.mhatt1 = MHAtt(hidden_size, num_heads, pdrop)\n        self.mhatt2 = MHAtt(hidden_size, num_heads, pdrop)\n        self.ffn = FFN(hidden_size, pdrop)\n        self.dropout1 = nn.Dropout(pdrop)\n        self.norm1 = LayerNorm(hidden_size)\n        self.dropout2 = nn.Dropout(pdrop)\n        self.norm2 = LayerNorm(hidden_size)",
        "detail": "models.qa_module.mcan_module",
        "documentation": {}
    },
    {
        "label": "MCAN_ED",
        "kind": 6,
        "importPath": "models.qa_module.mcan_module",
        "description": "models.qa_module.mcan_module",
        "peekOfCode": "class MCAN_ED(nn.Module):\n    def __init__(self, hidden_size, num_heads=8, num_layers=6, pdrop=0.1, num_layers_dec=6):\n        super(MCAN_ED, self).__init__()\n        self.enc_list = nn.ModuleList([SA(hidden_size, num_heads, pdrop) for _ in range(num_layers)])\n        self.dec_list = nn.ModuleList([SGA(hidden_size, num_heads, pdrop) for _ in range(num_layers_dec)])\n    def forward(self, x, y, x_mask, y_mask):\n        # Get hidden vector\n        for enc in self.enc_list:\n            x = enc(x, x_mask)\n        for dec in self.dec_list:",
        "detail": "models.qa_module.mcan_module",
        "documentation": {}
    },
    {
        "label": "MatchModule",
        "kind": 6,
        "importPath": "models.refnet.match_module",
        "description": "models.refnet.match_module",
        "peekOfCode": "class MatchModule(nn.Module):\n    def __init__(self, num_proposals=256, lang_size=256, hidden_size=128, lang_num_size=300, det_channel=128, head=4):\n        super().__init__()\n        self.num_proposals = num_proposals\n        self.lang_size = lang_size\n        self.hidden_size = hidden_size\n        self.match = nn.Sequential(\n            nn.Conv1d(hidden_size, hidden_size, 1),\n            nn.BatchNorm1d(hidden_size),\n            nn.PReLU(),",
        "detail": "models.refnet.match_module",
        "documentation": {}
    },
    {
        "label": "RefNet",
        "kind": 6,
        "importPath": "models.refnet.refnet",
        "description": "models.refnet.refnet",
        "peekOfCode": "class RefNet(nn.Module):\n    def __init__(self, num_class, num_heading_bin, num_size_cluster, mean_size_arr,\n                 input_feature_dim=0, num_proposal=128, vote_factor=1, sampling=\"vote_fps\",\n                 use_lang_classifier=True, use_bidir=False, no_reference=False,\n                 emb_size=300, hidden_size=256, dataset_config=None):\n        super().__init__()\n        self.num_class = num_class\n        self.num_heading_bin = num_heading_bin\n        self.num_size_cluster = num_size_cluster\n        self.mean_size_arr = mean_size_arr",
        "detail": "models.refnet.refnet",
        "documentation": {}
    },
    {
        "label": "ScaledDotProductAttention",
        "kind": 6,
        "importPath": "models.transformer.attention",
        "description": "models.transformer.attention",
        "peekOfCode": "class ScaledDotProductAttention(nn.Module):\n    '''\n    Scaled dot-product attention\n    '''\n    def __init__(self, d_model, d_k, d_v, h):\n        '''\n        :param d_model: Output dimensionality of the model\n        :param d_k: Dimensionality of queries and keys\n        :param d_v: Dimensionality of values\n        :param h: Number of heads",
        "detail": "models.transformer.attention",
        "documentation": {}
    },
    {
        "label": "MultiHeadAttention",
        "kind": 6,
        "importPath": "models.transformer.attention",
        "description": "models.transformer.attention",
        "peekOfCode": "class MultiHeadAttention(nn.Module):\n    '''\n    Multi-head attention layer with Dropout and Layer Normalization.\n    '''\n    def __init__(self, d_model, d_k, d_v, h, dropout=.1, identity_map_reordering=False, can_be_stateful=False,\n                 attention_module=None, attention_module_kwargs=None):\n        super(MultiHeadAttention, self).__init__()\n        self.identity_map_reordering = identity_map_reordering\n        if attention_module is not None:\n            if attention_module_kwargs is not None:",
        "detail": "models.transformer.attention",
        "documentation": {}
    },
    {
        "label": "PositionWiseFeedForward",
        "kind": 6,
        "importPath": "models.transformer.utils",
        "description": "models.transformer.utils",
        "peekOfCode": "class PositionWiseFeedForward(nn.Module):\n    '''\n    Position-wise feed forward layer\n    '''\n    def __init__(self, d_model=512, d_ff=2048, dropout=.1, identity_map_reordering=False):\n        super(PositionWiseFeedForward, self).__init__()\n        self.identity_map_reordering = identity_map_reordering\n        self.fc1 = nn.Linear(d_model, d_ff)\n        self.fc2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(p=dropout)",
        "detail": "models.transformer.utils",
        "documentation": {}
    },
    {
        "label": "position_embedding",
        "kind": 2,
        "importPath": "models.transformer.utils",
        "description": "models.transformer.utils",
        "peekOfCode": "def position_embedding(input, d_model):\n    input = input.view(-1, 1)\n    dim = torch.arange(d_model // 2, dtype=torch.float32, device=input.device).view(1, -1)\n    sin = torch.sin(input / 10000 ** (2 * dim / d_model))\n    cos = torch.cos(input / 10000 ** (2 * dim / d_model))\n    out = torch.zeros((input.shape[0], d_model), device=input.device)\n    out[:, ::2] = sin\n    out[:, 1::2] = cos\n    return out\ndef sinusoid_encoding_table(max_len, d_model, padding_idx=None):",
        "detail": "models.transformer.utils",
        "documentation": {}
    },
    {
        "label": "sinusoid_encoding_table",
        "kind": 2,
        "importPath": "models.transformer.utils",
        "description": "models.transformer.utils",
        "peekOfCode": "def sinusoid_encoding_table(max_len, d_model, padding_idx=None):\n    pos = torch.arange(max_len, dtype=torch.float32)\n    out = position_embedding(pos, d_model)\n    if padding_idx is not None:\n        out[padding_idx] = 0\n    return out\nclass PositionWiseFeedForward(nn.Module):\n    '''\n    Position-wise feed forward layer\n    '''",
        "detail": "models.transformer.utils",
        "documentation": {}
    },
    {
        "label": "get_refer_loss_v1",
        "kind": 2,
        "importPath": "optimization.loss_function",
        "description": "optimization.loss_function",
        "peekOfCode": "def get_refer_loss_v1(txt_cls_logits, obj_cls_post_logits, obj_cls_pre_logits, obj_cls_raw_logits, og3d_logits, tgt_object_label, tgt_object_id, obj_labels, obj_masks):\n    og3d_loss = F.cross_entropy(og3d_logits, tgt_object_id.squeeze(1))\n    txt_cls_loss = F.cross_entropy(txt_cls_logits, tgt_object_label.squeeze(1))\n    obj_cls_raw_loss = (F.cross_entropy(obj_cls_raw_logits.permute(0, 2, 1), obj_labels, reduction='none') * obj_masks).sum() / obj_masks.sum()\n    obj_cls_pre_loss = (F.cross_entropy(obj_cls_pre_logits.permute(0, 2, 1), obj_labels, reduction='none') * obj_masks).sum() / obj_masks.sum()\n    obj_cls_post_loss = (F.cross_entropy(obj_cls_post_logits.permute(0, 2, 1), obj_labels, reduction='none') * obj_masks).sum() / obj_masks.sum()\n    total_loss = og3d_loss + txt_cls_loss + obj_cls_raw_loss + obj_cls_pre_loss + obj_cls_post_loss\n    return total_loss, og3d_loss, txt_cls_loss, obj_cls_raw_loss, obj_cls_pre_loss, obj_cls_post_loss\n@registry.register_optimizer(\"qa_loss_v1\")\ndef get_qa_loss_v1(txt_cls_logits, obj_cls_post_logits, obj_cls_pre_logits, obj_cls_raw_logits, og3d_logits, answer_scores, tgt_object_label, tgt_object_id, obj_labels, obj_masks, answer_label):",
        "detail": "optimization.loss_function",
        "documentation": {}
    },
    {
        "label": "get_qa_loss_v1",
        "kind": 2,
        "importPath": "optimization.loss_function",
        "description": "optimization.loss_function",
        "peekOfCode": "def get_qa_loss_v1(txt_cls_logits, obj_cls_post_logits, obj_cls_pre_logits, obj_cls_raw_logits, og3d_logits, answer_scores, tgt_object_label, tgt_object_id, obj_labels, obj_masks, answer_label):\n    og3d_logits = og3d_logits.masked_fill_(og3d_logits == -float('inf'), 0)\n    og3d_loss = F.binary_cross_entropy_with_logits(og3d_logits, tgt_object_id.float(), reduction='sum', weight=obj_masks) / float(tgt_object_id.shape[0])\n    txt_cls_loss = F.binary_cross_entropy_with_logits(txt_cls_logits, tgt_object_label.float(), reduction='sum') / float(tgt_object_label.shape[0])\n    obj_cls_raw_loss = (F.cross_entropy(obj_cls_raw_logits.permute(0, 2, 1), obj_labels, reduction='none') * obj_masks).sum() / obj_masks.sum()\n    obj_cls_pre_loss = (F.cross_entropy(obj_cls_pre_logits.permute(0, 2, 1), obj_labels, reduction='none') * obj_masks).sum() / obj_masks.sum()\n    obj_cls_post_loss = (F.cross_entropy(obj_cls_post_logits.permute(0, 2, 1), obj_labels, reduction='none') * obj_masks).sum() / obj_masks.sum()\n    answer_loss = F.binary_cross_entropy_with_logits(answer_scores, answer_label.float(), reduction='sum') / answer_scores.shape[0]\n    total_loss = og3d_loss + txt_cls_loss + obj_cls_raw_loss + obj_cls_pre_loss + obj_cls_post_loss + answer_loss\n    return total_loss, og3d_loss, txt_cls_loss, obj_cls_raw_loss, obj_cls_pre_loss, obj_cls_post_loss, answer_loss",
        "detail": "optimization.loss_function",
        "documentation": {}
    },
    {
        "label": "get_pretrain_loss_v1",
        "kind": 2,
        "importPath": "optimization.loss_function",
        "description": "optimization.loss_function",
        "peekOfCode": "def get_pretrain_loss_v1(txt_lm_cls_logits, masked_lm_labels, scene_txt_match_logits, replace, obj_cls_post_logits, obj_cls_pre_logits, obj_cls_raw_logits, obj_labels, obj_sem_masks, obj_masks):\n    loss_fct = CrossEntropyLoss(ignore_index=-1)\n    masked_lm_labels.masked_fill_(replace.unsqueeze(1), -1)\n    lm_cls_loss = loss_fct(txt_lm_cls_logits.permute(0, 2, 1), masked_lm_labels)\n    match_loss = loss_fct(scene_txt_match_logits, replace.long())\n    obj_cls_raw_loss = (F.cross_entropy(obj_cls_raw_logits.permute(0, 2, 1), obj_labels, reduction='none') * obj_masks).sum() / obj_masks.sum()\n    obj_cls_pre_loss = (F.cross_entropy(obj_cls_pre_logits.permute(0, 2, 1), obj_labels, reduction='none') * obj_masks).sum() / obj_masks.sum()\n    obj_cls_post_loss = (F.cross_entropy(obj_cls_post_logits.permute(0, 2, 1), obj_labels, reduction='none') * obj_masks).sum() / obj_masks.sum()\n    obj_cls_pre_loss_mask = (F.cross_entropy(obj_cls_pre_logits.permute(0, 2, 1), obj_labels, reduction='none') * obj_masks * obj_sem_masks.logical_not()).sum() / (obj_masks * obj_sem_masks.logical_not()).sum()\n    obj_cls_pre_loss_unmask = (F.cross_entropy(obj_cls_pre_logits.permute(0, 2, 1), obj_labels, reduction='none') * obj_masks * obj_sem_masks).sum() / (obj_masks * obj_sem_masks).sum()",
        "detail": "optimization.loss_function",
        "documentation": {}
    },
    {
        "label": "get_qa_loss_v1",
        "kind": 2,
        "importPath": "optimization.loss_function",
        "description": "optimization.loss_function",
        "peekOfCode": "def get_qa_loss_v1(obj_cls_post_logits, obj_cls_pre_logits, obj_cls_raw_logits, answer_scores, obj_labels, obj_masks, answer_label):\n    obj_cls_raw_loss = (F.cross_entropy(obj_cls_raw_logits.permute(0, 2, 1), obj_labels, reduction='none') * obj_masks).sum() / obj_masks.sum()\n    obj_cls_pre_loss = (F.cross_entropy(obj_cls_pre_logits.permute(0, 2, 1), obj_labels, reduction='none') * obj_masks).sum() / obj_masks.sum()\n    obj_cls_post_loss = (F.cross_entropy(obj_cls_post_logits.permute(0, 2, 1), obj_labels, reduction='none') * obj_masks).sum() / obj_masks.sum()\n    answer_loss = F.binary_cross_entropy_with_logits(answer_scores, answer_label.float(), reduction='sum') / answer_scores.shape[0]\n    total_loss = obj_cls_raw_loss + obj_cls_pre_loss + obj_cls_post_loss + answer_loss\n    return total_loss, obj_cls_raw_loss, obj_cls_pre_loss, obj_cls_post_loss, answer_loss\n@registry.register_optimizer(\"caption_loss_v1\")\ndef get_caption_loss_v1(txt_lm_cls_logits, masked_lm_labels, obj_cls_post_logits, obj_cls_pre_logits, obj_cls_raw_logits, obj_labels, obj_masks):\n    loss_fct = CrossEntropyLoss(ignore_index=-1)",
        "detail": "optimization.loss_function",
        "documentation": {}
    },
    {
        "label": "get_caption_loss_v1",
        "kind": 2,
        "importPath": "optimization.loss_function",
        "description": "optimization.loss_function",
        "peekOfCode": "def get_caption_loss_v1(txt_lm_cls_logits, masked_lm_labels, obj_cls_post_logits, obj_cls_pre_logits, obj_cls_raw_logits, obj_labels, obj_masks):\n    loss_fct = CrossEntropyLoss(ignore_index=-1)\n    lm_cls_loss = loss_fct(txt_lm_cls_logits.permute(0, 2, 1), masked_lm_labels)\n    obj_cls_raw_loss = (F.cross_entropy(obj_cls_raw_logits.permute(0, 2, 1), obj_labels, reduction='none') * obj_masks).sum() / obj_masks.sum()\n    obj_cls_pre_loss = (F.cross_entropy(obj_cls_pre_logits.permute(0, 2, 1), obj_labels, reduction='none') * obj_masks).sum() / obj_masks.sum()\n    obj_cls_post_loss = (F.cross_entropy(obj_cls_post_logits.permute(0, 2, 1), obj_labels, reduction='none') * obj_masks).sum() / obj_masks.sum()\n    total_loss = lm_cls_loss + obj_cls_raw_loss + obj_cls_pre_loss + obj_cls_post_loss\n    return total_loss, lm_cls_loss, obj_cls_raw_loss, obj_cls_pre_loss, obj_cls_post_loss",
        "detail": "optimization.loss_function",
        "documentation": {}
    },
    {
        "label": "Pipeline",
        "kind": 6,
        "importPath": "pipeline.pipeline",
        "description": "pipeline.pipeline",
        "peekOfCode": "class Pipeline(ABC):\n    @abstractmethod\n    def initialize(self):\n        pass\n    @abstractmethod\n    def run(self):\n        pass\n    @abstractmethod\n    def end(self):\n        pass",
        "detail": "pipeline.pipeline",
        "documentation": {}
    },
    {
        "label": "OptimusPrimePipeline",
        "kind": 6,
        "importPath": "pipeline.pipeline",
        "description": "pipeline.pipeline",
        "peekOfCode": "class OptimusPrimePipeline(Pipeline, NormalDataloaderMixin, ModelOptimizationMixin, ModelEvaluationMixin, ModelMetricMixin, ModelLossMixin):\n    def __init__(self, cfg):\n        # build saver and logger\n        if not cfg['eval_task']:\n            self.logger = registry.get_utils(cfg['logger']['name'])(cfg)\n        self.saver = registry.get_utils(cfg['saver']['name'])(**cfg['saver']['args'])\n        # build model\n        self.lang_encoder = registry.get_language_model(cfg['lang_encoder']['name'])(**cfg['lang_encoder']['args']).cuda()\n        self.point_encoder = registry.get_vision_model(cfg['point_encoder']['name'])(**cfg['point_encoder']['args']).cuda()\n        self.unified_encoder = registry.get_vision_model(cfg['unified_encoder']['name'])(**cfg['unified_encoder']['args']).cuda()",
        "detail": "pipeline.pipeline",
        "documentation": {}
    },
    {
        "label": "PipelineFactory",
        "kind": 6,
        "importPath": "pipeline.pipeline_factory",
        "description": "pipeline.pipeline_factory",
        "peekOfCode": "class PipelineFactory(object):\n    def __init__(self):\n        pass\n    @classmethod\n    def create_pipelines_from_yml(cls, yml_path):\n        with open(yml_path, \"r\") as stream:\n            try:\n                yml_file = yaml.safe_load(stream)\n            except yaml.YAMLError as exc:\n                raise ValueError(exc)",
        "detail": "pipeline.pipeline_factory",
        "documentation": {}
    },
    {
        "label": "pipeline_factory",
        "kind": 5,
        "importPath": "pipeline.pipeline_factory",
        "description": "pipeline.pipeline_factory",
        "peekOfCode": "pipeline_factory = PipelineFactory()",
        "detail": "pipeline.pipeline_factory",
        "documentation": {}
    },
    {
        "label": "NormalDataloaderMixin",
        "kind": 6,
        "importPath": "pipeline.pipeline_mixin",
        "description": "pipeline.pipeline_mixin",
        "peekOfCode": "class NormalDataloaderMixin:\n    def __init__(self) -> None:\n        pass\n    def build_dataloader(self, dataset):\n        data_loader = torch.utils.data.DataLoader(\n            dataset,\n            batch_size=self.batch_size, \n            num_workers=8,\n            pin_memory=True,\n            shuffle=True, ",
        "detail": "pipeline.pipeline_mixin",
        "documentation": {}
    },
    {
        "label": "ModelOptimizationMixin",
        "kind": 6,
        "importPath": "pipeline.pipeline_mixin",
        "description": "pipeline.pipeline_mixin",
        "peekOfCode": "class ModelOptimizationMixin(object):\n    def __init__(self):\n        pass\n    @staticmethod\n    def warmup_cosine(step, warmup_step, tot_step):\n        if step <= warmup_step:\n            return step / warmup_step\n        return max(0.5 * (1 + math.cos((step - warmup_step) / (tot_step - warmup_step) * math.pi)), 1e-5)\n    def no_decay_param_group(self, parameters, lr):\n        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']",
        "detail": "pipeline.pipeline_mixin",
        "documentation": {}
    },
    {
        "label": "ModelEvaluationMixin",
        "kind": 6,
        "importPath": "pipeline.pipeline_mixin",
        "description": "pipeline.pipeline_mixin",
        "peekOfCode": "class ModelEvaluationMixin(object):\n    def eval_scanrefer(self, epoch):\n        eval_dict = {'target_metric': [], 'og_acc': [], 'og_acc_iou25': [], 'og_acc_iou50': [], 'og_acc_iou25_unique': [], 'og_acc_iou50_unique': [],\n                         'og_acc_iou25_multiple': [], 'og_acc_iou50_multiple': [], 'txt_acc': [], 'obj_cls_raw_acc': [], 'obj_cls_pre_acc': [], 'obj_cls_post_acc': []}\n         # run\n        total_count = 0\n        total_unique_count = 0\n        total_multiple_count = 0\n        if self.eval_task:\n            eval_results = []",
        "detail": "pipeline.pipeline_mixin",
        "documentation": {}
    },
    {
        "label": "ModelLossMixin",
        "kind": 6,
        "importPath": "pipeline.pipeline_mixin",
        "description": "pipeline.pipeline_mixin",
        "peekOfCode": "class ModelLossMixin(object):\n    def get_refer_loss(self, data_dict):\n        total_loss, og3d_loss, txt_cls_loss, obj_cls_raw_loss, obj_cls_pre_loss, obj_cls_post_loss = self.refer_loss(data_dict['txt_cls_logits'], data_dict['obj_cls_post_logits'], data_dict['obj_cls_pre_logits'], data_dict['obj_cls_raw_logits'], data_dict['og3d_logits'], \n                                  data_dict['tgt_object_label'], data_dict['tgt_object_id'], data_dict['obj_labels'], data_dict['obj_masks'])\n        data_dict['total_loss'] = total_loss\n        data_dict['og3d_loss'] = og3d_loss\n        data_dict['txt_cls_loss'] = txt_cls_loss\n        data_dict['obj_cls_raw_loss'] = obj_cls_raw_loss\n        data_dict['obj_cls_pre_loss'] = obj_cls_pre_loss\n        data_dict['obj_cls_post_loss'] = obj_cls_post_loss",
        "detail": "pipeline.pipeline_mixin",
        "documentation": {}
    },
    {
        "label": "ModelMetricMixin",
        "kind": 6,
        "importPath": "pipeline.pipeline_mixin",
        "description": "pipeline.pipeline_mixin",
        "peekOfCode": "class ModelMetricMixin(object):\n    def get_scanrefer_metrics(self, data_dict):\n        data_dict['og_acc'] = (torch.argmax(data_dict['og3d_logits'], dim=1) == data_dict['tgt_object_id'].squeeze(1)).sum().item() / float(len(data_dict['tgt_object_id']))\n        # get og acc iou 25 and 50\n        og_pred = torch.argmax(data_dict['og3d_logits'], dim=1)\n        iou25_correct = 0\n        iou50_correct = 0\n        iou25_unique_correct = 0\n        iou50_unique_correct = 0\n        iou25_multiple_correct = 0",
        "detail": "pipeline.pipeline_mixin",
        "documentation": {}
    },
    {
        "label": "Registry",
        "kind": 6,
        "importPath": "pipeline.registry",
        "description": "pipeline.registry",
        "peekOfCode": "class Registry:\n    mapping = {\n        \"dataset\": {},\n        \"vision_model\": {},\n        \"language_model\": {},\n        \"other_model\": {},\n        \"model_assembler\": {},\n        \"optimizer\": {},\n        \"scheduler\": {},\n        \"optimize_assembler\": {},",
        "detail": "pipeline.registry",
        "documentation": {}
    },
    {
        "label": "setup_imports",
        "kind": 2,
        "importPath": "pipeline.registry",
        "description": "pipeline.registry",
        "peekOfCode": "def setup_imports(base_folder=\"./\"):\n    # folders to import\n    folder_list = [\"dataset\", \"model\", \"optimization\", \"pipeline\", \"utils\"]\n    files = sum([glob.glob(os.path.join(base_folder, folder) + \"/**\", recursive=True) for folder in folder_list], [])\n    for f in files:\n        if f.endswith(\".py\") and not \"setup.py\" in f:\n            #f = os.path.realpath(f)\n            splits = f.split(os.sep)[1:]\n            file_name = splits[-1]\n            module_name = file_name[: file_name.find(\".py\")]",
        "detail": "pipeline.registry",
        "documentation": {}
    },
    {
        "label": "registry",
        "kind": 5,
        "importPath": "pipeline.registry",
        "description": "pipeline.registry",
        "peekOfCode": "registry = Registry()\ndef setup_imports(base_folder=\"./\"):\n    # folders to import\n    folder_list = [\"dataset\", \"model\", \"optimization\", \"pipeline\", \"utils\"]\n    files = sum([glob.glob(os.path.join(base_folder, folder) + \"/**\", recursive=True) for folder in folder_list], [])\n    for f in files:\n        if f.endswith(\".py\") and not \"setup.py\" in f:\n            #f = os.path.realpath(f)\n            splits = f.split(os.sep)[1:]\n            file_name = splits[-1]",
        "detail": "pipeline.registry",
        "documentation": {}
    },
    {
        "label": "export_one_scan",
        "kind": 2,
        "importPath": "scannet.batch_load_scannet_data",
        "description": "scannet.batch_load_scannet_data",
        "peekOfCode": "def export_one_scan(scan_name, output_filename_prefix):    \n    mesh_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '_vh_clean_2.ply')\n    agg_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '.aggregation.json')\n    seg_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '_vh_clean_2.0.010000.segs.json')\n    meta_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '.txt') # includes axisAlignment info for the train set scans.   \n    mesh_vertices, aligned_vertices, semantic_labels, instance_labels, instance_bboxes, aligned_instance_bboxes = export(mesh_file, agg_file, seg_file, meta_file, LABEL_MAP_FILE, None)\n    mask = np.logical_not(np.in1d(semantic_labels, DONOTCARE_CLASS_IDS))\n    mesh_vertices = mesh_vertices[mask,:]\n    aligned_vertices = aligned_vertices[mask,:]\n    semantic_labels = semantic_labels[mask]",
        "detail": "scannet.batch_load_scannet_data",
        "documentation": {}
    },
    {
        "label": "batch_export",
        "kind": 2,
        "importPath": "scannet.batch_load_scannet_data",
        "description": "scannet.batch_load_scannet_data",
        "peekOfCode": "def batch_export():\n    if not os.path.exists(OUTPUT_FOLDER):\n        print('Creating new data folder: {}'.format(OUTPUT_FOLDER))                \n        os.mkdir(OUTPUT_FOLDER)        \n    for scan_name in SCAN_NAMES:\n        output_filename_prefix = os.path.join(OUTPUT_FOLDER, scan_name)\n        # if os.path.exists(output_filename_prefix + '_vert.npy'): continue\n        print('-'*20+'begin')\n        print(datetime.datetime.now())\n        print(scan_name)",
        "detail": "scannet.batch_load_scannet_data",
        "documentation": {}
    },
    {
        "label": "SCANNET_DIR",
        "kind": 5,
        "importPath": "scannet.batch_load_scannet_data",
        "description": "scannet.batch_load_scannet_data",
        "peekOfCode": "SCANNET_DIR = 'scans'\nSCAN_NAMES = sorted([line.rstrip() for line in open('meta_data/scannetv2.txt')])\nLABEL_MAP_FILE = 'meta_data/scannetv2-labels.combined.tsv'\nDONOTCARE_CLASS_IDS = np.array([])\nOBJ_CLASS_IDS = np.array([3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]) # exclude wall (1), floor (2), ceiling (22)\nMAX_NUM_POINT = 50000\nOUTPUT_FOLDER = './scannet_data'\ndef export_one_scan(scan_name, output_filename_prefix):    \n    mesh_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '_vh_clean_2.ply')\n    agg_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '.aggregation.json')",
        "detail": "scannet.batch_load_scannet_data",
        "documentation": {}
    },
    {
        "label": "SCAN_NAMES",
        "kind": 5,
        "importPath": "scannet.batch_load_scannet_data",
        "description": "scannet.batch_load_scannet_data",
        "peekOfCode": "SCAN_NAMES = sorted([line.rstrip() for line in open('meta_data/scannetv2.txt')])\nLABEL_MAP_FILE = 'meta_data/scannetv2-labels.combined.tsv'\nDONOTCARE_CLASS_IDS = np.array([])\nOBJ_CLASS_IDS = np.array([3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]) # exclude wall (1), floor (2), ceiling (22)\nMAX_NUM_POINT = 50000\nOUTPUT_FOLDER = './scannet_data'\ndef export_one_scan(scan_name, output_filename_prefix):    \n    mesh_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '_vh_clean_2.ply')\n    agg_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '.aggregation.json')\n    seg_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '_vh_clean_2.0.010000.segs.json')",
        "detail": "scannet.batch_load_scannet_data",
        "documentation": {}
    },
    {
        "label": "LABEL_MAP_FILE",
        "kind": 5,
        "importPath": "scannet.batch_load_scannet_data",
        "description": "scannet.batch_load_scannet_data",
        "peekOfCode": "LABEL_MAP_FILE = 'meta_data/scannetv2-labels.combined.tsv'\nDONOTCARE_CLASS_IDS = np.array([])\nOBJ_CLASS_IDS = np.array([3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]) # exclude wall (1), floor (2), ceiling (22)\nMAX_NUM_POINT = 50000\nOUTPUT_FOLDER = './scannet_data'\ndef export_one_scan(scan_name, output_filename_prefix):    \n    mesh_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '_vh_clean_2.ply')\n    agg_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '.aggregation.json')\n    seg_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '_vh_clean_2.0.010000.segs.json')\n    meta_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '.txt') # includes axisAlignment info for the train set scans.   ",
        "detail": "scannet.batch_load_scannet_data",
        "documentation": {}
    },
    {
        "label": "DONOTCARE_CLASS_IDS",
        "kind": 5,
        "importPath": "scannet.batch_load_scannet_data",
        "description": "scannet.batch_load_scannet_data",
        "peekOfCode": "DONOTCARE_CLASS_IDS = np.array([])\nOBJ_CLASS_IDS = np.array([3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]) # exclude wall (1), floor (2), ceiling (22)\nMAX_NUM_POINT = 50000\nOUTPUT_FOLDER = './scannet_data'\ndef export_one_scan(scan_name, output_filename_prefix):    \n    mesh_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '_vh_clean_2.ply')\n    agg_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '.aggregation.json')\n    seg_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '_vh_clean_2.0.010000.segs.json')\n    meta_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '.txt') # includes axisAlignment info for the train set scans.   \n    mesh_vertices, aligned_vertices, semantic_labels, instance_labels, instance_bboxes, aligned_instance_bboxes = export(mesh_file, agg_file, seg_file, meta_file, LABEL_MAP_FILE, None)",
        "detail": "scannet.batch_load_scannet_data",
        "documentation": {}
    },
    {
        "label": "OBJ_CLASS_IDS",
        "kind": 5,
        "importPath": "scannet.batch_load_scannet_data",
        "description": "scannet.batch_load_scannet_data",
        "peekOfCode": "OBJ_CLASS_IDS = np.array([3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]) # exclude wall (1), floor (2), ceiling (22)\nMAX_NUM_POINT = 50000\nOUTPUT_FOLDER = './scannet_data'\ndef export_one_scan(scan_name, output_filename_prefix):    \n    mesh_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '_vh_clean_2.ply')\n    agg_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '.aggregation.json')\n    seg_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '_vh_clean_2.0.010000.segs.json')\n    meta_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '.txt') # includes axisAlignment info for the train set scans.   \n    mesh_vertices, aligned_vertices, semantic_labels, instance_labels, instance_bboxes, aligned_instance_bboxes = export(mesh_file, agg_file, seg_file, meta_file, LABEL_MAP_FILE, None)\n    mask = np.logical_not(np.in1d(semantic_labels, DONOTCARE_CLASS_IDS))",
        "detail": "scannet.batch_load_scannet_data",
        "documentation": {}
    },
    {
        "label": "MAX_NUM_POINT",
        "kind": 5,
        "importPath": "scannet.batch_load_scannet_data",
        "description": "scannet.batch_load_scannet_data",
        "peekOfCode": "MAX_NUM_POINT = 50000\nOUTPUT_FOLDER = './scannet_data'\ndef export_one_scan(scan_name, output_filename_prefix):    \n    mesh_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '_vh_clean_2.ply')\n    agg_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '.aggregation.json')\n    seg_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '_vh_clean_2.0.010000.segs.json')\n    meta_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '.txt') # includes axisAlignment info for the train set scans.   \n    mesh_vertices, aligned_vertices, semantic_labels, instance_labels, instance_bboxes, aligned_instance_bboxes = export(mesh_file, agg_file, seg_file, meta_file, LABEL_MAP_FILE, None)\n    mask = np.logical_not(np.in1d(semantic_labels, DONOTCARE_CLASS_IDS))\n    mesh_vertices = mesh_vertices[mask,:]",
        "detail": "scannet.batch_load_scannet_data",
        "documentation": {}
    },
    {
        "label": "OUTPUT_FOLDER",
        "kind": 5,
        "importPath": "scannet.batch_load_scannet_data",
        "description": "scannet.batch_load_scannet_data",
        "peekOfCode": "OUTPUT_FOLDER = './scannet_data'\ndef export_one_scan(scan_name, output_filename_prefix):    \n    mesh_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '_vh_clean_2.ply')\n    agg_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '.aggregation.json')\n    seg_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '_vh_clean_2.0.010000.segs.json')\n    meta_file = os.path.join(SCANNET_DIR, scan_name, scan_name + '.txt') # includes axisAlignment info for the train set scans.   \n    mesh_vertices, aligned_vertices, semantic_labels, instance_labels, instance_bboxes, aligned_instance_bboxes = export(mesh_file, agg_file, seg_file, meta_file, LABEL_MAP_FILE, None)\n    mask = np.logical_not(np.in1d(semantic_labels, DONOTCARE_CLASS_IDS))\n    mesh_vertices = mesh_vertices[mask,:]\n    aligned_vertices = aligned_vertices[mask,:]",
        "detail": "scannet.batch_load_scannet_data",
        "documentation": {}
    },
    {
        "label": "read_aggregation",
        "kind": 2,
        "importPath": "scannet.load_scannet_data",
        "description": "scannet.load_scannet_data",
        "peekOfCode": "def read_aggregation(filename):\n    object_id_to_segs = {}\n    label_to_segs = {}\n    with open(filename) as f:\n        data = json.load(f)\n        num_objects = len(data['segGroups'])\n        for i in range(num_objects):\n            object_id = data['segGroups'][i]['objectId'] + 1 # instance ids should be 1-indexed\n            label = data['segGroups'][i]['label']\n            segs = data['segGroups'][i]['segments']",
        "detail": "scannet.load_scannet_data",
        "documentation": {}
    },
    {
        "label": "read_segmentation",
        "kind": 2,
        "importPath": "scannet.load_scannet_data",
        "description": "scannet.load_scannet_data",
        "peekOfCode": "def read_segmentation(filename):\n    seg_to_verts = {}\n    with open(filename) as f:\n        data = json.load(f)\n        num_verts = len(data['segIndices'])\n        for i in range(num_verts):\n            seg_id = data['segIndices'][i]\n            if seg_id in seg_to_verts:\n                seg_to_verts[seg_id].append(i)\n            else:",
        "detail": "scannet.load_scannet_data",
        "documentation": {}
    },
    {
        "label": "export",
        "kind": 2,
        "importPath": "scannet.load_scannet_data",
        "description": "scannet.load_scannet_data",
        "peekOfCode": "def export(mesh_file, agg_file, seg_file, meta_file, label_map_file, output_file=None):\n    \"\"\" points are XYZ RGB (RGB in 0-255),\n    semantic label as nyu40 ids,\n    instance label as 1-#instance,\n    box as (cx,cy,cz,dx,dy,dz,semantic_label)\n    \"\"\"\n    label_map = scannet_utils.read_label_mapping(label_map_file, label_from='raw_category', label_to='nyu40id')    \n    # mesh_vertices = scannet_utils.read_mesh_vertices_rgb(mesh_file)\n    mesh_vertices = scannet_utils.read_mesh_vertices_rgb_normal(mesh_file)\n    # Load scene axis alignment matrix",
        "detail": "scannet.load_scannet_data",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "scannet.load_scannet_data",
        "description": "scannet.load_scannet_data",
        "peekOfCode": "def main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--scan_path', required=True, help='path to scannet scene (e.g., data/ScanNet/v2/scene0000_00')\n    parser.add_argument('--output_file', required=True, help='output file')\n    parser.add_argument('--label_map_file', required=True, help='path to scannetv2-labels.combined.tsv')\n    opt = parser.parse_args()\n    scan_name = os.path.split(opt.scan_path)[-1]\n    mesh_file = os.path.join(opt.scan_path, scan_name + '_vh_clean_2.ply')\n    agg_file = os.path.join(opt.scan_path, scan_name + '.aggregation.json')\n    seg_file = os.path.join(opt.scan_path, scan_name + '_vh_clean_2.0.010000.segs.json')",
        "detail": "scannet.load_scannet_data",
        "documentation": {}
    },
    {
        "label": "ScannetDatasetConfig",
        "kind": 6,
        "importPath": "scannet.model_util_scannet",
        "description": "scannet.model_util_scannet",
        "peekOfCode": "class ScannetDatasetConfig(object):\n    def __init__(self):\n        self.type2class = {'cabinet':0, 'bed':1, 'chair':2, 'sofa':3, 'table':4, 'door':5,\n            'window':6,'bookshelf':7,'picture':8, 'counter':9, 'desk':10, 'curtain':11,\n            'refrigerator':12, 'shower curtain':13, 'toilet':14, 'sink':15, 'bathtub':16, 'others':17}  \n        self.class2type = {self.type2class[t]:t for t in self.type2class}\n        self.nyu40ids = np.array([3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40]) # exclude wall (1), floor (2), ceiling (22)\n        self.nyu40id2class = self._get_nyu40id2class()\n        self.mean_size_arr = np.load(os.path.join(CONF.PATH.SCANNET, 'meta_data/scannet_reference_means.npz'))['arr_0']\n        self.num_class = len(self.type2class.keys())",
        "detail": "scannet.model_util_scannet",
        "documentation": {}
    },
    {
        "label": "in_hull",
        "kind": 2,
        "importPath": "scannet.model_util_scannet",
        "description": "scannet.model_util_scannet",
        "peekOfCode": "def in_hull(p, hull):\n    from scipy.spatial import Delaunay\n    if not isinstance(hull,Delaunay):\n        hull = Delaunay(hull)\n    return hull.find_simplex(p)>=0\ndef extract_pc_in_box3d(pc, box3d):\n    ''' pc: (N,3), box3d: (8,3) '''\n    box3d_roi_inds = in_hull(pc[:,0:3], box3d)\n    return pc[box3d_roi_inds,:], box3d_roi_inds\ndef rotate_aligned_boxes(input_boxes, rot_mat):    ",
        "detail": "scannet.model_util_scannet",
        "documentation": {}
    },
    {
        "label": "extract_pc_in_box3d",
        "kind": 2,
        "importPath": "scannet.model_util_scannet",
        "description": "scannet.model_util_scannet",
        "peekOfCode": "def extract_pc_in_box3d(pc, box3d):\n    ''' pc: (N,3), box3d: (8,3) '''\n    box3d_roi_inds = in_hull(pc[:,0:3], box3d)\n    return pc[box3d_roi_inds,:], box3d_roi_inds\ndef rotate_aligned_boxes(input_boxes, rot_mat):    \n    centers, lengths = input_boxes[:,0:3], input_boxes[:,3:6]    \n    new_centers = np.dot(centers, np.transpose(rot_mat))\n    dx, dy = lengths[:,0]/2.0, lengths[:,1]/2.0\n    new_x = np.zeros((dx.shape[0], 4))\n    new_y = np.zeros((dx.shape[0], 4))",
        "detail": "scannet.model_util_scannet",
        "documentation": {}
    },
    {
        "label": "rotate_aligned_boxes",
        "kind": 2,
        "importPath": "scannet.model_util_scannet",
        "description": "scannet.model_util_scannet",
        "peekOfCode": "def rotate_aligned_boxes(input_boxes, rot_mat):    \n    centers, lengths = input_boxes[:,0:3], input_boxes[:,3:6]    \n    new_centers = np.dot(centers, np.transpose(rot_mat))\n    dx, dy = lengths[:,0]/2.0, lengths[:,1]/2.0\n    new_x = np.zeros((dx.shape[0], 4))\n    new_y = np.zeros((dx.shape[0], 4))\n    for i, crnr in enumerate([(-1,-1), (1, -1), (1, 1), (-1, 1)]):        \n        crnrs = np.zeros((dx.shape[0], 3))\n        crnrs[:,0] = crnr[0]*dx\n        crnrs[:,1] = crnr[1]*dy",
        "detail": "scannet.model_util_scannet",
        "documentation": {}
    },
    {
        "label": "rotate_aligned_boxes_along_axis",
        "kind": 2,
        "importPath": "scannet.model_util_scannet",
        "description": "scannet.model_util_scannet",
        "peekOfCode": "def rotate_aligned_boxes_along_axis(input_boxes, rot_mat, axis):    \n    centers, lengths = input_boxes[:,0:3], input_boxes[:,3:6]    \n    new_centers = np.dot(centers, np.transpose(rot_mat))\n    if axis == \"x\":     \n        d1, d2 = lengths[:,1]/2.0, lengths[:,2]/2.0\n    elif axis == \"y\":\n        d1, d2 = lengths[:,0]/2.0, lengths[:,2]/2.0\n    else:\n        d1, d2 = lengths[:,0]/2.0, lengths[:,1]/2.0\n    new_1 = np.zeros((d1.shape[0], 4))",
        "detail": "scannet.model_util_scannet",
        "documentation": {}
    },
    {
        "label": "normalize_v3",
        "kind": 2,
        "importPath": "scannet.scannet_utils",
        "description": "scannet.scannet_utils",
        "peekOfCode": "def normalize_v3(arr):\n    ''' Normalize a numpy array of 3 component vectors shape=(n,3) '''\n    lens = np.sqrt( arr[:,0]**2 + arr[:,1]**2 + arr[:,2]**2 )\n    arr[:,0] /= (lens + 1e-8)\n    arr[:,1] /= (lens + 1e-8)\n    arr[:,2] /= (lens + 1e-8)                \n    return arr\ndef compute_normal(vertices, faces):\n    #Create a zeroed array with the same type and shape as our vertices i.e., per vertex normal\n    normals = np.zeros( vertices.shape, dtype=vertices.dtype )",
        "detail": "scannet.scannet_utils",
        "documentation": {}
    },
    {
        "label": "compute_normal",
        "kind": 2,
        "importPath": "scannet.scannet_utils",
        "description": "scannet.scannet_utils",
        "peekOfCode": "def compute_normal(vertices, faces):\n    #Create a zeroed array with the same type and shape as our vertices i.e., per vertex normal\n    normals = np.zeros( vertices.shape, dtype=vertices.dtype )\n    #Create an indexed view into the vertex array using the array of three indices for triangles\n    tris = vertices[faces]\n    #Calculate the normal for all the triangles, by taking the cross product of the vectors v1-v0, and v2-v0 in each triangle             \n    n = np.cross( tris[::,1 ] - tris[::,0]  , tris[::,2 ] - tris[::,0] )\n    # n is now an array of normals per triangle. The length of each normal is dependent the vertices, \n    # we need to normalize these, so that our next step weights each normal equally.\n    normalize_v3(n)",
        "detail": "scannet.scannet_utils",
        "documentation": {}
    },
    {
        "label": "represents_int",
        "kind": 2,
        "importPath": "scannet.scannet_utils",
        "description": "scannet.scannet_utils",
        "peekOfCode": "def represents_int(s):\n    ''' if string s represents an int. '''\n    try: \n        int(s)\n        return True\n    except ValueError:\n        return False\ndef read_label_mapping(filename, label_from='raw_category', label_to='nyu40id'):\n    assert os.path.isfile(filename)\n    mapping = dict()",
        "detail": "scannet.scannet_utils",
        "documentation": {}
    },
    {
        "label": "read_label_mapping",
        "kind": 2,
        "importPath": "scannet.scannet_utils",
        "description": "scannet.scannet_utils",
        "peekOfCode": "def read_label_mapping(filename, label_from='raw_category', label_to='nyu40id'):\n    assert os.path.isfile(filename)\n    mapping = dict()\n    with open(filename) as csvfile:\n        reader = csv.DictReader(csvfile, delimiter='\\t')\n        for row in reader:\n            mapping[row[label_from]] = int(row[label_to])\n    if represents_int(list(mapping.keys())[0]):\n        mapping = {int(k):v for k,v in mapping.items()}\n    return mapping",
        "detail": "scannet.scannet_utils",
        "documentation": {}
    },
    {
        "label": "read_mesh_vertices",
        "kind": 2,
        "importPath": "scannet.scannet_utils",
        "description": "scannet.scannet_utils",
        "peekOfCode": "def read_mesh_vertices(filename):\n    \"\"\" read XYZ for each vertex.\n    \"\"\"\n    assert os.path.isfile(filename)\n    with open(filename, 'rb') as f:\n        plydata = PlyData.read(f)\n        num_verts = plydata['vertex'].count\n        vertices = np.zeros(shape=[num_verts, 3], dtype=np.float32)\n        vertices[:,0] = plydata['vertex'].data['x']\n        vertices[:,1] = plydata['vertex'].data['y']",
        "detail": "scannet.scannet_utils",
        "documentation": {}
    },
    {
        "label": "read_mesh_vertices_rgb",
        "kind": 2,
        "importPath": "scannet.scannet_utils",
        "description": "scannet.scannet_utils",
        "peekOfCode": "def read_mesh_vertices_rgb(filename):\n    \"\"\" read XYZ RGB for each vertex.\n    Note: RGB values are in 0-255\n    \"\"\"\n    assert os.path.isfile(filename)\n    with open(filename, 'rb') as f:\n        plydata = PlyData.read(f)\n        num_verts = plydata['vertex'].count\n        vertices = np.zeros(shape=[num_verts, 6], dtype=np.float32)\n        vertices[:,0] = plydata['vertex'].data['x']",
        "detail": "scannet.scannet_utils",
        "documentation": {}
    },
    {
        "label": "read_mesh_vertices_rgb_normal",
        "kind": 2,
        "importPath": "scannet.scannet_utils",
        "description": "scannet.scannet_utils",
        "peekOfCode": "def read_mesh_vertices_rgb_normal(filename):\n    \"\"\" read XYZ RGB normals point cloud from filename PLY file \"\"\"\n    assert(os.path.isfile(filename))\n    with open(filename, 'rb') as f:\n        plydata = PlyData.read(f)\n        num_verts = plydata['vertex'].count\n        vertices = np.zeros(shape=[num_verts, 9], dtype=np.float32)\n        vertices[:,0] = plydata['vertex'].data['x']\n        vertices[:,1] = plydata['vertex'].data['y']\n        vertices[:,2] = plydata['vertex'].data['z']",
        "detail": "scannet.scannet_utils",
        "documentation": {}
    },
    {
        "label": "Bleu",
        "kind": 6,
        "importPath": "utils.bleu.bleu",
        "description": "utils.bleu.bleu",
        "peekOfCode": "class Bleu:\n    def __init__(self, n=4):\n        # default compute Blue score up to 4\n        self._n = n\n        self._hypo_for_image = {}\n        self.ref_for_image = {}\n    def compute_score(self, gts, res):\n        assert(gts.keys() == res.keys())\n        imgIds = gts.keys()\n        bleu_scorer = BleuScorer(n=self._n)",
        "detail": "utils.bleu.bleu",
        "documentation": {}
    },
    {
        "label": "BleuScorer",
        "kind": 6,
        "importPath": "utils.bleu.bleu_scorer",
        "description": "utils.bleu.bleu_scorer",
        "peekOfCode": "class BleuScorer(object):\n    \"\"\"Bleu scorer.\n    \"\"\"\n    __slots__ = \"n\", \"crefs\", \"ctest\", \"_score\", \"_ratio\", \"_testlen\", \"_reflen\", \"special_reflen\"\n    # special_reflen is used in oracle (proportional effective ref len for a node).\n    def copy(self):\n        ''' copy the refs.'''\n        new = BleuScorer(n=self.n)\n        new.ctest = copy.copy(self.ctest)\n        new.crefs = copy.copy(self.crefs)",
        "detail": "utils.bleu.bleu_scorer",
        "documentation": {}
    },
    {
        "label": "precook",
        "kind": 2,
        "importPath": "utils.bleu.bleu_scorer",
        "description": "utils.bleu.bleu_scorer",
        "peekOfCode": "def precook(s, n=4, out=False):\n    \"\"\"Takes a string as input and returns an object that can be given to\n    either cook_refs or cook_test. This is optional: cook_refs and cook_test\n    can take string arguments as well.\"\"\"\n    words = s.split()\n    counts = defaultdict(int)\n    for k in range(1,n+1):\n        for i in range(len(words)-k+1):\n            ngram = tuple(words[i:i+k])\n            counts[ngram] += 1",
        "detail": "utils.bleu.bleu_scorer",
        "documentation": {}
    },
    {
        "label": "cook_refs",
        "kind": 2,
        "importPath": "utils.bleu.bleu_scorer",
        "description": "utils.bleu.bleu_scorer",
        "peekOfCode": "def cook_refs(refs, eff=None, n=4): ## lhuang: oracle will call with \"average\"\n    '''Takes a list of reference sentences for a single segment\n    and returns an object that encapsulates everything that BLEU\n    needs to know about them.'''\n    reflen = []\n    maxcounts = {}\n    for ref in refs:\n        rl, counts = precook(ref, n)\n        reflen.append(rl)\n        for (ngram,count) in counts.items():",
        "detail": "utils.bleu.bleu_scorer",
        "documentation": {}
    },
    {
        "label": "cook_test",
        "kind": 2,
        "importPath": "utils.bleu.bleu_scorer",
        "description": "utils.bleu.bleu_scorer",
        "peekOfCode": "def cook_test(test, refs, eff=None, n=4):\n    '''Takes a test sentence and returns an object that\n    encapsulates everything that BLEU needs to know about it.'''\n    reflen, refmaxcounts = refs\n    testlen, counts = precook(test, n, True)\n    result = {}\n    # Calculate effective reference sentence length.\n    if eff == \"closest\":\n        result[\"reflen\"] = min((abs(l-testlen), l) for l in reflen)[1]\n    else: ## i.e., \"average\" or \"shortest\" or None",
        "detail": "utils.bleu.bleu_scorer",
        "documentation": {}
    },
    {
        "label": "Cider",
        "kind": 6,
        "importPath": "utils.cider.cider",
        "description": "utils.cider.cider",
        "peekOfCode": "class Cider:\n    \"\"\"\n    Main Class to compute the CIDEr metric\n    \"\"\"\n    def __init__(self, test=None, refs=None, n=4, sigma=6.0):\n        # set cider to sum over 1 to 4-grams\n        self._n = n\n        # set the standard deviation parameter for gaussian penalty\n        self._sigma = sigma\n    def compute_score(self, gts, res):",
        "detail": "utils.cider.cider",
        "documentation": {}
    },
    {
        "label": "CiderScorer",
        "kind": 6,
        "importPath": "utils.cider.cider_scorer",
        "description": "utils.cider.cider_scorer",
        "peekOfCode": "class CiderScorer(object):\n    \"\"\"CIDEr scorer.\n    \"\"\"\n    def copy(self):\n        ''' copy the refs.'''\n        new = CiderScorer(n=self.n)\n        new.ctest = copy.copy(self.ctest)\n        new.crefs = copy.copy(self.crefs)\n        return new\n    def __init__(self, test=None, refs=None, n=4, sigma=6.0):",
        "detail": "utils.cider.cider_scorer",
        "documentation": {}
    },
    {
        "label": "precook",
        "kind": 2,
        "importPath": "utils.cider.cider_scorer",
        "description": "utils.cider.cider_scorer",
        "peekOfCode": "def precook(s, n=4, out=False):\n    \"\"\"\n    Takes a string as input and returns an object that can be given to\n    either cook_refs or cook_test. This is optional: cook_refs and cook_test\n    can take string arguments as well.\n    :param s: string : sentence to be converted into ngrams\n    :param n: int    : number of ngrams for which representation is calculated\n    :return: term frequency vector for occuring ngrams\n    \"\"\"\n    words = s.split()",
        "detail": "utils.cider.cider_scorer",
        "documentation": {}
    },
    {
        "label": "cook_refs",
        "kind": 2,
        "importPath": "utils.cider.cider_scorer",
        "description": "utils.cider.cider_scorer",
        "peekOfCode": "def cook_refs(refs, n=4): ## lhuang: oracle will call with \"average\"\n    '''Takes a list of reference sentences for a single segment\n    and returns an object that encapsulates everything that BLEU\n    needs to know about them.\n    :param refs: list of string : reference sentences for some image\n    :param n: int : number of ngrams for which (ngram) representation is calculated\n    :return: result (list of dict)\n    '''\n    return [precook(ref, n) for ref in refs]\ndef cook_test(test, n=4):",
        "detail": "utils.cider.cider_scorer",
        "documentation": {}
    },
    {
        "label": "cook_test",
        "kind": 2,
        "importPath": "utils.cider.cider_scorer",
        "description": "utils.cider.cider_scorer",
        "peekOfCode": "def cook_test(test, n=4):\n    '''Takes a test sentence and returns an object that\n    encapsulates everything that BLEU needs to know about it.\n    :param test: list of string : hypothesis sentence for some image\n    :param n: int : number of ngrams for which (ngram) representation is calculated\n    :return: result (dict)\n    '''\n    return precook(test, n, True)\nclass CiderScorer(object):\n    \"\"\"CIDEr scorer.",
        "detail": "utils.cider.cider_scorer",
        "documentation": {}
    },
    {
        "label": "Meteor",
        "kind": 6,
        "importPath": "utils.meteor.meteor",
        "description": "utils.meteor.meteor",
        "peekOfCode": "class Meteor:\n    def __init__(self):\n        self.meteor_cmd = ['java', '-jar', '-Xmx2G', METEOR_JAR, \\\n                '-', '-', '-stdio', '-l', 'en', '-norm']\n        self.meteor_p = subprocess.Popen(self.meteor_cmd, \\\n                cwd=os.path.dirname(os.path.abspath(__file__)), \\\n                stdin=subprocess.PIPE, \\\n                stdout=subprocess.PIPE, \\\n                stderr=subprocess.PIPE)\n        # Used to guarantee thread safety",
        "detail": "utils.meteor.meteor",
        "documentation": {}
    },
    {
        "label": "METEOR_JAR",
        "kind": 5,
        "importPath": "utils.meteor.meteor",
        "description": "utils.meteor.meteor",
        "peekOfCode": "METEOR_JAR = 'meteor-1.5.jar'\n# print METEOR_JAR\nclass Meteor:\n    def __init__(self):\n        self.meteor_cmd = ['java', '-jar', '-Xmx2G', METEOR_JAR, \\\n                '-', '-', '-stdio', '-l', 'en', '-norm']\n        self.meteor_p = subprocess.Popen(self.meteor_cmd, \\\n                cwd=os.path.dirname(os.path.abspath(__file__)), \\\n                stdin=subprocess.PIPE, \\\n                stdout=subprocess.PIPE, \\",
        "detail": "utils.meteor.meteor",
        "documentation": {}
    },
    {
        "label": "Rouge",
        "kind": 6,
        "importPath": "utils.rouge.rouge",
        "description": "utils.rouge.rouge",
        "peekOfCode": "class Rouge():\n    '''\n    Class for computing ROUGE-L score for a set of candidate sentences for the MS COCO test set\n    '''\n    def __init__(self):\n        # vrama91: updated the value below based on discussion with Hovey\n        self.beta = 1.2\n    def calc_score(self, candidate, refs):\n        \"\"\"\n        Compute ROUGE-L score given one candidate and references for an image",
        "detail": "utils.rouge.rouge",
        "documentation": {}
    },
    {
        "label": "my_lcs",
        "kind": 2,
        "importPath": "utils.rouge.rouge",
        "description": "utils.rouge.rouge",
        "peekOfCode": "def my_lcs(string, sub):\n    \"\"\"\n    Calculates longest common subsequence for a pair of tokenized strings\n    :param string : list of str : tokens from a string split using whitespace\n    :param sub : list of str : shorter string, also split using whitespace\n    :returns: length (list of int): length of the longest common subsequence between the two strings\n    Note: my_lcs only gives length of the longest common subsequence, not the actual LCS\n    \"\"\"\n    if(len(string)< len(sub)):\n        sub, string = string, sub",
        "detail": "utils.rouge.rouge",
        "documentation": {}
    },
    {
        "label": "box3d_iou",
        "kind": 2,
        "importPath": "utils.box_util",
        "description": "utils.box_util",
        "peekOfCode": "def box3d_iou(corners1, corners2):\n    ''' Compute 3D bounding box IoU.\n    Input:\n        corners1: numpy array (8,3), assume up direction is Z\n        corners2: numpy array (8,3), assume up direction is Z\n    Output:\n        iou: 3D bounding box IoU\n    '''\n    x_min_1, x_max_1, y_min_1, y_max_1, z_min_1, z_max_1 = get_box3d_min_max(corners1)\n    x_min_2, x_max_2, y_min_2, y_max_2, z_min_2, z_max_2 = get_box3d_min_max(corners2)",
        "detail": "utils.box_util",
        "documentation": {}
    },
    {
        "label": "get_box3d_min_max",
        "kind": 2,
        "importPath": "utils.box_util",
        "description": "utils.box_util",
        "peekOfCode": "def get_box3d_min_max(corner):\n    ''' Compute min and max coordinates for 3D bounding box\n        Note: only for axis-aligned bounding boxes\n    Input:\n        corners: numpy array (8,3), assume up direction is Z (batch of N samples)\n    Output:\n        box_min_max: an array for min and max coordinates of 3D bounding box IoU\n    '''\n    min_coord = corner.min(axis=0)\n    max_coord = corner.max(axis=0)",
        "detail": "utils.box_util",
        "documentation": {}
    },
    {
        "label": "get_3d_box",
        "kind": 2,
        "importPath": "utils.box_util",
        "description": "utils.box_util",
        "peekOfCode": "def get_3d_box(center, box_size):\n    ''' box_size is array(l,w,h), heading_angle is radius clockwise from pos x axis, center is xyz of box center\n        output (8,3) array for 3D box cornders\n        Similar to utils/compute_orientation_3d\n    '''\n    l,w,h = box_size\n    # x_corners = [l/2,l/2,-l/2,-l/2,l/2,l/2,-l/2,-l/2]\n    # y_corners = [h/2,h/2,h/2,h/2,-h/2,-h/2,-h/2,-h/2]\n    # z_corners = [w/2,-w/2,-w/2,w/2,w/2,-w/2,-w/2,w/2]\n    x_corners = [l/2,l/2,-l/2,-l/2,l/2,l/2,-l/2,-l/2]",
        "detail": "utils.box_util",
        "documentation": {}
    },
    {
        "label": "Vocabulary",
        "kind": 6,
        "importPath": "utils.build_vocabulary",
        "description": "utils.build_vocabulary",
        "peekOfCode": "class Vocabulary(object):\n    def __init__(self, path=None):\n        self.vocab = {}\n        self.id_to_vocab = {}\n        self.id_to_bert = {}\n        if path is not None:\n            load_dict = torch.load(path)\n            self.vocab = load_dict['vocab']\n            self.id_to_vocab = load_dict['id_to_vocab']\n            self.id_to_bert = load_dict['id_to_bert']",
        "detail": "utils.build_vocabulary",
        "documentation": {}
    },
    {
        "label": "build_scanrefer_bert_vocabulary",
        "kind": 2,
        "importPath": "utils.build_vocabulary",
        "description": "utils.build_vocabulary",
        "peekOfCode": "def build_scanrefer_bert_vocabulary():\n    vocab = Vocabulary()\n    vocab.add_token('[EOS]', 102)\n    anno_file = os.path.join(SCAN_FAMILY_BASE, 'annotations/refer/scanrefer.jsonl')\n    save_path = os.path.join(SCAN_FAMILY_BASE, 'annotations/meta_data/scanrefer_vocab.pth')\n    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\",  do_lower_case=True)\n    with jsonlines.open(anno_file, 'r') as f:\n        for item in f:\n            ids = tokenizer.encode(item['utterance'], add_special_tokens=False)\n            for id in ids:",
        "detail": "utils.build_vocabulary",
        "documentation": {}
    },
    {
        "label": "get_vocabulary",
        "kind": 2,
        "importPath": "utils.build_vocabulary",
        "description": "utils.build_vocabulary",
        "peekOfCode": "def get_vocabulary(path):\n    vocab = Vocabulary(path)\n    return vocab",
        "detail": "utils.build_vocabulary",
        "documentation": {}
    },
    {
        "label": "Search",
        "kind": 6,
        "importPath": "utils.caption_search",
        "description": "utils.caption_search",
        "peekOfCode": "class Search(object):\n    def output_txt(self, txt_ids):\n        txt_len = 1\n        while txt_len < self.max_seq_length:\n            if txt_ids[txt_len] == 102:\n                break\n            txt_len += 1\n        txt_token_list = txt_ids[1:txt_len]\n        decoded_txt = self.tokenizer.decode(txt_token_list)\n        # add space",
        "detail": "utils.caption_search",
        "documentation": {}
    },
    {
        "label": "GreedySearch",
        "kind": 6,
        "importPath": "utils.caption_search",
        "description": "utils.caption_search",
        "peekOfCode": "class GreedySearch(Search):\n    def __init__(self, txt_ids, tokenizer, vocab, max_seq_length) -> None:\n        self.tokenizer = tokenizer\n        self.vocab = vocab\n        self.eos_map = {}\n        self.txt_ids = txt_ids.clone()\n        self.cur_id = 1\n        self.max_seq_length = max_seq_length\n        self.batch_size = txt_ids.shape[0]\n    def update(self, cur_cls_logit):",
        "detail": "utils.caption_search",
        "documentation": {}
    },
    {
        "label": "eval_ref_one_sample",
        "kind": 2,
        "importPath": "utils.eval_helper",
        "description": "utils.eval_helper",
        "peekOfCode": "def eval_ref_one_sample(pred_bbox, gt_bbox):\n    \"\"\" Evaluate one reference prediction\n    Args:\n        pred_bbox: 8 corners of prediction bounding box, (8, 3)\n        gt_bbox: 8 corners of ground truth bounding box, (8, 3)\n    Returns:\n        iou: intersection over union score\n    \"\"\"\n    iou = box3d_iou(pred_bbox, gt_bbox)\n    return iou",
        "detail": "utils.eval_helper",
        "documentation": {}
    },
    {
        "label": "construct_bbox_corners",
        "kind": 2,
        "importPath": "utils.eval_helper",
        "description": "utils.eval_helper",
        "peekOfCode": "def construct_bbox_corners(center, box_size):\n    sx, sy, sz = box_size\n    x_corners = [sx/2, sx/2, -sx/2, -sx/2, sx/2, sx/2, -sx/2, -sx/2]\n    y_corners = [sy/2, -sy/2, -sy/2, sy/2, sy/2, -sy/2, -sy/2, sy/2]\n    z_corners = [sz/2, sz/2, sz/2, sz/2, -sz/2, -sz/2, -sz/2, -sz/2]\n    corners_3d = np.vstack([x_corners, y_corners, z_corners])\n    corners_3d[0,:] = corners_3d[0,:] + center[0];\n    corners_3d[1,:] = corners_3d[1,:] + center[1];\n    corners_3d[2,:] = corners_3d[2,:] + center[2];\n    corners_3d = np.transpose(corners_3d)",
        "detail": "utils.eval_helper",
        "documentation": {}
    },
    {
        "label": "convert_pc_to_box",
        "kind": 2,
        "importPath": "utils.eval_helper",
        "description": "utils.eval_helper",
        "peekOfCode": "def convert_pc_to_box(obj_pc):\n    xmin = np.min(obj_pc[:,0])\n    ymin = np.min(obj_pc[:,1])\n    zmin = np.min(obj_pc[:,2])\n    xmax = np.max(obj_pc[:,0])\n    ymax = np.max(obj_pc[:,1])\n    zmax = np.max(obj_pc[:,2])\n    center = [(xmin+xmax)/2, (ymin+ymax)/2, (zmin+zmax)/2]\n    box_size = [xmax-xmin, ymax-ymin, zmax-zmin]\n    return center, box_size",
        "detail": "utils.eval_helper",
        "documentation": {}
    },
    {
        "label": "is_explicitly_view_dependent",
        "kind": 2,
        "importPath": "utils.eval_helper",
        "description": "utils.eval_helper",
        "peekOfCode": "def is_explicitly_view_dependent(tokens):\n    \"\"\"\n    :return: a boolean mask\n    \"\"\"\n    target_words = {'front', 'behind', 'back', 'right', 'left', 'facing', 'leftmost', 'rightmost',\n                    'looking', 'across'}\n    for token in tokens:\n        if token in target_words:\n            return True\n    return False",
        "detail": "utils.eval_helper",
        "documentation": {}
    },
    {
        "label": "LabelConverter",
        "kind": 6,
        "importPath": "utils.label_utils",
        "description": "utils.label_utils",
        "peekOfCode": "class LabelConverter(object):\n    def __init__(self, file_path):\n        self.raw_name_to_id = {}\n        self.nyu40id_to_id = {}\n        self.nyu40_name_to_id = {}\n        self.scannet_name_to_scannet_id = {'cabinet':0, 'bed':1, 'chair':2, 'sofa':3, 'table':4, 'door':5,\n            'window':6,'bookshelf':7,'picture':8, 'counter':9, 'desk':10, 'curtain':11,\n            'refrigerator':12, 'shower curtain':13, 'toilet':14, 'sink':15, 'bathtub':16, 'others':17}  \n        self.id_to_scannetid = {}\n        with open(file_path) as fd:",
        "detail": "utils.label_utils",
        "documentation": {}
    },
    {
        "label": "TensorboardLogger",
        "kind": 6,
        "importPath": "utils.logger",
        "description": "utils.logger",
        "peekOfCode": "class TensorboardLogger(object):\n    def __init__(self, cfg):\n        log_dir = cfg['logger']['args']['log_dir']\n        if not os.path.isdir(log_dir):\n            os.mkdir(log_dir)\n        self.writer = SummaryWriter(log_dir)\n    def log(self, log_dict, step=None):\n        for k, v in log_dict.items():\n            self.writer.add_scalar(k, v, step)",
        "detail": "utils.logger",
        "documentation": {}
    },
    {
        "label": "deep_update",
        "kind": 2,
        "importPath": "utils.logger",
        "description": "utils.logger",
        "peekOfCode": "def deep_update(d, u):\n    for k, v in u.items():\n        if isinstance(v, collections.abc.Mapping):\n            d[k] = deep_update(d.get(k, {}), v)\n        else:\n            d[k] = v\n    return d\n@registry.register_utils(\"tensorboard_logger\")\nclass TensorboardLogger(object):\n    def __init__(self, cfg):",
        "detail": "utils.logger",
        "documentation": {}
    },
    {
        "label": "random_sampling",
        "kind": 2,
        "importPath": "utils.pc_utils",
        "description": "utils.pc_utils",
        "peekOfCode": "def random_sampling(pc, num_sample, replace=None, return_choices=False):\n    \"\"\" Input is NxC, output is num_samplexC\n    \"\"\"\n    if replace is None: replace = (pc.shape[0]<num_sample)\n    choices = np.random.choice(pc.shape[0], num_sample, replace=replace)\n    if return_choices:\n        return pc[choices], choices\n    else:\n        return pc[choices]",
        "detail": "utils.pc_utils",
        "documentation": {}
    },
    {
        "label": "ModelSaver",
        "kind": 6,
        "importPath": "utils.saver",
        "description": "utils.saver",
        "peekOfCode": "class ModelSaver(object):\n    def __init__(self, save_dir=None, save_name=None, load_dir=None, load_name=None):\n        if save_dir and not os.path.isdir(save_dir):\n            os.mkdir(save_dir)\n        if save_dir:\n            self.save_path = os.path.join(save_dir, save_name)\n        else:\n            self.save_path = None\n        if load_dir is not None:\n            self.load_path = os.path.join(load_dir, load_name)",
        "detail": "utils.saver",
        "documentation": {}
    },
    {
        "label": "get_arg_parse",
        "kind": 2,
        "importPath": "run",
        "description": "run",
        "peekOfCode": "def get_arg_parse():\n    parser = argparse.ArgumentParser(description='config path')\n    parser.add_argument('--config', type=str, default=None, help='path of cfg')\n    args = parser.parse_args()\n    return args\nif __name__ == '__main__':\n    args = get_arg_parse()\n    setup_imports()\n    pipelines = pipeline_factory.create_pipelines_from_yml(args.config)\n    for pipeline in pipelines:",
        "detail": "run",
        "documentation": {}
    }
]